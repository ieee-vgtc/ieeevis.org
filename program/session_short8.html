<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Short Papers: Visualization with/for/in AI"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Short Papers: Visualization with/for/in AI"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Short Papers: Visualization with/for/in AI</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Visualization with/for/in AI</li></ol></nav><h1 class="session-title">VIS Short Papers: Visualization with/for/in AI</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-short.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-short.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Wenskovitch, John </h3><h3 class="session-room mt-4"> Room: Hall E1 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-05T10:15:00+00:00 &ndash; 2025-11-05T11:30:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-05T10:15:00+00:00 &ndash; 2025-11-05T11:30:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/short8.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945416660254782" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1270&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective&#39;, &#39;contributors&#39;: [&#39;Haotian Li&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:15:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:15:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Haotian Li&#39;, &#39;email&#39;: &#39;haotian.li@microsoft.com&#39;, &#39;affiliation&#39;: &#39;Microsoft Research Asia&#39;}, {&#39;name&#39;: &#39;Yun Wang&#39;, &#39;email&#39;: &#39;wangyun@microsoft.com&#39;, &#39;affiliation&#39;: &#39;Microsoft Research&#39;}, {&#39;name&#39;: &#39;Huamin Qu&#39;, &#39;email&#39;: &#39;huamin@cse.ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}], &#39;abstract&#39;: &#39;Human-AI collaborative tools attract attentions from the data storytelling community to lower the barrier of expertise and streamline the workflow. The recent advance in large-scale generative AI techniques, e.g., large language models (LLMs) and text-to-image models, has the potential to enhance data storytelling with their power in visual and narration generation. After two years since these techniques were publicly available, it is important to reflect our progress of applying them and have an outlook for future opportunities. To achieve the goal, we compare the collaboration patterns of the latest tools with those of earlier ones using a dedicated framework for understanding human-AI collaboration in data storytelling. Through comparison, we identify consistently widely studied patterns, e.g., human-creator + AI-assistant, and newly explored or emerging ones, e.g., AI-creator + human-reviewer. The benefits of these AI techniques and implications to human-AI collaboration are also revealed. We further propose future directions to hopefully ignite innovations.&#39;, &#39;uid&#39;: &#39;e4f2ed49-3ceb-485d-8d71-8463039a522b&#39;, &#39;keywords&#39;: [&#39;Data storytelling&#39;, &#39;human-AI collaboration&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_e4f2ed49-3ceb-485d-8d71-8463039a522b.html"> Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Haotian Li, Yun Wang, Huamin Qu </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Haotian Li </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:15:00.000Z &ndash; 2025-11-05T10:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1151&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding&#39;, &#39;contributors&#39;: [&#39;Can Liu&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:33:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Can Liu&#39;, &#39;email&#39;: &#39;can.liu.1996@gmail.com&#39;, &#39;affiliation&#39;: &#39;Nanyang Technological University&#39;}, {&#39;name&#39;: &#39;Chunlin Da&#39;, &#39;email&#39;: &#39;dachunlin00@gmail.com&#39;, &#39;affiliation&#39;: &#39;UNIVERSITY OF ST ANDREWS&#39;}, {&#39;name&#39;: &#39;Xiaoxiao Long&#39;, &#39;email&#39;: &#39;xxlong@nju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Nanjing University&#39;}, {&#39;name&#39;: &#39;Yuxiao Yang&#39;, &#39;email&#39;: &#39;yuxiao@fondant.design&#39;, &#39;affiliation&#39;: &#39;Tsinghua University&#39;}, {&#39;name&#39;: &#39;Yu Zhang&#39;, &#39;email&#39;: &#39;yuzhang94@outlook.com&#39;, &#39;affiliation&#39;: &#39;Huawei Technologies Co., Ltd&#39;}, {&#39;name&#39;: &#39;Yong WANG&#39;, &#39;email&#39;: &#39;yong-wang@ntu.edu.sg&#39;, &#39;affiliation&#39;: &#39;Nanyang Technological University&#39;}], &#39;abstract&#39;: &#39;Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a novel simplified vector format that encodes chart elements such as mark type, position, and size. The effectiveness of SimVec is demonstrated by using MLLMs to reconstruct chart information from SimVec formats. Then, we build a new visualization dataset, SimVecVis, to enhance the performance of MLLMs in visualization understanding, which consists of three key dimensions: bitmap images of charts, their SimVec representations, and corresponding data-centric question-answering (QA) pairs with explanatory chain-of-thought (CoT) descriptions. We finetune state-of-the-art MLLMs (e.g., MiniCPM and Qwen-VL), using SimVecVis with different dataset dimensions. The experimental results show that it leads to substantial performance improvements of MLLMs with good spatial perception capabilities (e.g., MiniCPM) in data-centric QA tasks. Our dataset and source code are available at: https://github.com/VIDA-Lab/SimVecVis.&#39;, &#39;uid&#39;: &#39;fe3d1eaa-d502-47c2-9939-61ad180dc260&#39;, &#39;keywords&#39;: [&#39;Visualization LLM&#39;, &#39;Multimodal LLM&#39;, &#39;Chart QA&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2506.21319&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/VIDA-Lab/SimVecVis&#39;} <h3 class="session-list-title"><a href="paper_fe3d1eaa-d502-47c2-9939-61ad180dc260.html"> SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Can Liu, Chunlin Da, Xiaoxiao Long, Yuxiao Yang, Yu Zhang, Yong WANG </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Can Liu </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:24:00.000Z &ndash; 2025-11-05T10:33:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1194&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;Towards Difficulty-Aware Analysis of Deep Neural Networks&#39;, &#39;contributors&#39;: [&#39;Linhao Meng&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:33:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:33:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Linhao Meng&#39;, &#39;email&#39;: &#39;l.meng1@tue.nl&#39;, &#39;affiliation&#39;: &#39;Eindhoven University of Technology&#39;}, {&#39;name&#39;: &#39;Stef van den Elzen&#39;, &#39;email&#39;: &#39;s.j.v.d.elzen@tue.nl&#39;, &#39;affiliation&#39;: &#39;Eindhoven University of Technology&#39;}, {&#39;name&#39;: &#39;Anna Vilanova&#39;, &#39;email&#39;: &#39;a.vilanova@tue.nl&#39;, &#39;affiliation&#39;: &#39;Eindhoven University of Technology&#39;}], &#39;abstract&#39;: &#39;Traditional instance-based model analysis focuses mainly on misclassified instances. However, this approach overlooks the varying difficulty associated with different instances. Ideally, a robust model should recognize and reflect the challenges presented by intrinsically difficult instances. It is also valuable to investigate whether the difficulty perceived by the model aligns with that perceived by humans. To address this, we propose incorporating instance difficulty into the deep neural network evaluation process, specifically for supervised classification tasks on image data. Specifically, we consider difficulty measures from three perspectives -- data, model, and human -- to facilitate comprehensive evaluation and comparison. Additionally, we develop an interactive visual tool, DifficultyEyes, to support the identification of instances of interest based on various difficulty patterns and to aid in analyzing potential data or model issues. Case studies demonstrate the effectiveness of our approach.&#39;, &#39;uid&#39;: &#39;b3c081c3-338e-4dd1-8d57-3a35f981facc&#39;, &#39;keywords&#39;: [&#39;Visualization&#39;, &#39;deep neural network&#39;, &#39;difficulty&#39;], &#39;preprint_link&#39;: &#39;https://doi.org/10.48550/arXiv.2507.00881&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_b3c081c3-338e-4dd1-8d57-3a35f981facc.html"> Towards Difficulty-Aware Analysis of Deep Neural Networks <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Linhao Meng, Stef van den Elzen, Anna Vilanova </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Linhao Meng </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:33:00.000Z &ndash; 2025-11-05T10:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1139&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations&#39;, &#39;contributors&#39;: [&#39;Narges Mahyar&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:42:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:42:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:51:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Mashrur Rashik&#39;, &#39;email&#39;: &#39;mrashik@cs.umass.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts Amherst&#39;}, {&#39;name&#39;: &#39;Jean-Daniel Fekete&#39;, &#39;email&#39;: &#39;jean-daniel.fekete@inria.fr&#39;, &#39;affiliation&#39;: &#39;Université Paris-Saclay, CNRS, Inria, LISN&#39;}, {&#39;name&#39;: &#39;Narges Mahyar&#39;, &#39;email&#39;: &#39;nmahyar@cs.umass.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts Amherst&#39;}], &#39;abstract&#39;: &#34;Communicating climate change remains challenging, as climate reports, though rich in data and visualizations, often feel too abstract or technical for the public. Although personalization can enhance communication, most tools still lack the narrative and visualization tailoring needed to connect with individual experiences. We present CLAImate, an AI-enabled prototype that personalizes conversation narratives and localizes visualizations based on users&#39; climate knowledge and geographic location. We evaluated CLAImate through internal verification of factual correctness, a formative study with experts, and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70% FACTSCORE. Visualization experts appreciated its clarity and personalization, and seven out of ten UK participants reported better understanding and local relevance of climate risks with CLAImate. We also discuss design challenges in personalization, accuracy, and scalability, and outline future directions for integrating visualizations in personalized conversational interfaces.&#34;, &#39;uid&#39;: &#39;55c2bdfa-142e-44ed-989c-76c9b32b3d66&#39;, &#39;keywords&#39;: [&#39;Other Application Areas; Communication/Presentation&#39;, &#39;Storytelling; General Public; Software Prototype.&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.11677&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/9vgf2/&#39;} <h3 class="session-list-title"><a href="paper_55c2bdfa-142e-44ed-989c-76c9b32b3d66.html"> CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Mashrur Rashik, Jean-Daniel Fekete, Narges Mahyar </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Narges Mahyar </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:42:00.000Z &ndash; 2025-11-05T10:51:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1124&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations&#39;, &#39;contributors&#39;: [&#39;Nicholas Hinds&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:51:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:51:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T11:00:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Aniket Nuthalapati&#39;, &#39;email&#39;: &#39;nutha010@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota&#39;}, {&#39;name&#39;: &#39;Nicholas Hinds&#39;, &#39;email&#39;: &#39;hinds084@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota Twin Cities&#39;}, {&#39;name&#39;: &#39;Brian Lim&#39;, &#39;email&#39;: &#39;brianlim@comp.nus.edu.sg&#39;, &#39;affiliation&#39;: &#39;National University of Singapore&#39;}, {&#39;name&#39;: &#39;Qianwen Wang&#39;, &#39;email&#39;: &#39;qianwen@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota&#39;}], &#39;abstract&#39;: &#39;As AI systems become increasingly integrated into high-stakes domains, enabling users to accurately interpret model behavior is critical. While AI explanations are readily available, users often struggle to reason effectively with these explanations, limiting their ability to validate or learn from AI decisions. To address this gap, we introduce Reverse Mapping, a novel approach that enhances visual explanations by incorporating user-derived insights back into the explanation workflow. Our system extracts structured insights from free-form user interpretations using a large language model and maps them back onto visual explanations through interactive annotations and coordinated multi-view visualizations. Inspired by the verification loop in the visualization knowledge generation model, this design aims to foster more deliberate, reflective interaction with AI explanations. We demonstrate our approach in a prototype system with two use cases and qualitative user feedback.&#39;, &#39;uid&#39;: &#39;494d7752-3810-46fa-906c-610bb1c013ef&#39;, &#39;keywords&#39;: [&#39;XAI Visualization&#39;, &#39;reverse mapping&#39;, &#39;insight verification&#39;, &#39;explainable AI&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_494d7752-3810-46fa-906c-610bb1c013ef.html"> Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Aniket Nuthalapati, Nicholas Hinds, Brian Lim, Qianwen Wang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Nicholas Hinds </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:51:00.000Z &ndash; 2025-11-05T11:00:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1049&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction&#39;, &#39;contributors&#39;: [&#39;Jiyeon Bae&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T11:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T11:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T11:09:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Jiyeon Bae&#39;, &#39;email&#39;: &#39;bjy7266@gmail.com&#39;, &#39;affiliation&#39;: &#39;Seoul National University&#39;}, {&#39;name&#39;: &#39;Hyeon Jeon&#39;, &#39;email&#39;: &#39;hj@hcil.snu.ac.kr&#39;, &#39;affiliation&#39;: &#39;Seoul National University&#39;}, {&#39;name&#39;: &#39;Jinwook Seo&#39;, &#39;email&#39;: &#39;jseo@snu.ac.kr&#39;, &#39;affiliation&#39;: &#39;Seoul National University&#39;}], &#39;abstract&#39;: &#39;Evaluating the accuracy of dimensionality reduction (DR) projections in preserving the structure of high-dimensional data is crucial for reliable visual analytics. Diverse evaluation metrics targeting different structural characteristics have thus been developed. However, evaluations of DR projections can become biased if highly correlated metrics—those measuring similar structural characteristics—are inadvertently selected, favoring DR techniques that emphasize those characteristics. To address this issue, we propose a novel workflow that reduces bias in the selection of evaluation metrics by clustering metrics based on their empirical correlations rather than on their intended design characteristics alone. Our workflow works by computing metric similarity using pairwise correlations, clustering metrics to minimize overlap, and selecting a representative metric from each cluster. Quantitative experiments demonstrate that our approach improves the stability of DR evaluation, which indicates that our workflow contributes to mitigating evaluation bias.&#39;, &#39;uid&#39;: &#39;3dd6a593-74c1-4ea0-b7bc-249654187e88&#39;, &#39;keywords&#39;: [&#39;Dimensionality reduction&#39;, &#39;Evaluation metrics&#39;, &#39;Correlation analysis&#39;, &#39;Benchmarking&#39;, &#39;Visual analytics&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/JiyeonBae/dr-metric-selection.git&#39;} <h3 class="session-list-title"><a href="paper_3dd6a593-74c1-4ea0-b7bc-249654187e88.html"> Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Jiyeon Bae, Hyeon Jeon, Jinwook Seo </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jiyeon Bae </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T11:00:00.000Z &ndash; 2025-11-05T11:09:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1345&#39;, &#39;session_id&#39;: &#39;short8&#39;, &#39;title&#39;: &#39;Does visualization help AI understand data?&#39;, &#39;contributors&#39;: [&#39;Johnathan Sun&#39;, &#39;Victoria Li&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T11:09:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T11:09:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T11:18:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Johnathan Sun&#39;, &#39;email&#39;: &#39;jlsun@college.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard&#39;}, {&#39;name&#39;: &#39;Victoria Li&#39;, &#39;email&#39;: &#39;vrli@college.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard&#39;}, {&#39;name&#39;: &#39;Martin Wattenberg&#39;, &#39;email&#39;: &#39;wattenberg@gmail.com&#39;, &#39;affiliation&#39;: &#39;Harvard&#39;}], &#39;abstract&#39;: &#39;Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines---providing a blank chart and a chart with mismatched data---shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.&#39;, &#39;uid&#39;: &#39;dc011ac4-7257-4423-9aa4-a46452c10067&#39;, &#39;keywords&#39;: [&#39;AI&#39;, &#39;Workflow Design&#39;, &#39;Human-Machine Analysis.&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;Our source code is thoroughly documented, and we provide original, full datasets on Github for end-to-end verification and reproducibility. Our supplemental material contains additional analytics and graphics to document our experimental setup and results.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/johnathansun/lvlm-vis-data-understanding&#39;} <h3 class="session-list-title"><a href="paper_dc011ac4-7257-4423-9aa4-a46452c10067.html"> Does visualization help AI understand data? <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Johnathan Sun, Victoria Li, Martin Wattenberg </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Johnathan Sun, Victoria Li </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T11:09:00.000Z &ndash; 2025-11-05T11:18:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-short.html">VIS Short Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-05T10:15:00+00:00'
    endTime = '2025-11-05T11:30:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "e1-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>