<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Short Papers: Virtual Short Paper Session: Viz for AI & AI for Viz"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Short Papers: Virtual Short Paper Session: Viz for AI & AI for Viz"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Short Papers: Virtual Short Paper Session: Viz for AI &amp; AI for Viz</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Virtual Short Paper Session: Viz for AI &amp; AI for Viz</li></ol></nav><h1 class="session-title">VIS Short Papers: Virtual Short Paper Session: Viz for AI &amp; AI for Viz</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-short.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-short.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Jun Tao </h3><h3 class="session-room mt-4"> Room: Room 0.94 + 0.95 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-05T10:15:00+00:00 &ndash; 2025-11-05T11:30:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-05T10:15:00+00:00 &ndash; 2025-11-05T11:30:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/short12.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1128&#39;, &#39;session_id&#39;: &#39;short12&#39;, &#39;title&#39;: &#39;Challenges &amp; Opportunities with LLM-Assisted Visualization Retargeting&#39;, &#39;contributors&#39;: [&#39;Luke S. Snyder&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:15:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:15:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Luke S. Snyder&#39;, &#39;email&#39;: &#39;snyderl@cs.washington.edu&#39;, &#39;affiliation&#39;: &#39;University of Washington&#39;}, {&#39;name&#39;: &#39;Chenglong Wang&#39;, &#39;email&#39;: &#39;clwang15uw@gmail.com&#39;, &#39;affiliation&#39;: &#39;Microsoft Research&#39;}, {&#39;name&#39;: &#39;Steven Drucker&#39;, &#39;email&#39;: &#39;sdrucker@microsoft.com&#39;, &#39;affiliation&#39;: &#39;Microsoft Research&#39;}], &#39;abstract&#39;: &#39;Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.&#39;, &#39;uid&#39;: &#39;605afb22-7f43-443f-9183-77604f4577f3&#39;, &#39;keywords&#39;: [&#39;Visualization&#39;, &#39;Large Language Models&#39;, &#39;Retargeting&#39;], &#39;preprint_link&#39;: &#39;http://arxiv.org/abs/2507.01436&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_605afb22-7f43-443f-9183-77604f4577f3.html"> Challenges &amp; Opportunities with LLM-Assisted Visualization Retargeting <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Luke S. Snyder, Chenglong Wang, Steven Drucker </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Luke S. Snyder </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:15:00.000Z &ndash; 2025-11-05T10:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1239&#39;, &#39;session_id&#39;: &#39;short12&#39;, &#39;title&#39;: &#39;BNNVis: Towards Visual Analytics for Bayesian Neural Networks&#39;, &#39;contributors&#39;: [&#39;Gabriel Appleby&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:33:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Gabriel Appleby&#39;, &#39;email&#39;: &#39;gabriel.appleby@gmail.com&#39;, &#39;affiliation&#39;: &#39;National Renewable Energy Laboratory&#39;}, {&#39;name&#39;: &#39;Malik Hassanaly&#39;, &#39;email&#39;: &#39;malik.hassanaly@nrel.gov&#39;, &#39;affiliation&#39;: &#39;National Renewable Energy Laboratory&#39;}, {&#39;name&#39;: &#39;Jen Rogers&#39;, &#39;email&#39;: &#39;jennifer.rogers@inl.gov&#39;, &#39;affiliation&#39;: &#39;Idaho National Lab&#39;}, {&#39;name&#39;: &#39;Juliane Mueller&#39;, &#39;email&#39;: &#39;juliane.mueller@nrel.gov&#39;, &#39;affiliation&#39;: &#39;NREL&#39;}, {&#39;name&#39;: &#39;Kristi Potter&#39;, &#39;email&#39;: &#39;kristi.potter@nrel.gov&#39;, &#39;affiliation&#39;: &#39;National Renewable Energy Laboratory&#39;}], &#39;abstract&#39;: &#34;Bayesian Neural Networks (BNNs) offer a principled approach to modeling uncertainty in addition to providing predictions, making them particularly valuable for high-stake domains where uncertainty quantification is required. However, their adoption remains low, partly due to the difficulty in tuning and interpreting these models and their results. To address this limitation, we introduce BNNVis, a visual analytics tool designed to visualize BNNs and their results. BNNVis allows the user to understand the architecture and learned posterior weight distributions of their BNN at a glance and how these distributions differ from their prior. Additionally, the system helps them understand the distribution and magnitude of the accompanying uncertainties of the model&#39;s predictions. BNNVis provides insight into the final predictions and the model, helping practitioners tune and interpret BNNs and their results. We describe a usage scenario to demonstrate how the features of BNNVis come together to support a practitioner in using a BNN.&#34;, &#39;uid&#39;: &#39;362f56c3-81b6-4af1-aeb4-022ee36079d5&#39;, &#39;keywords&#39;: [&#39;Visual Analytics&#39;, &#39;Visualization&#39;, &#39;Machine Learning&#39;, &#39;Bayesian Neural-Networks&#39;, &#39;Uncertainty Visualization&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_362f56c3-81b6-4af1-aeb4-022ee36079d5.html"> BNNVis: Towards Visual Analytics for Bayesian Neural Networks <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Gabriel Appleby, Malik Hassanaly, Jen Rogers, Juliane Mueller, Kristi Potter </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Gabriel Appleby </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:24:00.000Z &ndash; 2025-11-05T10:33:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1200&#39;, &#39;session_id&#39;: &#39;short12&#39;, &#39;title&#39;: &#39;ReVise: A Human-AI Interface for Incremental Algorithmic Recourse&#39;, &#39;contributors&#39;: [&#39;Kaustav Bhattacharjee&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:33:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:33:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Kaustav Bhattacharjee&#39;, &#39;email&#39;: &#39;kaustavbhatt94@gmail.com&#39;, &#39;affiliation&#39;: &#39;Pacific Northwest National Laboratory&#39;}, {&#39;name&#39;: &#39;Jun Yuan&#39;, &#39;email&#39;: &#39;jy448@njit.edu&#39;, &#39;affiliation&#39;: &#39;New Jersey Institute of Technology&#39;}, {&#39;name&#39;: &#39;Aritra Dasgupta&#39;, &#39;email&#39;: &#39;dasgupta.aritra@gmail.com&#39;, &#39;affiliation&#39;: &#39;New Jersey Institute of Technology&#39;}], &#39;abstract&#39;: &#39;The recent adoption of artificial intelligence in socio-technical systems raises concerns about the black-box nature of the resulting decisions in fields such as hiring, finance, admissions, etc. If data subjects—such as job applicants, loan applicants, and students—receive an unfavorable outcome, they may be interested in algorithmic recourse, which involves updating certain features to yield a more favorable result when re-evaluated by algorithmic decision-making. Unfortunately, when individuals do not fully understand the incremental steps needed to change their circumstances, they risk following misguided paths that can lead to significant, long-term adverse consequences. Existing recourse approaches focus exclusively on the final recourse goal but neglect the possible incremental steps to reach the goal with real-life constraints, user preferences, and model artifacts. To address this gap, we formulate a visual analytic workflow for incremental recourse planning in collaboration with AI/ML experts and contribute an interactive visualization interface that helps data subjects efficiently navigate the recourse alternatives and make an informed decision. We present a usage scenario and subjective feedback from observational studies with twelve graduate students using a real-world dataset, which demonstrates that our approach can be instrumental for data subjects in choosing a suitable recourse path.&#39;, &#39;uid&#39;: &#39;0ecc86cf-63c0-4483-9849-b207a438bb59&#39;, &#39;keywords&#39;: [&#39;Visual analytics&#39;, &#39;recourse&#39;, &#39;machine learning&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;This is a novel method for designing visual analytic interfaces for algorithmic recourse. Paper will be put on arXiV soon.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/ngzcm/&#39;} <h3 class="session-list-title"><a href="paper_0ecc86cf-63c0-4483-9849-b207a438bb59.html"> ReVise: A Human-AI Interface for Incremental Algorithmic Recourse <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Kaustav Bhattacharjee, Jun Yuan, Aritra Dasgupta </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Kaustav Bhattacharjee </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:33:00.000Z &ndash; 2025-11-05T10:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1222&#39;, &#39;session_id&#39;: &#39;short12&#39;, &#39;title&#39;: &#39;The Agentopia Times: Understanding and Mitigating Hallucinations in Multi-Agent LLM Systems via Data Journalism Gameplay&#39;, &#39;contributors&#39;: [&#39;Yilin Lu&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:42:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:42:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T10:51:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yilin Lu&#39;, &#39;email&#39;: &#39;lu000661@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota, Twin Cities&#39;}, {&#39;name&#39;: &#39;Shurui Du&#39;, &#39;email&#39;: &#39;du000288@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota&#39;}, {&#39;name&#39;: &#39;Qianwen Wang&#39;, &#39;email&#39;: &#39;qianwen@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota&#39;}], &#39;abstract&#39;: &#39;Large language models (LLMs) are increasingly used to support data analysis and visualization tasks, but remain prone to hallucinations. Recent work suggests that multi-agent systems (MAS) can mitigate hallucinations by enabling internal validation and cross-verification. However, learning effective MAS coordination strategies to mitigate hallucination remains challenging, particularly for newcomers, due to the wide range of coordination strategies and the lack of interactive, hands-on learning tools. To address this, we present The Agentopia Times, an educational game that teaches hallucination mitigation through active experimentation with MAS coordination strategies. The Agentopia Times simulates a newsroom where LLM agents collaborate to create data-driven narratives, with users tasked with adjusting communication protocols to manage hallucinated content. The game features a structured mapping between MAS coordination and familiar gameplay mechanics, providing immediate feedback on hallucination outcomes. Through use cases and preliminary user feedback, we demonstrate how The Agentopia Times enables users to explore and mitigate hallucination in MAS.&#39;, &#39;uid&#39;: &#39;3b5e9042-feed-4d23-814b-7242a997aea1&#39;, &#39;keywords&#39;: [&#39;LLM&#39;, &#39;visualization generation&#39;, &#39;educational game&#39;, &#39;LLM hallucination&#39;, &#39;Multi-Agent&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/Visual-Intelligence-UMN/The-Agentopia-Times&#39;} <h3 class="session-list-title"><a href="paper_3b5e9042-feed-4d23-814b-7242a997aea1.html"> The Agentopia Times: Understanding and Mitigating Hallucinations in Multi-Agent LLM Systems via Data Journalism Gameplay <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yilin Lu, Shurui Du, Qianwen Wang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yilin Lu </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:42:00.000Z &ndash; 2025-11-05T10:51:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1350&#39;, &#39;session_id&#39;: &#39;short12&#39;, &#39;title&#39;: &#39;XplainAct: Visualization for Personalized Intervention Insights&#39;, &#39;contributors&#39;: [&#39;Yanming Zhang&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T10:51:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T10:51:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T11:00:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yanming Zhang&#39;, &#39;email&#39;: &#39;yanming.zhang@stonybrook.edu&#39;, &#39;affiliation&#39;: &#39;Stony Brook University&#39;}, {&#39;name&#39;: &#39;Krishnakumar Hegde&#39;, &#39;email&#39;: &#39;khegde@cs.stonybrook.edu&#39;, &#39;affiliation&#39;: &#39;Stony Brook University&#39;}, {&#39;name&#39;: &#39;Klaus Mueller&#39;, &#39;email&#39;: &#39;mueller@cs.sunysb.edu&#39;, &#39;affiliation&#39;: &#39;Stony Brook University&#39;}], &#39;abstract&#39;: &#39;Causality helps people reason about and understand complex systems, particularly through what-if analyses that explore how interventions might alter outcomes. Although existing methods embrace causal reasoning using interventions and counterfactual analysis, they primarily focus on effects at the population level. These approaches often fall short in systems characterized by significant heterogeneity, where the impact of an intervention can vary widely across subgroups. To address this challenge, we present XplainAct, a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations. We demonstrate the effectiveness of XplainAct through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.&#39;, &#39;uid&#39;: &#39;258e8105-5eb0-4316-99bc-40522d2b6d15&#39;, &#39;keywords&#39;: [&#39;Explainable AI&#39;, &#39;Causality&#39;, &#39;Visual Analytics&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_258e8105-5eb0-4316-99bc-40522d2b6d15.html"> XplainAct: Visualization for Personalized Intervention Insights <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yanming Zhang, Krishnakumar Hegde, Klaus Mueller </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yanming Zhang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T10:51:00.000Z &ndash; 2025-11-05T11:00:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1360&#39;, &#39;session_id&#39;: &#39;short12&#39;, &#39;title&#39;: &#39;Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice&#39;, &#39;contributors&#39;: [&#39;Yongsu Ahn&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T11:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T11:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T11:09:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yongsu Ahn&#39;, &#39;email&#39;: &#39;yongsu.ahn@pitt.edu&#39;, &#39;affiliation&#39;: &#39;Boston College&#39;}, {&#39;name&#39;: &#39;Nam Wook Kim&#39;, &#39;email&#39;: &#39;nam.wook.kim@bc.edu&#39;, &#39;affiliation&#39;: &#39;Boston College&#39;}], &#39;abstract&#39;: &#39;This paper investigates why recent generative AI models outperform humans in data visualization knowledge tasks. Through systematic comparative analysis of responses to visualization questions, we find that differences exist between two ChatGPT models and human outputs over rhetorical structure, knowledge breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more advanced model, displays a hybrid of characteristics from both humans and ChatGPT-3.5. The two models were generally favored over human responses, while their strengths in coverage and breadth, and emphasis on technical and task-oriented visualization feedback collectively shaped higher overall quality. Based on our findings, we draw implications for advancing user experiences based on the potential of LLMs and human perception over their capabilities, with relevance to broader applications of AI.&#39;, &#39;uid&#39;: &#39;8a0db17e-c752-4f4d-a713-082d59c2d1f7&#39;, &#39;keywords&#39;: [&#39;Generative AI&#39;, &#39;LLM&#39;, &#39;Visualization&#39;, &#39;Question and answering&#39;, &#39;ChatGPT&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_8a0db17e-c752-4f4d-a713-082d59c2d1f7.html"> Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yongsu Ahn, Nam Wook Kim </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yongsu Ahn </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T11:00:00.000Z &ndash; 2025-11-05T11:09:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-short.html">VIS Short Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-05T10:15:00+00:00'
    endTime = '2025-11-05T11:30:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "0_94_0_95-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>