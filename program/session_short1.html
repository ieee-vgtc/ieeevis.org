<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Short Papers: Immersive Visualization and Extended Reality"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Short Papers: Immersive Visualization and Extended Reality"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Short Papers: Immersive Visualization and Extended Reality</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Immersive Visualization and Extended Reality</li></ol></nav><h1 class="session-title">VIS Short Papers: Immersive Visualization and Extended Reality</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-short.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-short.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Lonni Besancon </h3><h3 class="session-room mt-4"> Room: Hall M2 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T14:45:00+00:00 &ndash; 2025-11-06T16:00:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T14:45:00+00:00 &ndash; 2025-11-06T16:00:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/short1.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945290818551969" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1080&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;Validation through Replication of Augmented Reality as a Visualization Technique for Scholarly Publications in Astronomy&#39;, &#39;contributors&#39;: [&#39;Mackenzie Michael Creamer&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T14:45:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T14:45:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T14:54:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Mackenzie Michael Creamer&#39;, &#39;email&#39;: &#39;18mcreamer@gmail.com&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}, {&#39;name&#39;: &#39;Jonathan Carifio&#39;, &#39;email&#39;: &#39;jonathan.carifio@cfa.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard-Smithsonian Center for Astrophysics&#39;}, {&#39;name&#39;: &#39;Alyssa Goodman&#39;, &#39;email&#39;: &#39;agoodman@cfa.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard-Smithsonian Center for Astrophysics&#39;}, {&#39;name&#39;: &#39;Michelle Borkin&#39;, &#39;email&#39;: &#39;m.borkin@neu.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}], &#39;abstract&#39;: &#39;The fields of astronomy and astrophysics are evolving and incorporating technologies to effectively view and explore 3D data visualizations, including Augmented Reality (AR). An analysis of the feasibility of using AR in journal publications for 3D visualizations took place two years ago, in 2023, when Adams et al. evaluated whether the perceived workload between AR and non-AR technologies was comparable. Given that the use of AR for astronomy journal publications was, and still is, in its infancy, the original study had to utilize data intended for K-12 education that had similar interactions and data types as a proxy for real-world data that could be visualized in future astronomy publications. In this paper, we present the results of a conceptual replication study of Adams et al.’s work to validate whether their findings hold with real astronomy stimuli. We found in our replication that many of the trends in the original study hold true, but that the workload experienced by participants was significantly higher under multiple conditions when using real-world data. Additionally, we found that the tradeoff between engagement and workload was as prevalent in the replication as it was in the original study. Our results provide a new framing for researchers to understand the tradeoffs of immersive visualization technologies and the increased workload of pairing these tools with complex, scientific stimuli. All Supplemental Material in our study is available at https://osf.io/j8urq/.&#39;, &#39;uid&#39;: &#39;7baf358e-7ab0-44c2-b0c3-dbab651a9268&#39;, &#39;keywords&#39;: [&#39;Augmented Reality&#39;, &#39;Human Subjects-Empirical Study&#39;, &#39;Astronomy Visualization&#39;, &#39;Replication Study&#39;, &#39;AR-Enhanced Publications&#39;], &#39;preprint_link&#39;: &#39;https://osf.io/preprints/osf/v6sfg_v1&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;Our study is a direct replication, accompanied by extensive supplemental materials that enhance transparency. These include a detailed comparison of methodological differences with the original study, fully documented source code for all analyses, and an additional experiment quantifying the impact of software variance.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/j8urq/files/osfstorage&#39;} <h3 class="session-list-title"><a href="paper_7baf358e-7ab0-44c2-b0c3-dbab651a9268.html"> Validation through Replication of Augmented Reality as a Visualization Technique for Scholarly Publications in Astronomy <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Mackenzie Michael Creamer, Jonathan Carifio, Alyssa Goodman, Michelle Borkin </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Mackenzie Michael Creamer </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T14:45:00.000Z &ndash; 2025-11-06T14:54:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1076&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena&#39;, &#39;contributors&#39;: [&#39;Yang Ouyang&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T14:54:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T14:54:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:03:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yang Ouyang&#39;, &#39;email&#39;: &#39;ouyy@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Yuchen Wu&#39;, &#39;email&#39;: &#39;wuych3@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Xiyuan Wang&#39;, &#39;email&#39;: &#39;wangxy7@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Laixin Xie&#39;, &#39;email&#39;: &#39;xielx@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Weicong Cheng&#39;, &#39;email&#39;: &#39;chengwc@ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Jianping Gan&#39;, &#39;email&#39;: &#39;magan@ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Quan Li&#39;, &#39;email&#39;: &#39;liquan@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Xiaojuan Ma&#39;, &#39;email&#39;: &#39;mxj@cse.ust.hk&#39;, &#39;affiliation&#39;: &#39;Hong Kong University of Science and Technology&#39;}], &#39;abstract&#39;: &#39;Communicating the complexity of oceanic phenomena—such as hypoxia and acidification—poses a persistent challenge for marine science. Despite advances in sensing technologies and computational models, conventional formats like static visualizations and text-based reports often fall short in conveying the dynamics of ocean changes. To address this gap, we present OceanVive, an immersive and interactive visualization system that transforms complex ocean datasets into navigable spatial narratives. OceanVive incorporates an exploratory panel on a table-sized tablet for managing immersive content on a large screen and integrates adaptive visual encodings, contextual storytelling, and intuitive navigation pathways to support effective communication. We validate the system through expert interviews, demonstrating its potential to enhance science communication and promote deeper public understanding.&#39;, &#39;uid&#39;: &#39;84d64ebc-a5cc-4fd5-855f-ead6dc4ddfc0&#39;, &#39;keywords&#39;: [&#39;Immersive visualization&#39;, &#39;Communication&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_84d64ebc-a5cc-4fd5-855f-ead6dc4ddfc0.html"> OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yang Ouyang, Yuchen Wu, Xiyuan Wang, Laixin Xie, Weicong Cheng, Jianping Gan, Quan Li, Xiaojuan Ma </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yang Ouyang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T14:54:00.000Z &ndash; 2025-11-06T15:03:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1094&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions&#39;, &#39;contributors&#39;: [&#39;Qixuan Liu&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:03:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:03:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Qixuan Liu&#39;, &#39;email&#39;: &#39;1155203213@link.cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;CUHK&#39;}, {&#39;name&#39;: &#39;Shi Qiu&#39;, &#39;email&#39;: &#39;shiqiu@cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;CUHK&#39;}, {&#39;name&#39;: &#39;Yinqiao Wang&#39;, &#39;email&#39;: &#39;yqwang@cse.cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;CUHK&#39;}, {&#39;name&#39;: &#39;Xiwen Wu&#39;, &#39;email&#39;: &#39;xiwenwu@cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;CUHK&#39;}, {&#39;name&#39;: &#39;Kenneth Siu Ho Chok&#39;, &#39;email&#39;: &#39;kennethchok@surgery.cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;CUHK&#39;}, {&#39;name&#39;: &#39;Chi-Wing Fu&#39;, &#39;email&#39;: &#39;cwfu@cse.cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;The Chinese University of Hong Kong&#39;}, {&#39;name&#39;: &#39;Pheng Ann Heng&#39;, &#39;email&#39;: &#39;pheng@cse.cuhk.edu.hk&#39;, &#39;affiliation&#39;: &#39;The Chinese University of Hong Kong&#39;}], &#39;abstract&#39;: &#34;Volumetric medical imaging technologies produce detailed 3D representations of anatomical structures. However, effective medical data visualization and exploration pose significant challenges, especially for individuals with limited medical expertise. We introduce a novel XR-based system with two key innovations: (1) a coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models and (2) a multimodal interaction framework combining hand gestures with LLM-enabled voice commands. We conduct preliminary evaluations, including a 15-participant user study and expert interviews, to demonstrate the system&#39;s abilities to enhance spatial understanding and reduce cognitive load. Experimental results show notable improvements in task completion times, usability metrics, and interaction effectiveness enhanced by LLM-driven voice control. While identifying areas for future refinement, our findings highlight the potential of this immersive visualization system to advance medical training and clinical practice. Our demo application and supplemental materials are available for download at: https://osf.io/bpjq5/.&#34;, &#39;uid&#39;: &#39;25ece7fd-1a08-463e-b526-9e6a16a62b59&#39;, &#39;keywords&#39;: [&#39;Medical Visualization&#39;, &#39;Medical XR&#39;, &#39;Multimodal Interaction&#39;], &#39;preprint_link&#39;: &#39;http://arxiv.org/abs/2506.22926&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/bpjq5/&#39;} <h3 class="session-list-title"><a href="paper_25ece7fd-1a08-463e-b526-9e6a16a62b59.html"> Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Qixuan Liu, Shi Qiu, Yinqiao Wang, Xiwen Wu, Kenneth Siu Ho Chok, Chi-Wing Fu, Pheng Ann Heng </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Qixuan Liu </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:03:00.000Z &ndash; 2025-11-06T15:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1125&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;Exploring AR Label Placements in Visually Cluttered Scenarios&#39;, &#39;contributors&#39;: [&#39;Ji Hwan Park&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:12:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:12:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:21:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Ji Hwan Park&#39;, &#39;email&#39;: &#39;jhpigm@rit.edu&#39;, &#39;affiliation&#39;: &#39;Rochester Institute of Technology&#39;}, {&#39;name&#39;: &#39;Braden Roper&#39;, &#39;email&#39;: &#39;bradenroper@ou.edu&#39;, &#39;affiliation&#39;: &#39;The University of Oklahoma&#39;}, {&#39;name&#39;: &#39;Amirhossein Arezoumand&#39;, &#39;email&#39;: &#39;amirhossein.arezoumand@ou.edu&#39;, &#39;affiliation&#39;: &#39;University of Oklahoma&#39;}, {&#39;name&#39;: &#39;Tien Tran&#39;, &#39;email&#39;: &#39;tien.g.tran@ou.edu&#39;, &#39;affiliation&#39;: &#39;University of Oklahoma&#39;}], &#39;abstract&#39;: &#34;We investigate methods for placing labels in AR environments that have visually cluttered scenes. As the number of items increases in a scene within the user&#39; FOV, it is challenging to effectively place labels based on existing label placement guidelines. To address this issue, we implemented three label placement techniques for in-view objects for AR applications. We specifically target a scenario, where various items of different types are scattered within the user&#39;s field of view, and multiple items of the same type are situated close together. We evaluate three placement techniques for three target tasks. Our study shows that using a label to spatially group the same types of items is beneficial for identifying, comparing, and summarizing data.&#34;, &#39;uid&#39;: &#39;5a4eadeb-75eb-48ac-8a15-c21a5b376113&#39;, &#39;keywords&#39;: [&#39;Label placement&#39;, &#39;view management systems&#39;, &#39;augmented reality&#39;, &#39;situated visualization&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.00198&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_5a4eadeb-75eb-48ac-8a15-c21a5b376113.html"> Exploring AR Label Placements in Visually Cluttered Scenarios <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Ji Hwan Park, Braden Roper, Amirhossein Arezoumand, Tien Tran </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Ji Hwan Park </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:12:00.000Z &ndash; 2025-11-06T15:21:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1213&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;Visualizing Player Movement and Game Events through Space-Time Cubes in Extended Reality&#39;, &#39;contributors&#39;: [&#39;Fabian Beck&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:21:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:21:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:30:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Niko S. Nannemann&#39;, &#39;email&#39;: &#39;niko-sebastian.nannemann@stud.uni-bamberg.de&#39;, &#39;affiliation&#39;: &#39;University of Bamberg&#39;}, {&#39;name&#39;: &#39;Shivam Agarwal&#39;, &#39;email&#39;: &#39;shivamworking@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Bamberg&#39;}, {&#39;name&#39;: &#39;Fabian Beck&#39;, &#39;email&#39;: &#39;fabian.beck@uni-bamberg.de&#39;, &#39;affiliation&#39;: &#39;University of Bamberg&#39;}], &#39;abstract&#39;: &#39;To understand multiplayer gameplay behavior in a bomb-laying game, this paper explores the visual representation of player movement and events using space-time cubes. Complementing a previous two-dimensional event visualization, the approach focuses on contextualizing the player trajectories on the board with important events. Leveraging extended reality technology, the three-dimensional space-time-cube representation of a game session can be placed like columns in the virtual space. Various techniques support the interactive exploration of the spatiotemporal data. We demonstrate insights that can be found through the analysis of AI agent play behavior.&#39;, &#39;uid&#39;: &#39;3c025133-4562-4d9a-aff2-725dd723a2a2&#39;, &#39;keywords&#39;: [&#39;Game analytics&#39;, &#39;space-time cube&#39;, &#39;event visualization&#39;, &#39;immersive analytics&#39;, &#39;extended reality.&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_3c025133-4562-4d9a-aff2-725dd723a2a2.html"> Visualizing Player Movement and Game Events through Space-Time Cubes in Extended Reality <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Niko S. Nannemann, Shivam Agarwal, Fabian Beck </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Fabian Beck </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:21:00.000Z &ndash; 2025-11-06T15:30:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1273&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics&#39;, &#39;contributors&#39;: [&#39;Eric Zimmermann&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:39:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Eric Zimmermann&#39;, &#39;email&#39;: &#39;e.zimmermann@uni-rostock.de&#39;, &#39;affiliation&#39;: &#39;Institute for Visual and Analytic Computing&#39;}, {&#39;name&#39;: &#39;Stefan Bruckner&#39;, &#39;email&#39;: &#39;stefan.bruckner@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Rostock&#39;}], &#39;abstract&#39;: &#39;Immersive visualization of network data enables users to physically navigate and interact with complex structures, but managing transitions between detailed local (egocentric) views and global (exocentric) overviews remains a major challenge. We present a multifocus probe technique for immersive environments that allows users to instantiate multiple egocentric subgraph views while maintaining persistent links to the global network context. Each probe acts as a portable local focus, enabling fine-grained inspection and editing of distant or occluded regions. Visual and haptic guidance mechanisms ensure context preservation during multi-scale interaction. We demonstrate and discuss the usability of our technique for the editing of network data.&#39;, &#39;uid&#39;: &#39;f4284776-c09e-4528-8684-cb06c085d1b1&#39;, &#39;keywords&#39;: [&#39;Virtual Reality&#39;, &#39;Graph&#39;, &#39;Focus+Context&#39;, &#39;Interaction&#39;, &#39;Editing&#39;], &#39;preprint_link&#39;: &#39;http://arxiv.org/abs/2507.01140&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_f4284776-c09e-4528-8684-cb06c085d1b1.html"> Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Eric Zimmermann, Stefan Bruckner </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Eric Zimmermann </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:30:00.000Z &ndash; 2025-11-06T15:39:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1243&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;Walking Through Time: A Hybrid Immersive System for Spatial and Expressive Timeline Authoring&#39;, &#39;contributors&#39;: [&#39;Veronica Vu&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:39:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:39:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Veronica Vu&#39;, &#39;email&#39;: &#39;ptv0002@uah.edu&#39;, &#39;affiliation&#39;: &#39;University of Alabama in Huntsville&#39;}, {&#39;name&#39;: &#39;Yogesh Rai&#39;, &#39;email&#39;: &#39;yr0011@uah.edu&#39;, &#39;affiliation&#39;: &#39;UAH&#39;}, {&#39;name&#39;: &#39;Haeyong Chung&#39;, &#39;email&#39;: &#39;hc0021@uah.edu&#39;, &#39;affiliation&#39;: &#39;University of Alabama in Huntsville&#39;}], &#39;abstract&#39;: &#39;This paper introduces a timeline authoring tool that integrates Augmented Reality and tablet-based spatial interaction, enabling users to create large-scale non-linear timelines by leveraging spatial cognition and embodied interaction. In this system, the AR headset provides a large, immersive space for visualizing and interacting with a room-sized 3D timeline, while the tablet allows for precise sketching, annotation, and sculpting of timeline structures such as curves and branches. We conducted a design workshop to explore the user experience with this tool, which supports the creation of diverse expressive timelines for two datasets.&#39;, &#39;uid&#39;: &#39;f999c7ff-3a3e-4d54-a591-9d720a8d8244&#39;, &#39;keywords&#39;: [&#39;Timeline&#39;, &#39;authoring&#39;, &#39;mobile&#39;, &#39;immersive analytics.&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;This work introduces a novel hybrid AR system that supports the authoring of large-scale, expressive timeline visualizations through tablet-based spatial interaction and immersive AR space.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_f999c7ff-3a3e-4d54-a591-9d720a8d8244.html"> Walking Through Time: A Hybrid Immersive System for Spatial and Expressive Timeline Authoring <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Veronica Vu, Yogesh Rai, Haeyong Chung </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Veronica Vu </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:39:00.000Z &ndash; 2025-11-06T15:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1134&#39;, &#39;session_id&#39;: &#39;short1&#39;, &#39;title&#39;: &#39;manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality&#39;, &#39;contributors&#39;: [&#39;Samuel Pantze&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:57:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Samuel Pantze&#39;, &#39;email&#39;: &#39;s.pantze@hzdr.de&#39;, &#39;affiliation&#39;: &#39;Center for Advanced Systems Understanding (CASUS)&#39;}, {&#39;name&#39;: &#39;Jean-Yves Tinevez&#39;, &#39;email&#39;: &#39;tinevez@pasteur.fr&#39;, &#39;affiliation&#39;: &#39;Institut Pasteur&#39;}, {&#39;name&#39;: &#39;Matthew McGinity&#39;, &#39;email&#39;: &#39;matthew.mcginity@tu-dresden.de&#39;, &#39;affiliation&#39;: &#39;Technische Universität Dresden&#39;}, {&#39;name&#39;: &#39;Ulrik Günther&#39;, &#39;email&#39;: &#39;ulrik.guenther@hzdr.de&#39;, &#39;affiliation&#39;: &#39;Helmholtz-Zentrum Dresden-Rossendorf e.V&#39;}], &#39;abstract&#39;: &#39;We propose manvr3d, a VR platform for immersive, AI-assisted human-in-the-loop cell tracking. Life scientists reconstruct the developmental history of organisms at the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. However, reconstruction of cell trajectories and lineage trees is a highly time consuming and error prone task. Common tools are often limited to 2D image display, which greatly limits spatial understanding and navigation. Deep Learning-based algorithms accelerate this process, yet depend heavily on manually-annotated, high-quality ground truth data and curation. In this work, we bridge the gap between Deep Learning-based cell tracking software and 3D/VR visualization to create a hybrid AI-human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the third dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists. We present here the technical architecture of our platform and first analysis of performance. Our code is released open source.&#39;, &#39;uid&#39;: &#39;b9fa4e27-b3e0-425b-b3c6-af1ebecd478a&#39;, &#39;keywords&#39;: [&#39;Systems Biology&#39;, &#39;Virtual Reality&#39;, &#39;Microscopy&#39;, &#39;Cell Tracking&#39;, &#39;Volume Rendering&#39;, &#39;Eye Tracking&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2505.03440&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/rn9h7/&#39;} <h3 class="session-list-title"><a href="paper_b9fa4e27-b3e0-425b-b3c6-af1ebecd478a.html"> manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Samuel Pantze </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:48:00.000Z &ndash; 2025-11-06T15:57:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-short.html">VIS Short Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T14:45:00+00:00'
    endTime = '2025-11-06T16:00:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "m2-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>