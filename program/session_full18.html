<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Immersive & Ubiquitous Analytics"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Immersive & Ubiquitous Analytics"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Immersive &amp; Ubiquitous Analytics</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Immersive &amp; Ubiquitous Analytics</li></ol></nav><h1 class="session-title">VIS Full Papers: Immersive &amp; Ubiquitous Analytics</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Jillian Aurisano </h3><h3 class="session-room mt-4"> Room: Hall M2 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T13:00:00+00:00 &ndash; 2025-11-06T14:15:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T13:00:00+00:00 &ndash; 2025-11-06T14:15:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full18.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945290818551969" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1017&#39;, &#39;session_id&#39;: &#39;full18&#39;, &#39;title&#39;: &#39;Running with Data: a Survey of the Current Research and a Design Exploration of Future Immersive Visualisations&#39;, &#39;contributors&#39;: [&#39;Ang Li&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Ang Li&#39;, &#39;email&#39;: &#39;leon.li.ang@foxmail.com&#39;, &#39;affiliation&#39;: &#39;University of Queensland&#39;}, {&#39;name&#39;: &#39;Charles Perin&#39;, &#39;email&#39;: &#39;cperin@uvic.ca&#39;, &#39;affiliation&#39;: &#39;University of Victoria&#39;}, {&#39;name&#39;: &#39;Gianluca Demartini&#39;, &#39;email&#39;: &#39;g.demartini@uq.edu.au&#39;, &#39;affiliation&#39;: &#39;University of Queensland&#39;}, {&#39;name&#39;: &#39;Stephen Viller&#39;, &#39;email&#39;: &#39;viller@acm.org&#39;, &#39;affiliation&#39;: &#39;University of Queensland&#39;}, {&#39;name&#39;: &#39;Jarrod Knibbe&#39;, &#39;email&#39;: &#39;j.knibbe@uq.edu.au&#39;, &#39;affiliation&#39;: &#39;The University of Queensland&#39;}, {&#39;name&#39;: &#39;Maxime Cordeil&#39;, &#39;email&#39;: &#39;m.cordeil@uq.edu.au&#39;, &#39;affiliation&#39;: &#39;The University of Queensland&#39;}], &#39;abstract&#39;: &#39;This work investigates the current research on in-situ visualisations for running: visualisations about data that are referred to during the running activity. We analyse 47 papers from 33 Human-Computer Interaction and Visualisation venues and identify six dimensions of a design space of in-situ running visualisations. Our analysis of this design space highlights an emerging trend: a shift from on-body, peripersonal visualisations (i.e., in the space within direct reach, such as visualisations on a smartwatch or a mobile phone display) towards extrapersonal displays (i.e., in the space beyond immediate reach, such as visualisations in immersive augmented reality displays) that integrate data in the runner’s surrounding environment. We explore this opportunity by conducting a series of workshops with 10 active runners in total, eliciting design concepts for running visualisations and interactions beyond conventional 2D displays. We find that runners show a strong interest for visualisation designs that favour more context-aware, interactive, and unobtrusive experiences that seamlessly integrate with their run. These findings inform a set of design considerations for future immersive running visualisations and highlight directions for further research.&#39;, &#39;uid&#39;: &#39;54daa89a-c0e5-4d1e-9891-4ce10711d2f9&#39;, &#39;keywords&#39;: [&#39;Running&#39;, &#39;Jogging; Survey&#39;, &#39;Taxonomy; Human-Subjects Qualitative Studies; Personal Visual Analytics; Mobile; Augmented/Mixed/Extended Reality&#39;, &#39;Immersive&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_54daa89a-c0e5-4d1e-9891-4ce10711d2f9.html"> Running with Data: a Survey of the Current Research and a Design Exploration of Future Immersive Visualisations <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Ang Li, Charles Perin, Gianluca Demartini, Stephen Viller, Jarrod Knibbe, Maxime Cordeil </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Ang Li </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:00:00.000Z &ndash; 2025-11-06T13:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1257&#39;, &#39;session_id&#39;: &#39;full18&#39;, &#39;title&#39;: &#39;Investigating the Effects of Augmented Reality on Message Credibility When Visualizing Environmental Impacts&#39;, &#39;contributors&#39;: [&#39;Aymeric Ferron&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:12:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:12:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Aymeric Ferron&#39;, &#39;email&#39;: &#39;aymeric.ferron@inria.fr&#39;, &#39;affiliation&#39;: &#39;Université de Bordeaux&#39;}, {&#39;name&#39;: &#39;Ambre Assor&#39;, &#39;email&#39;: &#39;ambre.assor@inria.fr&#39;, &#39;affiliation&#39;: &#39;Inria, CNRS, LISN&#39;}, {&#39;name&#39;: &#39;Pierre Dragicevic&#39;, &#39;email&#39;: &#39;pierre.dragice@gmail.com&#39;, &#39;affiliation&#39;: &#39;Inria, CNRS, Univ. Bordeaux&#39;}, {&#39;name&#39;: &#39;Yvonne Jansen&#39;, &#39;email&#39;: &#39;yvonne.jansen@cnrs.fr&#39;, &#39;affiliation&#39;: &#39;CNRS, Inria, Univ. Bordeaux, LaBRI&#39;}], &#39;abstract&#39;: &#39;Augmented reality (AR) has increasingly been used to communicate environmental impacts, offering greater engagement than conventional displays. However, its effect on message credibility—how much people believe in the content of the communication—remains unclear. In a preregistered study, we compared the perceived credibility of environmental information presented via visualizations on an AR headset or a desktop display. We created display-specific visual encodings (3D concrete for AR, 2D bar charts for desktop) and added two control conditions to cross display and encoding. We found no difference in message credibility between AR and desktop, though concrete AR was rated most engaging. Supplementary material is available at https://osf.io/n4p5c/.&#39;, &#39;uid&#39;: &#39;86dbd3cf-0467-4e0e-8612-b70910cdefbc&#39;, &#39;keywords&#39;: [&#39;Augmented Reality&#39;, &#39;Data Visualization&#39;, &#39;Credibility&#39;, &#39;Comparative Study&#39;, &#39;Sustainable HCI&#39;], &#39;preprint_link&#39;: &#39;https://hal.science/hal-05200516/document&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;We conducted a pre-registered study: https://osf.io/3djhs\nAll data and analyses scripts are available on OSF.\nThe code is publicly available on our Gitlab: https://gitlab.inria.fr/aferron/ar-credibility\nWe provide extra documentation on our OSF repository, including videos, questionnaires and the applications we tested: https://osf.io/n4p5c/&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/n4p5c/files/osfstorage&#39;} <h3 class="session-list-title"><a href="paper_86dbd3cf-0467-4e0e-8612-b70910cdefbc.html"> Investigating the Effects of Augmented Reality on Message Credibility When Visualizing Environmental Impacts <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Aymeric Ferron, Ambre Assor, Pierre Dragicevic, Yvonne Jansen </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Aymeric Ferron </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:12:00.000Z &ndash; 2025-11-06T13:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1747&#39;, &#39;session_id&#39;: &#39;full18&#39;, &#39;title&#39;: &#39;Embodied Natural Language Interaction (NLI): Speech Input Patterns in Immersive Analytics&#39;, &#39;contributors&#39;: [&#39;Hyemi Song&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:36:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Hyemi Song&#39;, &#39;email&#39;: &#39;hsong02@cs.umd.edu&#39;, &#39;affiliation&#39;: &#39;University of Maryland&#39;}, {&#39;name&#39;: &#39;Matthew Johnson&#39;, &#39;email&#39;: &#39;mjohns28@terpmail.umd.edu&#39;, &#39;affiliation&#39;: &#39;University of Maryland&#39;}, {&#39;name&#39;: &#39;Kirsten Whitley&#39;, &#39;email&#39;: &#39;visual.tycho@gmail.com&#39;, &#39;affiliation&#39;: &#39;Department of Defense&#39;}, {&#39;name&#39;: &#39;Eric Krokos&#39;, &#39;email&#39;: &#39;ericpkrokos@gmail.com&#39;, &#39;affiliation&#39;: &#39;Department of Defense&#39;}, {&#39;name&#39;: &#39;Amitabh Varshney&#39;, &#39;email&#39;: &#39;varshney@cs.umd.edu&#39;, &#39;affiliation&#39;: &#39;University of Maryland&#39;}], &#39;abstract&#39;: &#39;Embodiment shapes how users verbally express intent when interacting with data through speech interfaces in immersive analytics. Despite growing interest in Natural Language Interactions (NLIs) for visual analytics in immersive environments, users’ speech patterns and their use of embodiment cues in speech remain underexplored. Understanding their interplay is crucial to bridging the gap between users’ intent and an immersive analytic system. To address this, we report the results from 15 participants in a user study conducted using the Wizard of Oz method. We performed axial coding on 1,280 speech acts derived from 734 utterances, examining how analysis tasks are carried out with embodiment and linguistic features. Next, we measured Speech Input Uncertainty for each analysis task using the semantic entropy of utterances, estimating how uncertain users’ speech inputs appear to an analytic system. Through these analyses, we identified five speech input patterns, showing that users dynamically blend embodied and non-embodied speech acts depending on data analysis tasks, phases, and Embodiment Reliance driven by the counts and types of embodiment cues in each utterance. We then examined how these patterns align with user reflections on factors that challenge speech interaction during the study. Finally, we propose design implications aligned with the five patterns.&#39;, &#39;uid&#39;: &#39;33454738-a9a9-4944-a27f-262d1da9f35d&#39;, &#39;keywords&#39;: [&#39;Embodiment&#39;, &#39;Natural Language Interaction (NLI)&#39;, &#39;Immersive Analytics&#39;, &#39;Speech Patterns&#39;, &#39;Semantic Entropy&#39;, &#39;User Intent&#39;, &#39;Speech Acts&#39;], &#39;preprint_link&#39;: &#39;&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/sv4fn/&#39;} <h3 class="session-list-title"><a href="paper_33454738-a9a9-4944-a27f-262d1da9f35d.html"> Embodied Natural Language Interaction (NLI): Speech Input Patterns in Immersive Analytics <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Hyemi Song, Matthew Johnson, Kirsten Whitley, Eric Krokos, Amitabh Varshney </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Hyemi Song </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:24:00.000Z &ndash; 2025-11-06T13:36:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2023-09-0571.R2&#39;, &#39;session_id&#39;: &#39;full18&#39;, &#39;title&#39;: &#39;Augmented Dynamic Data Physicalization: Blending Shape-changing Data Sculptures with Virtual Content for Interactive Visualization&#39;, &#39;contributors&#39;: [&#39;Raimund Dachselt&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:36:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:36:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Severin Engert&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Andreas Peetz&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Konstantin Klamka&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Pierre Surer&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Tobias Isenberg&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Raimund Dachselt&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;We investigate the concept of Augmented Dynamic Data Physicalization, the combination of shape-changing physical data representations with high-resolution virtual content. Tangible data sculptures, for example using mid-air shape-changing interfaces, are aesthetically appealing and persistent, but also technically and spatially limited. Blending them with Augmented Reality overlays such as scales, labels, or other contextual information opens up new possibilities. We explore the potential of this promising combination and propose a set of essential visualization components and interaction principles. They facilitate sophisticated hybrid data visualizations, for example Overview &amp; Detail techniques or 3D view aggregations. We discuss three implemented applications that demonstrate how our approach can be used for personal information hubs, interactive exhibitions, and immersive data analytics. Based on these use cases, we conducted hands-on sessions with external experts, resulting in valuable feedback and insights. They highlight the potential of combining dynamic physicalizations with dynamic AR overlays to create rich and engaging data experiences.&#39;, &#39;uid&#39;: &#39;436e7b45-c89c-41ee-a1a7-7eb5e807dc28&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;Visualization&#39;, &#39;Augmented reality&#39;, &#39;Data analysis&#39;, &#39;Electronic mail&#39;, &#39;Training&#39;, &#39;Shape&#39;, &#39;Resists&#39;, &#39;Museums&#39;, &#39;Meteorology&#39;], &#39;preprint_link&#39;: &#39;https://imld.de/addp&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3547432&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_436e7b45-c89c-41ee-a1a7-7eb5e807dc28.html"> Augmented Dynamic Data Physicalization: Blending Shape-changing Data Sculptures with Virtual Content for Interactive Visualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Severin Engert, Andreas Peetz, Konstantin Klamka, Pierre Surer, Tobias Isenberg, Raimund Dachselt </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Raimund Dachselt </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:36:00.000Z &ndash; 2025-11-06T13:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-07-0533&#39;, &#39;session_id&#39;: &#39;full18&#39;, &#39;title&#39;: &#39;Exploring Spatial Hybrid User Interface for Visual Sensemaking&#39;, &#39;contributors&#39;: [&#39;Wai Tong&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T14:00:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Wai Tong&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Haobo Li&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Meng Xia&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Kam Kwai Wong&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Ting-Chuen Pong&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Huamin Qu&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yalong Yang&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#34;We built a spatial hybrid system that combines a personal computer (PC) and virtual reality (VR) for visual sensemaking, addressing limitations in both environments. Although VR offers immense potential for interactive data visualization (e.g., large display space and spatial navigation), it can also present challenges such as imprecise interactions and user fatigue. At the same time, a PC offers precise and familiar interactions but has limited display space and interaction modality. Therefore, we iteratively designed a spatial hybrid system (PC+VR) to complement these two environments by enabling seamless switching between PC and VR environments. To evaluate the system&#39;s effectiveness and user experience, we compared it to using a single computing environment (i.e., PC-only and VR-only). Our study results (N=18) showed that spatial PC+VR could combine the benefits of both devices to outperform user preference for VR-only without a negative impact on performance from device switching overhead. Finally, we discussed future design implications.&#34;, &#39;uid&#39;: &#39;424d3365-a7d7-4908-b3da-516f6bb9c7a7&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;Visualization&#39;, &#39;Three-dimensional displays&#39;, &#39;Switches&#39;, &#39;Navigation&#39;, &#39;Keyboards&#39;, &#39;Hands&#39;, &#39;User experience&#39;, &#39;Rendering (computer graphics)&#39;, &#39;Visual analytics&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2502.00853&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3538771&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_424d3365-a7d7-4908-b3da-516f6bb9c7a7.html"> Exploring Spatial Hybrid User Interface for Visual Sensemaking <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Wai Tong, Haobo Li, Meng Xia, Kam Kwai Wong, Ting-Chuen Pong, Huamin Qu, Yalong Yang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Wai Tong </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:48:00.000Z &ndash; 2025-11-06T14:00:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-09-0881.R1&#39;, &#39;session_id&#39;: &#39;full18&#39;, &#39;title&#39;: &#39;XROps: A Visual Workflow Management System for Dynamic Immersive Analytics.&#39;, &#39;contributors&#39;: [&#39;Suemin Jeon&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T14:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T14:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T14:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Suemin Jeon&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;JunYoung Choi&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Haejin Jeong&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Won-Ki Jeong&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;Immersive analytics is gaining attention across multiple domains due to its capability to facilitate intuitive data analysis in expansive environments through user interaction with data. However, creating immersive analytics systems for specific tasks is challenging due to the need for programming expertise and significant development effort. Despite the introduction of various immersive visualization authoring toolkits, domain experts still face hurdles in adopting immersive analytics into their workflow, particularly when faced with dynamically changing tasks and data in real time. To lower such technical barriers, we introduce XROps, a web-based authoring system that allows users to create immersive analytics applications through interactive visual programming, without the need for low-level scripting or coding. XROps enables dynamic immersive analytics authoring by allowing users to modify each step of the data visualization process with immediate feedback, enabling them to build visualizations on-the-fly and adapt to changing environments. It also supports the integration and visualization of real-time sensor data from XR devices—a key feature of immersive analytics—facilitating the creation of various analysis scenarios. We evaluated the usability of XROps through a user study and demonstrate its efficacy and usefulness in several example scenarios.&#39;, &#39;uid&#39;: &#39;b859563f-62a0-4ddc-be29-b8e583eb10a7&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;Visualization&#39;, &#39;Programming&#39;, &#39;Real-time systems&#39;, &#39;Three-dimensional displays&#39;, &#39;Encoding&#39;, &#39;Programming profession&#39;, &#39;Workflow management software&#39;, &#39;Training&#39;, &#39;Electronic mail&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.10043&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3546467&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_b859563f-62a0-4ddc-be29-b8e583eb10a7.html"> XROps: A Visual Workflow Management System for Dynamic Immersive Analytics. <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Suemin Jeon, JunYoung Choi, Haejin Jeong, Won-Ki Jeong </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Suemin Jeon </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T14:00:00.000Z &ndash; 2025-11-06T14:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T13:00:00+00:00'
    endTime = '2025-11-06T14:15:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "m2-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>