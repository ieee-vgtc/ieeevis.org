<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Vis & Language"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Vis & Language"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Vis &amp; Language</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Vis &amp; Language</li></ol></nav><h1 class="session-title">VIS Full Papers: Vis &amp; Language</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Vidya Setlur </h3><h3 class="session-room mt-4"> Room: Hall M2 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T10:15:00+00:00 &ndash; 2025-11-06T11:30:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T10:15:00+00:00 &ndash; 2025-11-06T11:30:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full34.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945290818551969" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1029&#39;, &#39;session_id&#39;: &#39;full34&#39;, &#39;title&#39;: &#39;DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models&#39;, &#39;contributors&#39;: [&#39;Liwenhan Xie&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:15:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:15:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T10:27:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Liwenhan Xie&#39;, &#39;email&#39;: &#39;liwenhan.xie@connect.ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Yanna Lin&#39;, &#39;email&#39;: &#39;ylindg@connect.ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Can Liu&#39;, &#39;email&#39;: &#39;can.liu.1996@gmail.com&#39;, &#39;affiliation&#39;: &#39;Nanyang Technological University&#39;}, {&#39;name&#39;: &#39;Huamin Qu&#39;, &#39;email&#39;: &#39;huamin@cse.ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Xinhuan Shu&#39;, &#39;email&#39;: &#39;xinhuan.shu@gmail.com&#39;, &#39;affiliation&#39;: &#39;Newcastle University&#39;}], &#39;abstract&#39;: &#34;Creating aesthetically pleasing data visualizations remains challenging for users without design expertise or familiarity with visualization tools. To address this gap, we present DataWink, a system that enables users to create custom visualizations by adapting high-quality examples. Our approach combines large multimodal models (LMMs) to extract data encoding from existing SVG-based visualization examples, featuring an intermediate representation of visualizations that bridges primitive SVG and visualization programs. Users may express adaptation goals to a conversational agent and control the visual appearance through widgets generated on demand. With an interactive interface, users can modify both data mappings and visual design elements while maintaining the original visualization&#39;s aesthetic quality. To evaluate DataWink, we conduct a user study (N=12) with replication and free-form exploration tasks. As a result, DataWink is recognized for its learnability and effectiveness in personalized authoring tasks. Our results demonstrate the potential of example-driven approaches for democratizing visualization creation.&#34;, &#39;uid&#39;: &#39;30cf9291-b159-4a33-95bd-47f194b893a0&#39;, &#39;keywords&#39;: [&#39;Visualization template&#39;, &#39;Lazy data binding&#39;, &#39;Visualization by example&#39;, &#39;Dynamic abstractions&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.17734&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_30cf9291-b159-4a33-95bd-47f194b893a0.html"> DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Liwenhan Xie, Yanna Lin, Can Liu, Huamin Qu, Xinhuan Shu </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Liwenhan Xie </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:15:00.000Z &ndash; 2025-11-06T10:27:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1349&#39;, &#39;session_id&#39;: &#39;full34&#39;, &#39;title&#39;: &#39;Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models&#39;, &#39;contributors&#39;: [&#39;Weihan Zhang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:27:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:27:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T10:39:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Weihan Zhang&#39;, &#39;email&#39;: &#39;zhangwh79@mail2.sysu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Sun Yat-sen University&#39;}, {&#39;name&#39;: &#39;Jun Tao&#39;, &#39;email&#39;: &#39;taoj23@mail.sysu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Sun Yat-sen University&#39;}], &#39;abstract&#39;: &#39;Explorative flow visualization allows domain experts to analyze complex flow structures by interactively investigating flow patterns. However, traditional visual interfaces often rely on specialized graphical representations and interactions, which require additional effort to learn and use. Natural language interaction offers a more intuitive alternative, but teaching machines to recognize diverse scientific concepts and extract corresponding structures from flow data poses a significant challenge. In this paper, we introduce an automated framework that aligns flow pattern representations with the semantic space of large language models (LLMs), eliminating the need for manual labeling. Our approach encodes streamline segments using a denoising autoencoder and maps the generated flow pattern representations to LLM embeddings via a projector layer. This alignment empowers semantic matching between textual embeddings and flow representations through an attention mechanism, enabling the extraction of corresponding flow patterns based on textual descriptions. To enhance accessibility, we develop an interactive interface that allows users to query and visualize flow structures using natural language. Through case studies, we demonstrate the effectiveness of our framework in enabling intuitive and intelligent flow exploration.&#39;, &#39;uid&#39;: &#39;d207baab-2afc-4d7a-bbfd-cf81768ec83c&#39;, &#39;keywords&#39;: [&#39;Flow visualization&#39;, &#39;natural language&#39;, &#39;streamlines&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2508.06300&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_d207baab-2afc-4d7a-bbfd-cf81768ec83c.html"> Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Weihan Zhang, Jun Tao </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Weihan Zhang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:27:00.000Z &ndash; 2025-11-06T10:39:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1701&#39;, &#39;session_id&#39;: &#39;full34&#39;, &#39;title&#39;: &#39;Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis&#39;, &#39;contributors&#39;: [&#39;Furui Cheng&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:39:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:39:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T10:51:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Furui Cheng&#39;, &#39;email&#39;: &#39;furui.cheng@inf.ethz.ch&#39;, &#39;affiliation&#39;: &#39;ETH Zürich&#39;}, {&#39;name&#39;: &#39;Vilém Zouhar&#39;, &#39;email&#39;: &#39;vzouhar@inf.ethz.ch&#39;, &#39;affiliation&#39;: &#39;ETH Zurich&#39;}, {&#39;name&#39;: &#39;Robin Chan&#39;, &#39;email&#39;: &#39;robin.chan@inf.ethz.ch&#39;, &#39;affiliation&#39;: &#39;ETH Zürich&#39;}, {&#39;name&#39;: &#39;Daniel Fürst&#39;, &#39;email&#39;: &#39;daniel.fuerst@uni-konstanz.de&#39;, &#39;affiliation&#39;: &#39;University of Konstanz&#39;}, {&#39;name&#39;: &#39;Hendrik Strobelt&#39;, &#39;email&#39;: &#39;hendrik@strobelt.com&#39;, &#39;affiliation&#39;: &#39;IBM Research AI&#39;}, {&#39;name&#39;: &#39;Mennatallah El-Assady&#39;, &#39;email&#39;: &#39;melassady@ai.ethz.ch&#39;, &#39;affiliation&#39;: &#39;ETH Zürich&#39;}], &#39;abstract&#39;: &#39;Understanding the behavior of large language models (LLMs) is crucial for ensuring their safe and reliable use. However, existing explainable AI (XAI) methods for LLMs primarily rely on word-level explanations, which are often computationally inefficient and misaligned with human reasoning processes. Moreover, these methods often treat explanation as a one-time output, overlooking its inherently interactive and iterative nature. In this paper, we present LLM Analyzer, an interactive visualization system that addresses these limitations by enabling intuitive and efficient exploration of LLM behaviors through counterfactual analysis. Our system features a novel algorithm that generates fluent and semantically meaningful counterfactuals via targeted removal and replacement operations at user-defined levels of granularity. These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization, supporting dynamic analysis of model behavior. A user study with LLM practitioners and interviews with experts demonstrate the system’s usability and effectiveness, emphasizing the importance of involving humans in the explanation process as active participants rather than passive recipients.&#39;, &#39;uid&#39;: &#39;7a68a26e-4b20-444f-a76a-eee7a08ab3be&#39;, &#39;keywords&#39;: [&#39;Counterfactual&#39;, &#39;Explainable Artificial Intelligence&#39;, &#39;Large Language Model&#39;, &#39;Visualization&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2405.00708&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;The paper contributes an interactive visualization system for analyzing large language model behaviors together with an algorithm for meaning textual counterfactual generation.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_7a68a26e-4b20-444f-a76a-eee7a08ab3be.html"> Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Furui Cheng, Vilém Zouhar, Robin Chan, Daniel Fürst, Hendrik Strobelt, Mennatallah El-Assady </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Furui Cheng </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:39:00.000Z &ndash; 2025-11-06T10:51:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1789&#39;, &#39;session_id&#39;: &#39;full34&#39;, &#39;title&#39;: &#39;The Impact of Visual Segmentation on Lexical Word Recognition&#39;, &#39;contributors&#39;: [&#39;Matthew Termuende&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:51:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:51:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T11:03:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Matthew Termuende&#39;, &#39;email&#39;: &#39;matttermuende@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Victoria&#39;}, {&#39;name&#39;: &#39;Kevin Larson&#39;, &#39;email&#39;: &#39;kevlar@microsoft.com&#39;, &#39;affiliation&#39;: &#39;Microsoft&#39;}, {&#39;name&#39;: &#39;Miguel Nacenta&#39;, &#39;email&#39;: &#39;nacenta@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Victoria&#39;}], &#39;abstract&#39;: &#39;When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing\nits meaning. For example, “rough”, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where\neach group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational\noperation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text\nnecessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible\ngroupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of\ntime in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the\nreading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the\nvisualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered\nlexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections,\nunderlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow\ndown word identification, but each to a different extent (between 32.7ms—color technique—and 70.7ms—connection technique).\nThese findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in\nthese visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement\nof the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within\ntext without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables\npresented unique patterns that deviate from existing theories, suggesting new directions for future inquiry.&#39;, &#39;uid&#39;: &#39;bf10121c-1a25-4391-b766-450e53b2348e&#39;, &#39;keywords&#39;: [&#39;Reading&#39;, &#39;Word Recognition&#39;, &#39;Text Visualization&#39;, &#39;Text Interaction&#39;, &#39;Phonological Cues&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://doi.org/10.17605/OSF.IO/79Y5S&#39;} <h3 class="session-list-title"><a href="paper_bf10121c-1a25-4391-b766-450e53b2348e.html"> The Impact of Visual Segmentation on Lexical Word Recognition <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Matthew Termuende, Kevin Larson, Miguel Nacenta </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Matthew Termuende </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:51:00.000Z &ndash; 2025-11-06T11:03:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1818&#39;, &#39;session_id&#39;: &#39;full34&#39;, &#39;title&#39;: &#39;DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning&#39;, &#39;contributors&#39;: [&#39;Zhihao Shuai&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T11:03:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T11:03:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T11:15:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Zhihao Shuai&#39;, &#39;email&#39;: &#39;zhihaoshuai@hkust-gz.edu.cn&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology (Guangzhou)&#39;}, {&#39;name&#39;: &#39;Boyan LI&#39;, &#39;email&#39;: &#39;bli303@connect.hkust-gz.edu.cn&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology (Guangzhou)&#39;}, {&#39;name&#39;: &#39;siyu yan&#39;, &#39;email&#39;: &#39;syan195@connect.hkust-gz.edu.cn&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology (Guangzhou)&#39;}, {&#39;name&#39;: &#39;Yuyu Luo&#39;, &#39;email&#39;: &#39;yuyuluo@hkust-gz.edu.cn&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology (Guangzhou)&#39;}, {&#39;name&#39;: &#39;Weikai Yang&#39;, &#39;email&#39;: &#39;weikaiyang@hkust-gz.edu.cn&#39;, &#39;affiliation&#39;: &#39;Hong Kong University of Science and Technology (Guangzhou)&#39;}], &#39;abstract&#39;: &#39;Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.&#39;, &#39;uid&#39;: &#39;f4073f80-c0bd-4d83-9c72-3dcfd060ef7f&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;automatic visualization&#39;, &#39;large language models&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/pdf/2508.01700&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/Bvivib-shuai/DeepVIS&#39;} <h3 class="session-list-title"><a href="paper_f4073f80-c0bd-4d83-9c72-3dcfd060ef7f.html"> DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Zhihao Shuai, Boyan LI, siyu yan, Yuyu Luo, Weikai Yang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Zhihao Shuai </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T11:03:00.000Z &ndash; 2025-11-06T11:15:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-2050&#39;, &#39;session_id&#39;: &#39;full34&#39;, &#39;title&#39;: &#39;Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation&#39;, &#39;contributors&#39;: [&#39;Xuan Zhao&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T11:15:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T11:15:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T11:27:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Xuan Zhao&#39;, &#39;email&#39;: &#39;zhaox269@mail2.sysu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Sun Yat-sen University&#39;}, {&#39;name&#39;: &#39;Jun Tao&#39;, &#39;email&#39;: &#39;taoj23@mail.sysu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Sun Yat-sen University&#39;}], &#39;abstract&#39;: &#39;Exploring volumetric data is crucial for interpreting scientific datasets. However, selecting optimal viewpoints for effective navigation can be challenging, particularly for users without extensive domain expertise or familiarity with 3D navigation. In this paper, we propose a novel framework that leverages natural language interaction to enhance volumetric data exploration. Our approach encodes volumetric blocks to capture and differentiate underlying structures. It further incorporates a CLIP Score mechanism, which provides semantic information to the blocks to guide navigation. The navigation is empowered by a reinforcement learning framework that leverage these semantic cues to efficiently search for and identify desired viewpoints that align with the user’s intent. The selected viewpoints are evaluated using CLIP Score to ensure that they best reflect the user queries. By automating viewpoint selection, our method improves the efficiency of volumetric data navigation and enhances the interpretability of complex scientific phenomena.&#39;, &#39;uid&#39;: &#39;3f463b80-5efc-4150-8b6f-3f5fbb1aed72&#39;, &#39;keywords&#39;: [&#39;Volume rendering&#39;, &#39;Viewpoint navigation&#39;, &#39;Natural language interaction&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_3f463b80-5efc-4150-8b6f-3f5fbb1aed72.html"> Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Xuan Zhao, Jun Tao </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Xuan Zhao </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T11:15:00.000Z &ndash; 2025-11-06T11:27:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T10:15:00+00:00'
    endTime = '2025-11-06T11:30:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "m2-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>