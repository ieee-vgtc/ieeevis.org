<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2024/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/1.12/auth0-spa-js.production.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2024/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2024/js/data/persistor.js"></script><script src="/static/2024/js/data/api.js"></script><link rel="shortcut icon" href="/static/2024/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2024/css/Zilla.css" rel="stylesheet"><link href="/static/2024/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2024/css/main.css"><link rel="stylesheet" href="/static/2024/css/fa_solid.css"><link rel="stylesheet" href="/static/2024/css/lazy_load.css"><link rel="stylesheet" href="/static/2024/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2024 - Paper: Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness"><meta name="twitter:description" content="Recent advancements in vision models have significantly enhanced their ability to perform complex chart understanding tasks, such as chart captioning and chart question answering. However, assessing how these models process charts remains challenging. Existing benchmarks only coarsely evaluate how well the model performs the given task without thoroughly evaluating the underlying mechanisms that drive performance, such as how models extract image embeddings. This gap limits our understanding of the model's perceptual capabilities regarding fundamental graphical components. Therefore, we introduce a novel evaluation framework designed to assess the graphical perception of image embedding models. In the context of chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. We first assess channel accuracy through the linearity of embeddings, which is the degree to which the perceived magnitude is proportional to the size of the stimulus. % based on the assumption that perceived magnitude should be proportional to the size of Conversely, distances between embeddings serve as a measure of discriminability; embeddings that are far apart can be considered discriminable. Our experiments on a general image embedding model, CLIP, provided that it perceives channel accuracy differently from humans and demonstrated distinct discriminability in specific channels such as length, tilt, and curvature. We aim to extend our work as a more general benchmark for reliable visual encoders and enhance a model for two distinctive goals for future applications: precise chart comprehension and mimicking human perception."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="Recent advancements in vision models have significantly enhanced their ability to perform complex chart understanding tasks, such as chart captioning and chart question answering. However, assessing how these models process charts remains challenging. Existing benchmarks only coarsely evaluate how well the model performs the given task without thoroughly evaluating the underlying mechanisms that drive performance, such as how models extract image embeddings. This gap limits our understanding of the model's perceptual capabilities regarding fundamental graphical components. Therefore, we introduce a novel evaluation framework designed to assess the graphical perception of image embedding models. In the context of chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. We first assess channel accuracy through the linearity of embeddings, which is the degree to which the perceived magnitude is proportional to the size of the stimulus. % based on the assumption that perceived magnitude should be proportional to the size of Conversely, distances between embeddings serve as a measure of discriminability; embeddings that are far apart can be considered discriminable. Our experiments on a general image embedding model, CLIP, provided that it perceives channel accuracy differently from humans and demonstrated distinct discriminability in specific channels such as length, tilt, and curvature. We aim to extend our work as a more general benchmark for reliable visual encoders and enhance a model for two distinctive goals for future applications: precise chart comprehension and mimicking human perception."><meta name="title" property="og:title" content="Virtual IEEE VIS 2024 - Paper: Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness"><meta property="og:type" content="website"><title>IEEE VIS 2024 Content: Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness</title></head> <body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style> <div class="container mb-5"> <div class="tabs"> </div> <div class="content"> <div class="row mt-3"> <div class="col-md-12"> <nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"> <ol class="breadcrumb"> <li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a> </li> <li class="breadcrumb-item"><a href="session_short0.html">Short Papers</a> </li> <li class="breadcrumb-item active text-truncate" aria-current="page">Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness</li> </ol> </nav> <h1 class="paper-title">Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness</h1> <div class="checkbox-bookmark fas" style="font-size: 24pt;position: absolute; top:10px; right:20px;" data-tippy-content="(un-)bookmark this paper"> &#xf02e; </div> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> <a href="mailto:dtngus0111@gmail.com">Soohyun Lee</a> - Seoul National University, Seoul, Korea, Republic of </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Minsuk Chang - Seoul National University, Seoul, Korea, Republic of </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Seokhyeon Park - Seoul National University, Seoul, Korea, Republic of </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Jinwook Seo - Seoul National University, Seoul, Korea, Republic of </h4> <h3 class="session-room mt-4"> <span class="fas mr-1">&#xf108;</span> Room: To Be Announced </h3> </div> </div> <div class="row my-3"> <div class="col-md-8"> <figure class="figure"> <figcaption class="figure-caption" id="figure-caption"></figcaption> </figure> </div> </div> <div class="row my-3"> <div class="col-md-8"> </div> </div> <div class="row my-3"> <div class="col-md-8"> <h5 class="paper-details-heading">Abstract</h5> <p>Recent advancements in vision models have significantly enhanced their ability to perform complex chart understanding tasks, such as chart captioning and chart question answering. However, assessing how these models process charts remains challenging. Existing benchmarks only coarsely evaluate how well the model performs the given task without thoroughly evaluating the underlying mechanisms that drive performance, such as how models extract image embeddings. This gap limits our understanding of the model&#39;s perceptual capabilities regarding fundamental graphical components. Therefore, we introduce a novel evaluation framework designed to assess the graphical perception of image embedding models. In the context of chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. We first assess channel accuracy through the linearity of embeddings, which is the degree to which the perceived magnitude is proportional to the size of the stimulus. % based on the assumption that perceived magnitude should be proportional to the size of Conversely, distances between embeddings serve as a measure of discriminability; embeddings that are far apart can be considered discriminable. Our experiments on a general image embedding model, CLIP, provided that it perceives channel accuracy differently from humans and demonstrated distinct discriminability in specific channels such as length, tilt, and curvature. We aim to extend our work as a more general benchmark for reliable visual encoders and enhance a model for two distinctive goals for future applications: precise chart comprehension and mimicking human perception.</p> </div> </div> <script lang="js">
      const paperID = "v-short-1072"
      $(document).ready(() => {
        tippy('[data-tippy-content]');

        const allBookmarks =
          d3.selectAll('.checkbox-bookmark')
            .on("click", function () {
              const newValue = !d3.select(this).classed('selected');
              API.markSet(API.storeIDs.bookmarked, paperID, newValue);
              d3.select(this).classed('selected', newValue);
            })
        API.markGet(API.storeIDs.bookmarked, paperID).then(is_bookmarked => {
          is_bookmarked = !!is_bookmarked;
          allBookmarks.classed('selected', is_bookmarked);
        })
        API.markSet(API.storeIDs.visited, paperID, true);

      })

    </script> <script src="/static/2024/js/views/timezone.js"></script> </div> </div> <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" â€“ ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script> </body> </html>