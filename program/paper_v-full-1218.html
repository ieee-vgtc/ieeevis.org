<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2024/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/1.12/auth0-spa-js.production.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2024/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2024/js/data/persistor.js"></script><script src="/static/2024/js/data/api.js"></script><link rel="shortcut icon" href="/static/2024/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2024/css/Zilla.css" rel="stylesheet"><link href="/static/2024/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2024/css/main.css"><link rel="stylesheet" href="/static/2024/css/fa_solid.css"><link rel="stylesheet" href="/static/2024/css/lazy_load.css"><link rel="stylesheet" href="/static/2024/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2024 - Paper: Graph Transformer for Label Placement"><meta name="twitter:description" content="Placing text labels is a common way to explain key elements in a given scene. Given a graphic input and original label information, how to place labels to meet both geometric and aesthetic requirements is an open challenging problem. Geometry-wise, traditional rule-driven solutions struggle to capture the complex interactions between labels, let alone consider graphical/appearance content. In terms of aesthetics, training/evaluation data ideally require nontrivial effort and expertise in design, thus resulting in a lack of decent datasets for learning-based methods. To address the above challenges, we formulate the task with a graph representation, where nodes correspond to labels and edges to interactions between labels, and treat label placement as a node position prediction problem. With this novel representation, we design a Label Placement Graph Transformer (LPGT) to predict label positions. Specifically, edge-level attention, conditioned on node representations, is introduced to reveal potential relationships between labels. To integrate graphic/image information, we design a feature aligning strategy that extracts deep features for nodes and edges efficiently. Next, to address the dataset issue, we collect commercial illustrations with professionally designed label layouts from household appliance manuals, and annotate them with useful information to create a novel dataset named the Appliance Manual Illustration Labels (AMIL) dataset. In the thorough evaluation on AMIL, our LPGT solution achieves promising label placement performance compared with popular baselines. Our algorithm is available at https://github.com/JingweiQu/LPGT."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2024/paper_images/v-full-1218_Image.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2024/paper_images/v-full-1218_Image.png"><meta name="description" property="og:description" content="Placing text labels is a common way to explain key elements in a given scene. Given a graphic input and original label information, how to place labels to meet both geometric and aesthetic requirements is an open challenging problem. Geometry-wise, traditional rule-driven solutions struggle to capture the complex interactions between labels, let alone consider graphical/appearance content. In terms of aesthetics, training/evaluation data ideally require nontrivial effort and expertise in design, thus resulting in a lack of decent datasets for learning-based methods. To address the above challenges, we formulate the task with a graph representation, where nodes correspond to labels and edges to interactions between labels, and treat label placement as a node position prediction problem. With this novel representation, we design a Label Placement Graph Transformer (LPGT) to predict label positions. Specifically, edge-level attention, conditioned on node representations, is introduced to reveal potential relationships between labels. To integrate graphic/image information, we design a feature aligning strategy that extracts deep features for nodes and edges efficiently. Next, to address the dataset issue, we collect commercial illustrations with professionally designed label layouts from household appliance manuals, and annotate them with useful information to create a novel dataset named the Appliance Manual Illustration Labels (AMIL) dataset. In the thorough evaluation on AMIL, our LPGT solution achieves promising label placement performance compared with popular baselines. Our algorithm is available at https://github.com/JingweiQu/LPGT."><meta name="title" property="og:title" content="Virtual IEEE VIS 2024 - Paper: Graph Transformer for Label Placement"><meta property="og:type" content="website"><title>IEEE VIS 2024 Content: Graph Transformer for Label Placement</title></head> <body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style> <div class="container mb-5"> <div class="tabs"> </div> <div class="content"> <div class="row mt-3"> <div class="col-md-12"> <nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"> <ol class="breadcrumb"> <li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a> </li> <li class="breadcrumb-item"><a href="session_full31.html">Visual Design: Sketching and Labeling</a> </li> <li class="breadcrumb-item active text-truncate" aria-current="page">Graph Transformer for Label Placement</li> </ol> </nav> <h1 class="paper-title">Graph Transformer for Label Placement</h1> <div class="checkbox-bookmark fas" style="font-size: 24pt;position: absolute; top:10px; right:20px;" data-tippy-content="(un-)bookmark this paper"> &#xf02e; </div> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> <a href="mailto:qujingwei@swu.edu.cn">Jingwei Qu</a> - Southwest University, Beibei, China </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Pingshun Zhang - Southwest University, Chongqing, China </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Enyu Che - Southwest University, Beibei, China </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Yinan Chen - COLLEGE OF COMPUTER AND INFORMATION SCIENCE, SOUTHWEST UNIVERSITY SCHOOL OF SOFTWAREC, Chongqin, China </h4> <h4 class="paper-authors pb-2 mt-2"> <span class="fas mr-1">&#xf183;</span> Haibin Ling - Stony Brook University, New York, United States </h4> <h5 class="paper-link pb-2"> <a href="https://ieeevis.b-cdn.net/vis_2024/pdfs/v-full-1218.pdf"> <span class="fas mr-1">&#xf15c;</span> Download camera-ready PDF </a> </h5> <h3 class="session-room mt-4"> <span class="fas mr-1">&#xf108;</span> <a href="room_bayshore2.html"> Room: Bayshore II </a> </h3> <h5 class="paper-presentation pb-2"> <span class="format-date">2024-10-17T15:03:00Z</span> <span alt="Change timezone on schedule page" class="timezone tztooltip"> <strong>GMT<span class="selectedTimezone">-0600</span></strong> <span class="tztooltiptext">Change your timezone on the schedule page</span> </span> <br> <span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"> <span class="relative-time">2024-10-17T15:03:00Z</span> <span class="current-time tztooltiptext"></span> </span> </h5> </div> </div> <div class="row my-3"> <div class="col-md-8"> <figure class="figure"> <img class="figure-img img-fluid" src="https://ieeevis.b-cdn.net/vis_2024/paper_images/v-full-1218_Image.png" alt="Exemplar figure, described by caption below" aria-describedby="figure-caption"> <figcaption class="figure-caption" id="figure-caption">GNN-driven label placement. For a set of labels to be placed in a graphic, Label Placement Graph Transformer (LPGT) predicts the label layout given the graphic and raw label information. First, a complete graph is constructed to capture the relationship between labels. Its node and edge features are generated from the label information and image features. Next, given the graph as input, LPGT iteratively learns the displacements of the nodes by a sequence of GNN modules. The graph is updated by each module and taken as input for the next module.</figcaption> </figure> </div> </div> <div class="row my-3"> <div class="col-md-8"> </div> </div> <div class="row my-3"> <div class="col-md-8"> <h5 class="paper-details-heading">Fast forward</h5> <video controls width="720" crossorigin="anonymous"> <source src="https://ieeevis-uploads.b-cdn.net/vis24/v-full/v-full-1218/v-full-1218_Preview.mp4?token=V_BqaYvYHAVkbDe2N4Q-tw9rdfS_5RQGXLTQGgkspUk&expires=1730433600" type="video/mp4"> <track label="English" kind="subtitles" srclang="en" src default> </video> </div> </div> <div class="row my-3"> <div class="col-md-8"> <h5 class="paper-details-heading">Keywords</h5> <p>Label placement, Graph neural network, Transformer</p> <h5 class="paper-details-heading">Abstract</h5> <p>Placing text labels is a common way to explain key elements in a given scene. Given a graphic input and original label information, how to place labels to meet both geometric and aesthetic requirements is an open challenging problem. Geometry-wise, traditional rule-driven solutions struggle to capture the complex interactions between labels, let alone consider graphical/appearance content. In terms of aesthetics, training/evaluation data ideally require nontrivial effort and expertise in design, thus resulting in a lack of decent datasets for learning-based methods. To address the above challenges, we formulate the task with a graph representation, where nodes correspond to labels and edges to interactions between labels, and treat label placement as a node position prediction problem. With this novel representation, we design a Label Placement Graph Transformer (LPGT) to predict label positions. Specifically, edge-level attention, conditioned on node representations, is introduced to reveal potential relationships between labels. To integrate graphic/image information, we design a feature aligning strategy that extracts deep features for nodes and edges efficiently. Next, to address the dataset issue, we collect commercial illustrations with professionally designed label layouts from household appliance manuals, and annotate them with useful information to create a novel dataset named the Appliance Manual Illustration Labels (AMIL) dataset. In the thorough evaluation on AMIL, our LPGT solution achieves promising label placement performance compared with popular baselines. Our algorithm is available at https://github.com/JingweiQu/LPGT.</p> </div> </div> <script lang="js">
      const paperID = "v-full-1218"
      $(document).ready(() => {
        tippy('[data-tippy-content]');

        const allBookmarks =
          d3.selectAll('.checkbox-bookmark')
            .on("click", function () {
              const newValue = !d3.select(this).classed('selected');
              API.markSet(API.storeIDs.bookmarked, paperID, newValue);
              d3.select(this).classed('selected', newValue);
            })
        API.markGet(API.storeIDs.bookmarked, paperID).then(is_bookmarked => {
          is_bookmarked = !!is_bookmarked;
          allBookmarks.classed('selected', is_bookmarked);
        })
        API.markSet(API.storeIDs.visited, paperID, true);

      })

    </script> <script src="/static/2024/js/views/timezone.js"></script> </div> </div> <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" â€“ ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script> </body> </html>