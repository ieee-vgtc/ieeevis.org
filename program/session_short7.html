<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Short Papers: Perception & Semantics"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Short Papers: Perception & Semantics"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Short Papers: Perception &amp; Semantics</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Perception &amp; Semantics</li></ol></nav><h1 class="session-title">VIS Short Papers: Perception &amp; Semantics</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-short.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-short.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Szafir, Danielle Albers </h3><h3 class="session-room mt-4"> Room: Room 1.14 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T14:45:00+00:00 &ndash; 2025-11-06T16:00:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T14:45:00+00:00 &ndash; 2025-11-06T16:00:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/short7.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945285051519147" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1262&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation&#39;, &#39;contributors&#39;: [&#39;George Bell&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T14:45:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T14:45:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T14:54:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;George Bell&#39;, &#39;email&#39;: &#39;g.bell1@newcastle.ac.uk&#39;, &#39;affiliation&#39;: &#39;Newcastle University&#39;}, {&#39;name&#39;: &#39;Alma Cantu&#39;, &#39;email&#39;: &#39;alma.cantu@ncl.ac.uk&#39;, &#39;affiliation&#39;: &#39;Newcastle University&#39;}], &#39;abstract&#39;: &#34;Adjusting transparency is a common method of mitigating occlusion but is often detrimental for understanding the relative depth relationships between objects as well as removes potentially important information from the occluding object. We propose using dichoptic opacity, a novel method for occlusion management that contrasts the transparency of occluders presented to each eye. This allows for better simultaneous understanding of both occluder and occluded. A user study highlights the technique&#39;s potential, showing strong user engagement and a clear preference for dichoptic opacity over traditional presentations. While it does not determine optimal transparency values, it reveals promising trends in both percentage and range that merit further investigation.&#34;, &#39;uid&#39;: &#39;1a6e079e-6775-4742-80e6-1bfa9f98605d&#39;, &#39;keywords&#39;: [&#39;Occlusion&#39;, &#39;dichoptic presentation&#39;, &#39;transparency&#39;, &#39;3D surface&#39;, &#39;stereoscopy&#39;], &#39;preprint_link&#39;: &#39;https://doi.org/10.48550/arXiv.2506.22841&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/georgebellbell/Dichoptic-Opacity-Experiment-Data.git&#39;} <h3 class="session-list-title"><a href="paper_1a6e079e-6775-4742-80e6-1bfa9f98605d.html"> Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: George Bell, Alma Cantu </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> George Bell </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T14:45:00.000Z &ndash; 2025-11-06T14:54:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1118&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;From Perception to Decision: Assessing the Role of Chart Type Affordances in High-Level Decision Tasks&#39;, &#39;contributors&#39;: [&#39;Yixuan Li&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T14:54:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T14:54:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:03:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yixuan Li&#39;, &#39;email&#39;: &#39;yixuanli@gatech.edu&#39;, &#39;affiliation&#39;: &#39;Georgia Institute of Technology&#39;}, {&#39;name&#39;: &#39;Emery Berger&#39;, &#39;email&#39;: &#39;emery@cs.umass.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts Amherst&#39;}, {&#39;name&#39;: &#39;Minsuk Kahng&#39;, &#39;email&#39;: &#39;minsuk.kahng@gmail.com&#39;, &#39;affiliation&#39;: &#39;Yonsei University&#39;}, {&#39;name&#39;: &#39;Cindy Xiong Bearfield&#39;, &#39;email&#39;: &#39;cxiong@gatech.edu&#39;, &#39;affiliation&#39;: &#39;Georgia Tech&#39;}], &#39;abstract&#39;: &#39;Visualization design influences how people perceive data patterns, yet most research focuses on low-level analytic tasks, such as finding correlations. The extent to which these perceptual affordances translate to high-level decision-making in the real world remains underexplored. Through a case study of academic mentorship selection using bar charts and pie charts, we investigated whether chart types differentially influence how students evaluate faculty research profiles. Our crowdsourced experiment revealed only minimal differences in decision outcomes between chart types, suggesting that perceptual affordances established in controlled analytical tasks may not directly translate to high-level decision scenarios. These findings emphasize the importance of evaluating visualizations within real-world contexts and highlight the need to distinguish between perceptual and decision affordances when developing visualization guidelines.&#39;, &#39;uid&#39;: &#39;787f3aab-3371-4bf1-94c6-9141cd0db272&#39;, &#39;keywords&#39;: [&#39;Visual Affordances&#39;, &#39;Decision-Making&#39;, &#39;Perception&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2410.04686&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/gfuwp/&#39;} <h3 class="session-list-title"><a href="paper_787f3aab-3371-4bf1-94c6-9141cd0db272.html"> From Perception to Decision: Assessing the Role of Chart Type Affordances in High-Level Decision Tasks <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yixuan Li, Emery Berger, Minsuk Kahng, Cindy Xiong Bearfield </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yixuan Li </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T14:54:00.000Z &ndash; 2025-11-06T15:03:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1272&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Comparing Pre-attentive Visual Variables in a Target Identification Task for Glanceable Visualizations&#39;, &#39;contributors&#39;: [&#39;Fairouz Grioui&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:03:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:03:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Fairouz Grioui&#39;, &#39;email&#39;: &#39;fairouz.grioui@vis.uni-stuttgart.de&#39;, &#39;affiliation&#39;: &#39;University of Stuttgart&#39;}, {&#39;name&#39;: &#39;Yasmin Amzir&#39;, &#39;email&#39;: &#39;st166324@stud.uni-stuttgart.de&#39;, &#39;affiliation&#39;: &#39;Universität Stuttgart&#39;}, {&#39;name&#39;: &#39;Nina Doerr&#39;, &#39;email&#39;: &#39;nina.doerr@visus.uni-stuttgart.de&#39;, &#39;affiliation&#39;: &#39;University of Stuttgart&#39;}, {&#39;name&#39;: &#39;Tanja Blascheck&#39;, &#39;email&#39;: &#39;research@blascheck.eu&#39;, &#39;affiliation&#39;: &#39;University of Stuttgart&#39;}], &#39;abstract&#39;: &#39;In this paper, we evaluated the use of three pre-attentive visual variables, namely, area, color hue, and flicker, to highlight elements on four charts: bar chart, line chart, linear progress chart, and radial progress chart. We exposed 48 participants---using short time lapses of 250 ms---to a set of stimuli showing one of the four charts with the three visual variables. Then we examined the accuracy of their answers in identifying the highlighted target element under sedentary and walking conditions. Our results show that all three visual variables can be perceived with a medium to high accuracy rate, though performance varies depending on the mobility condition, the visual variable, and even the chart type. Among the visual variables, color and flicker yield the best target identification accuracy rates, while the area proves to be the least effective. The findings from our first investigation contribute to the design of efficient smartwatch micro visualizations that support glanceable and in-motion data reading.&#39;, &#39;uid&#39;: &#39;60b8dd96-21c8-4a3d-8e8b-b50374e1230e&#39;, &#39;keywords&#39;: [&#39;pre-attentive visual variables&#39;, &#39;micro visualization&#39;, &#39;mobile visualizations&#39;, &#39;smartwatch.&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/n348a/?view_only=139b008328dd44708945ed6fab310a1a&#39;} <h3 class="session-list-title"><a href="paper_60b8dd96-21c8-4a3d-8e8b-b50374e1230e.html"> Comparing Pre-attentive Visual Variables in a Target Identification Task for Glanceable Visualizations <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Fairouz Grioui, Yasmin Amzir, Nina Doerr, Tanja Blascheck </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Fairouz Grioui </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:03:00.000Z &ndash; 2025-11-06T15:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1152&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Visualizing on the Wrist: Impact of Motion, Dial Shape and Visualization Type on Smartwatch&#39;, &#39;contributors&#39;: [&#39;Yu Liu&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:12:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:12:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:21:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Zhouxuan Xia&#39;, &#39;email&#39;: &#39;zhouxuan.xia21@student.xjtlu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Xi’an Jiaotong-Liverpool University&#39;}, {&#39;name&#39;: &#39;Fengyuan Liao&#39;, &#39;email&#39;: &#39;fengyuan.liao22@student.xjtlu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Xi’an Jiaotong-Liverpool University&#39;}, {&#39;name&#39;: &#39;Jinyuan Du&#39;, &#39;email&#39;: &#39;jinyuan.du18@student.xjtlu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Xi’an Jiaotong-Liverpool University&#39;}, {&#39;name&#39;: &#39;Yu Liu&#39;, &#39;email&#39;: &#39;yu.liu02@xjtlu.edu.cn&#39;, &#39;affiliation&#39;: &#34;Xi&#39;an Jiaotong-Liverpool University&#34;}], &#39;abstract&#39;: &#39;Smartwatches increasingly serve as daily data analysis tools, yet their visualization designs face unique challenges: small screens, diverse dial shapes (round/square), and mobile usage contexts (sitting/walking). These factors may interact in unexpected ways, potentially compromising data readability during real-world use. Through a controlled experiment (N=32) comparing bar vs. radial visualizations across dial shapes and motion states, we found: (1) walking significantly reduces estimation accuracy, (2) bar charts consistently outperform radial variants by better supporting linear perception, and (3) radial bar graphs on square dials create particularly poor affordances than other combination. Our work provides the empirical evidence for how smartwatch dial factors, usage contexts and visualization design jointly impact data interpretation, offering concrete design guidelines for smartwatch visualization. Supplemental material is available at https://osf.io/k7fa3/.&#39;, &#39;uid&#39;: &#39;918b3f1e-6e9b-43f0-bb8b-5bad6b3956ef&#39;, &#39;keywords&#39;: [&#39;Smartwatch Visualization&#39;, &#39;Mobile Visualization&#39;, &#39;Human-Computer Interaction&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/k7fa3/&#39;} <h3 class="session-list-title"><a href="paper_918b3f1e-6e9b-43f0-bb8b-5bad6b3956ef.html"> Visualizing on the Wrist: Impact of Motion, Dial Shape and Visualization Type on Smartwatch <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Zhouxuan Xia, Fengyuan Liao, Jinyuan Du, Yu Liu </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yu Liu </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:12:00.000Z &ndash; 2025-11-06T15:21:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1314&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Evolving CAM16-UCS for Modular Accessible Perceptibility and Simultaneous Contrast Detection Tool for Geovisualization&#39;, &#39;contributors&#39;: [&#39;P. William Limpisathian&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:21:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:21:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:30:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;P. William Limpisathian&#39;, &#39;email&#39;: &#39;limpisathian@wisc.edu&#39;, &#39;affiliation&#39;: &#39;University of Wisconsin-Madison&#39;}, {&#39;name&#39;: &#39;Susannah Cox&#39;, &#39;email&#39;: &#39;sccox@wisc.edu&#39;, &#39;affiliation&#39;: &#39;University of Wisconsin - Madison&#39;}], &#39;abstract&#39;: &#39;Accessibility is a foundational principle of effective visualization design, yet achieving perceptual accessibility remains a challenge for both experts and non-experts. Standards like the WCAG2 contrast ratio are intended to guide designers toward accessible color use but are increasingly misapplied as oversimplified and misunderstood shortcuts for achieving perceptual accessibility. This misuse highlights the need for more nuanced, perceptually grounded evaluation tools. In response, we present our Modular Accessible Perceptibility (MAP) contrast model and tool, which integrates advanced color appearance models to assess perceptibility and flag risks associated with simultaneous contrast. By augmenting the CAM16-UCS model with a CAM16 CCz simultaneous chromatic contrast calculation, our model can isolate and measure the predicted shift resulting from simultaneous contrast along the chroma axis. This enables a more refined evaluation of visual contrast that better reflects the complexities of human color perception. Additionally, our tool performs modularly segmented automated checks to assign MAP contrast index scores, offering a holistic assessment of a map’s visual accessibility. This paper details the development and validation of the MAP model and tool, underscoring accessibility and inclusive design as fundamental rights, and demonstrating their utility in advancing perceptual accessibility in geovisualization.&#39;, &#39;uid&#39;: &#39;adaa08c8-ae8e-42b3-aeb5-506a2afe1dd1&#39;, &#39;keywords&#39;: [&#39;accessibility&#39;, &#39;perception&#39;, &#39;cartography&#39;, &#39;design&#39;, &#39;WCAG&#39;, &#39;CIE&#39;, &#39;color appearance model&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://neurocarto.geography.wisc.edu/2025/05/30/maptoolkit/&#39;} <h3 class="session-list-title"><a href="paper_adaa08c8-ae8e-42b3-aeb5-506a2afe1dd1.html"> Evolving CAM16-UCS for Modular Accessible Perceptibility and Simultaneous Contrast Detection Tool for Geovisualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: P. William Limpisathian, Susannah Cox </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> P. William Limpisathian </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:21:00.000Z &ndash; 2025-11-06T15:30:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1328&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Semantic Priming in Word Clouds&#39;, &#39;contributors&#39;: [&#39;Jake Jasmer&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:39:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Xingyi Zhang&#39;, &#39;email&#39;: &#39;xingyiz@stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#39;Jake Jasmer&#39;, &#39;email&#39;: &#39;jakejasmer@gmail.com&#39;, &#39;affiliation&#39;: &#39;Carleton College&#39;}, {&#39;name&#39;: &#39;Ethan Masadde&#39;, &#39;email&#39;: &#39;masaddee@carleton.edu&#39;, &#39;affiliation&#39;: &#39;Carleton College&#39;}, {&#39;name&#39;: &#39;Eric Alexander&#39;, &#39;email&#39;: &#39;ealexander@carleton.edu&#39;, &#39;affiliation&#39;: &#39;Carleton College&#39;}], &#39;abstract&#39;: &#34;While word clouds pack a lot of data into a relatively small area, it is unclear whether readers actually benefit from all of that information, or if they are only processing a few of the words shown. We sought to determine if words outside of a reader&#39;s central vision were contributing to their interpretation by leveraging the semantic priming effect: a phenomenon in which participants will more quickly recognize a word if they have been recently primed with a word that is semantically related. We presented participants with word clouds containing related and unrelated words to see whether they might prime this lexical decision task more strongly than single words alone. We showed that the peripheral contents of a cloud do affect participant performance at this task, though more work is needed to understand the impacts of these differences in real-world settings.&#34;, &#39;uid&#39;: &#39;deb36a29-0227-4522-a7d0-42623a49081e&#39;, &#39;keywords&#39;: [&#39;word clouds&#39;, &#39;semantic priming&#39;, &#39;text perception&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_deb36a29-0227-4522-a7d0-42623a49081e.html"> Semantic Priming in Word Clouds <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Xingyi Zhang, Jake Jasmer, Ethan Masadde, Eric Alexander </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jake Jasmer </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:30:00.000Z &ndash; 2025-11-06T15:39:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1227&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Texture Semantics is Robust to Scaling&#39;, &#39;contributors&#39;: [&#39;Zoe S. Howard&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:39:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:39:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Zoe S. Howard&#39;, &#39;email&#39;: &#39;zoeshoward@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Wisconsin - Madison&#39;}, {&#39;name&#39;: &#39;Karen B. Schloss&#39;, &#39;email&#39;: &#39;kschloss@wisc.edu&#39;, &#39;affiliation&#39;: &#39;University of Wisconsin - Madison&#39;}], &#39;abstract&#39;: &#39;Studies of visual semantics for information visualization aim to understand observers’ expectations about the meaning of visual features (e.g., color, texture) because visualizations that align with those expectations are easier to interpret. Previous work on visual semantics focused primarily on color, with the implicit assumption that color semantics is unaffected by changes in the size of the visualization (given sufficient perceptual discriminability across sizes). Changing size from small scale (e.g., small figures in a paper) to large scale (e.g., large figures in a slide presentation) is straightforward for visualizations that have solid colored regions, but can be more complicated for visualizations with heterogeneous textures because there are multiple ways to scale textures—zooming or repeating texture elements. Previous work suggested that original textures were more perceptually similar to repeat-scaled rather than zoom-scaled textures. Here, we found that texture semantics was preserved after both types of enlargement, suggesting that texture semantics is robust to scaling, at least for geometric textures in which elements are visible at all scales.&#39;, &#39;uid&#39;: &#39;64c1bc8e-6529-47d5-8f6e-c194f4766ec2&#39;, &#39;keywords&#39;: [&#39;information visualization&#39;, &#39;perceptual semantics&#39;, &#39;visual reasoning&#39;, &#39;texture perception&#39;, &#39;visual communication&#39;], &#39;preprint_link&#39;: &#39;https://osf.io/preprints/osf/4uadx_v1&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;The experiment stimuli, data, and analysis code are posted on OSF&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/s7ghe/?view_only=66b92b5dd3e74ee3a231e9b4c9e786a6&#39;} <h3 class="session-list-title"><a href="paper_64c1bc8e-6529-47d5-8f6e-c194f4766ec2.html"> Texture Semantics is Robust to Scaling <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Zoe S. Howard, Karen B. Schloss </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Zoe S. Howard </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:39:00.000Z &ndash; 2025-11-06T15:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1332&#39;, &#39;session_id&#39;: &#39;short7&#39;, &#39;title&#39;: &#39;Global Extrema Bias Perception and Recall of Average Data Values in Line Charts&#39;, &#39;contributors&#39;: [&#39;Cindy Xiong Bearfield&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T15:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T15:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T15:57:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Tejas Savalia&#39;, &#39;email&#39;: &#39;tsavalia@umass.edu&#39;, &#39;affiliation&#39;: &#39;Emory University&#39;}, {&#39;name&#39;: &#39;Andrew Lovett&#39;, &#39;email&#39;: &#39;andrew.lovett@nrl.navy.mil&#39;, &#39;affiliation&#39;: &#39;US Naval Research Lab&#39;}, {&#39;name&#39;: &#39;Cristina Ceja&#39;, &#39;email&#39;: &#39;crceja@u.northwestern.edu&#39;, &#39;affiliation&#39;: &#39;Northwestern University&#39;}, {&#39;name&#39;: &#39;Rosemary Cowell&#39;, &#39;email&#39;: &#39;rosie.cowell@colorado.edu&#39;, &#39;affiliation&#39;: &#39;University of Colorado Boulder&#39;}, {&#39;name&#39;: &#39;Cindy Xiong Bearfield&#39;, &#39;email&#39;: &#39;cxiong@gatech.edu&#39;, &#39;affiliation&#39;: &#39;Georgia Tech&#39;}], &#39;abstract&#39;: &#39;Experiments in visualization perception demonstrate that people can perceive positions highly accurately. However, position encodings can be susceptible to systematic biases depending on the intrinsic properties of the visualized data, such as its shape and cognitive processes of memory and perception. Using line charts as a case study, we investigate how the shape of data, such as local and global extrema, can bias the perception and recall of average data values. In two studies, participants estimated the average data values in a line chart by adjusting a slider with their mouse. We found that participants’ estimates were systematically biased toward the direction of the global extremum. When multiple salient extrema were present, estimates appeared influenced by several extrema simultaneously but ultimately leaned toward the global extremum. \nNotably, the strength of this bias varied depending on whether participants were perceiving or recalling the mean. This work advances our understanding of how extrema influence perception and memory, potentially exaggerating or underestimating critical trends and contributing to a skewed interpretation of data. These findings offer valuable guidance for the design of narrative visualization tools and data storytelling strategies.&#39;, &#39;uid&#39;: &#39;aa5d2960-377f-4f57-a6a1-0cb8c110af24&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;Memory&#39;, &#39;Perception&#39;, &#39;Shape&#39;, &#39;Global Maxima and Minima&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_aa5d2960-377f-4f57-a6a1-0cb8c110af24.html"> Global Extrema Bias Perception and Recall of Average Data Values in Line Charts <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Tejas Savalia, Andrew Lovett, Cristina Ceja, Rosemary Cowell, Cindy Xiong Bearfield </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Cindy Xiong Bearfield </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T15:48:00.000Z &ndash; 2025-11-06T15:57:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-short.html">VIS Short Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T14:45:00+00:00'
    endTime = '2025-11-06T16:00:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "1_14-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>