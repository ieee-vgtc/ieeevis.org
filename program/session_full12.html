<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Explanation, Exploration, and Model Configuration"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Explanation, Exploration, and Model Configuration"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Explanation, Exploration, and Model Configuration</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Explanation, Exploration, and Model Configuration</li></ol></nav><h1 class="session-title">VIS Full Papers: Explanation, Exploration, and Model Configuration</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Timo Ropinski </h3><h3 class="session-room mt-4"> Room: Hall E1 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T08:30:00+00:00 &ndash; 2025-11-06T09:45:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T08:30:00+00:00 &ndash; 2025-11-06T09:45:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full12.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945308053078097" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1416&#39;, &#39;session_id&#39;: &#39;full12&#39;, &#39;title&#39;: &#39;VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models&#39;, &#39;contributors&#39;: [&#39;Huanchen Wang&#39;, &#39;Wencheng Zhang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T08:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Huanchen Wang&#39;, &#39;email&#39;: &#39;wanghc1999@gmail.com&#39;, &#39;affiliation&#39;: &#39;Southern University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Wencheng Zhang&#39;, &#39;email&#39;: &#39;zhangwc2024@mail.sustech.edu&#39;, &#39;affiliation&#39;: &#39;Southern University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Zhiqiang Wang&#39;, &#39;email&#39;: &#39;wangzq_2021@outlook.com&#39;, &#39;affiliation&#39;: &#39;Southern University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Zhicong Lu&#39;, &#39;email&#39;: &#39;zlu6@gmu.edu&#39;, &#39;affiliation&#39;: &#39;George Mason University&#39;}, {&#39;name&#39;: &#39;Yuxin Ma&#39;, &#39;email&#39;: &#39;mayx@sustech.edu.cn&#39;, &#39;affiliation&#39;: &#39;Southern University of Science and Technology&#39;}], &#39;abstract&#39;: &#39;Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.&#39;, &#39;uid&#39;: &#39;e8ed00a7-2d3b-46b8-9edd-6853dd9c2f8c&#39;, &#39;keywords&#39;: [&#39;Visual analytics&#39;, &#39;multi-modal model&#39;, &#39;corruption robustness&#39;, &#39;image captioning&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_e8ed00a7-2d3b-46b8-9edd-6853dd9c2f8c.html"> VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Huanchen Wang, Wencheng Zhang, Zhiqiang Wang, Zhicong Lu, Yuxin Ma </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Huanchen Wang, Wencheng Zhang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:30:00.000Z &ndash; 2025-11-06T08:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1432&#39;, &#39;session_id&#39;: &#39;full12&#39;, &#39;title&#39;: &#39;Cluster-Based Random Forest Visualization and Interpretation&#39;, &#39;contributors&#39;: [&#39;Christofer Meinecke&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:42:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:42:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T08:54:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Max Sondag&#39;, &#39;email&#39;: &#39;max.sondag@hotmail.com&#39;, &#39;affiliation&#39;: &#39;Maastricht University&#39;}, {&#39;name&#39;: &#39;Christofer Meinecke&#39;, &#39;email&#39;: &#39;cmeinecke@informatik.uni-leipzig.de&#39;, &#39;affiliation&#39;: &#39;Leipzig University&#39;}, {&#39;name&#39;: &#39;Dennis Collaris&#39;, &#39;email&#39;: &#39;d.a.c.collaris@tue.nl&#39;, &#39;affiliation&#39;: &#39;Eindhoven University of Technology&#39;}, {&#39;name&#39;: &#39;Tatiana von Landesberger&#39;, &#39;email&#39;: &#39;landesberger@cs.uni-koeln.de&#39;, &#39;affiliation&#39;: &#39;University of Cologne&#39;}, {&#39;name&#39;: &#39;Stef van den Elzen&#39;, &#39;email&#39;: &#39;s.j.v.d.elzen@tue.nl&#39;, &#39;affiliation&#39;: &#39;Eindhoven University of Technology&#39;}], &#39;abstract&#39;: &#39;Random forests are a machine learning method used to automatically classify datasets and consist of a multitude of decision trees. While these random forests often have higher performance and generalize better than a single decision tree, they are also harder to interpret. This paper presents a visualization method and system to increase interpretability of random forests. We cluster similar trees which enables users to interpret how the model performs in general without needing to analyze each individual decision tree in detail, or interpret an oversimplified summary of the full forest. To meaningfully cluster the decision trees, we introduce a new distance metric that takes into account both the decision rules as well as the predictions of a pair of decision trees. We also propose two new visualization methods that visualize both clustered and individual decision trees: (1) The Feature Plot, which visualizes the topological position of features in the decision trees, and (2) the Rule Plot, which visualizes the decision rules of the decision trees. We demonstrate the efficacy of our approach through a case study on the “Glass” dataset, which is a relatively complex standard machine learning dataset, as well as a small user study.&#39;, &#39;uid&#39;: &#39;d3492f20-d356-4c55-992d-dbc0fa0fe0af&#39;, &#39;keywords&#39;: [&#39;Random Forest&#39;, &#39;Decision Tree&#39;, &#39;Tree clustering&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.22665&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/maxie12/RandomForestVis&#39;} <h3 class="session-list-title"><a href="paper_d3492f20-d356-4c55-992d-dbc0fa0fe0af.html"> Cluster-Based Random Forest Visualization and Interpretation <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Max Sondag, Christofer Meinecke, Dennis Collaris, Tatiana von Landesberger, Stef van den Elzen </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Christofer Meinecke </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:42:00.000Z &ndash; 2025-11-06T08:54:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1446&#39;, &#39;session_id&#39;: &#39;full12&#39;, &#39;title&#39;: &#39;Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models&#39;, &#39;contributors&#39;: [&#39;Zhanna Kaufman&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:54:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:54:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:06:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Zhanna Kaufman&#39;, &#39;email&#39;: &#39;zhannakaufma@umass.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts Amherst&#39;}, {&#39;name&#39;: &#39;Madeline Endres&#39;, &#39;email&#39;: &#39;mendres@umass.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts Amherst&#39;}, {&#39;name&#39;: &#39;Cindy Xiong Bearfield&#39;, &#39;email&#39;: &#39;cxiong@gatech.edu&#39;, &#39;affiliation&#39;: &#39;Georgia Tech&#39;}, {&#39;name&#39;: &#39;Yuriy Brun&#39;, &#39;email&#39;: &#39;brun@cs.umass.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts&#39;}], &#39;abstract&#39;: &#39;Systems relying on ML have become ubiquitous, but so has biased behavior within them. Research shows that bias significantly affects stakeholders’ trust in systems and how they use them. Further, stakeholders of different backgrounds view and trust the same systems differently. Thus, how ML models’ behavior is explained plays a key role in comprehension and trust. We survey explainability visualizations, creating a taxonomy of design characteristics. We conduct user studies to evaluate five state-of-the-art visualization tools (LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how taxonomy characteristics affect comprehension, bias perception, and trust for non-expert ML users. Surprisingly, we find an inverse relationship between comprehension and trust: the better users understand the models, the less they trust them. We investigate the cause and find that this relationship is strongly mediated by bias perception: more comprehensible visualizations increase people’s perception of bias, and increased bias perception reduces trust. We confirm this relationship is causal: Manipulating explainability visualizations to control comprehension, bias perception, and trust, we show that visualization design can significantly (p &lt; 0.001) increase comprehension, increase perceived bias, and reduce trust. Conversely, reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust even when comprehension remains high. Our work advances understanding of how comprehension affects trust and systematically investigates visualization’s role in facilitating responsible ML applications.&#39;, &#39;uid&#39;: &#39;1b3b1b15-e378-4c65-8984-717a7f7baa3a&#39;, &#39;keywords&#39;: [&#39;Visualization design&#39;, &#39;explainability&#39;, &#39;trust&#39;, &#39;bias in machine learning.&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2508.00140&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/c87xm/?view_only=31dfc1f2a7624f5cb20b0f07d3730df3&#39;} <h3 class="session-list-title"><a href="paper_1b3b1b15-e378-4c65-8984-717a7f7baa3a.html"> Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Zhanna Kaufman, Madeline Endres, Cindy Xiong Bearfield, Yuriy Brun </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Zhanna Kaufman </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:54:00.000Z &ndash; 2025-11-06T09:06:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1590&#39;, &#39;session_id&#39;: &#39;full12&#39;, &#39;title&#39;: &#39;EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts&#39;, &#39;contributors&#39;: [&#39;Kushin Mukherjee&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:06:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:06:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:18:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Kushin Mukherjee&#39;, &#39;email&#39;: &#39;kushinm11@gmail.com&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#39;Donghao Ren&#39;, &#39;email&#39;: &#39;donghao@apple.com&#39;, &#39;affiliation&#39;: &#39;Apple&#39;}, {&#39;name&#39;: &#39;Dominik Moritz&#39;, &#39;email&#39;: &#39;domoritz@cmu.edu&#39;, &#39;affiliation&#39;: &#39;Apple&#39;}, {&#39;name&#39;: &#39;Yannick Assogba&#39;, &#39;email&#39;: &#39;yassogba@gmail.com&#39;, &#39;affiliation&#39;: &#39;Apple&#39;}], &#39;abstract&#39;: &#39;Multimodal vision-language models (VLMs) continue to achieve ever-improving scores on chart understanding benchmarks. Yet, we find that this progress does not fully capture the breadth of visual reasoning capabilities essential for interpreting charts. We introduce ENCQA, a novel benchmark informed by the visualization literature, designed to provide systematic coverage of visual encodings and analytic tasks that are crucial for chart understanding. ENCQA provides 2,076 synthetic question-answer pairs, enabling balanced coverage of six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks (find extrema, retrieve value, find anomaly, filter values, compute derived value exact, compute derived value relative, correlate values, and correlate values relative). Our evaluation of 9 state-of-the-art VLMs reveals that performance varies significantly across encodings within the same task, as well as across tasks. Contrary to expectations, we observe that performance does not improve with model size for many task-encoding pairs. Our results suggest that advancing chart understanding requires targeted strategies addressing specific visual reasoning gaps, rather than solely scaling up model or dataset size.&#39;, &#39;uid&#39;: &#39;9942014b-e0f8-4cae-9b2a-81167d226c7b&#39;, &#39;keywords&#39;: [&#39;Visual encodings&#39;, &#39;visualization understanding tasks&#39;, &#39;machine chart understanding&#39;, &#39;vision-language models&#39;, &#39;model benchmarking&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/pdf/2508.04650v1&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_9942014b-e0f8-4cae-9b2a-81167d226c7b.html"> EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Kushin Mukherjee, Donghao Ren, Dominik Moritz, Yannick Assogba </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Kushin Mukherjee </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:06:00.000Z &ndash; 2025-11-06T09:18:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-05-0337&#39;, &#39;session_id&#39;: &#39;full12&#39;, &#39;title&#39;: &#39;AttributionScanner: A Visual Analytics System for Model Validation with Metadata-Free Slice Finding&#39;, &#39;contributors&#39;: [&#39;Jorge Ono&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:18:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:18:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:30:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Xiwei Xuan&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Jorge Piazentin Ono&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Liang Gou&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Kwan-Liu Ma&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Liu Ren&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;Data slice finding is an emerging technique for validating machine learning (ML) models by identifying and analyzing subgroups in a dataset that exhibit poor performance, often characterized by distinct feature sets or descriptive metadata. However, in the context of validating vision models involving unstructured image data, this approach faces significant challenges, including the laborious and costly requirement for additional metadata and the complex task of interpreting the root causes of underperformance. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for metadata-free data slice finding. Our system identifies interpretable data slices that involve common model behaviors and visualizes these patterns through an Attribution Mosaic design. Our interactive interface provides straightforward guidance for users to detect, interpret, and annotate predominant model issues, such as spurious correlations (model biases) and mislabeled data, with minimal effort. Additionally, it employs a cutting-edge model regularization technique to mitigate the detected issues and enhance the model’s performance. The efficacy of AttributionScanner is demonstrated through use cases involving two benchmark datasets, with qualitative and quantitative evaluations showcasing its substantial effectiveness in vision model validation, ultimately leading to more reliable and accurate models.&#39;, &#39;uid&#39;: &#39;87a90010-17cf-4a03-a81f-38b56bb28d64&#39;, &#39;keywords&#39;: [&#39;Computational modeling&#39;, &#39;Analytical models&#39;, &#39;Data visualization&#39;, &#39;Correlation&#39;, &#39;Accuracy&#39;, &#39;Visual analytics&#39;, &#39;Annotations&#39;, &#39;Image color analysis&#39;, &#39;Vectors&#39;, &#39;Training&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2401.06462v1&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3546644&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_87a90010-17cf-4a03-a81f-38b56bb28d64.html"> AttributionScanner: A Visual Analytics System for Model Validation with Metadata-Free Slice Finding <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Xiwei Xuan, Jorge Piazentin Ono, Liang Gou, Kwan-Liu Ma, Liu Ren </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jorge Ono </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:18:00.000Z &ndash; 2025-11-06T09:30:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-12-1092.&#39;, &#39;session_id&#39;: &#39;full12&#39;, &#39;title&#39;: &#39;Visagreement: Visualizing and Exploring Explanations (Dis)Agreement&#39;, &#39;contributors&#39;: [&#39;Priscylla Silva&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Priscylla Silva&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Vitoria Guardieiro&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Brian Barr&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Claudio Silva&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Luis Gustavo Nonato&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#34;The emergence of distinct machine learning explanation methods has leveraged a number of new issues to be investigated. The disagreement problem is one such issue, as there may be scenarios where the output of different explanation methods disagree with each other. Although understanding how often, when, and where explanation methods agree or disagree is important to increase confidence in the explanations, few works have been dedicated to investigating such a problem. In this work, we proposed Visagreement, a visualization tool designed to assist practitioners in investigating the disagreement problem. Visagreement builds upon metrics to quantitatively compare and evaluate explanations, enabling visual resources to uncover where and why methods mostly agree or disagree. The tool is tailored for tabular data with binary classification and focuses on local feature importance methods. In the provided use cases, Visagreement turned out to be effective in revealing, among other phenomena, how disagreements relate to the quality of the explanations and machine learning model accuracy, thus assisting users in deciding where and when to trust explanations. To assess the effectiveness and practical utility of Visagreement, we conducted an evaluation involving four experts. These experts assessed the tool&#39;s Effectiveness, Usability, and Impact on Decision-Making. The experts confirm the Visagreement tool&#39;s effectiveness and user-friendliness, making it a valuable asset for analyzing and exploring (dis)agreements.&#34;, &#39;uid&#39;: &#39;86b81feb-984c-4b76-bda1-202a9c48c25c&#39;, &#39;keywords&#39;: [&#39;Measurement&#39;, &#39;Machine learning&#39;, &#39;Data visualization&#39;, &#39;Accuracy&#39;, &#39;Extraterrestrial measurements&#39;, &#39;Analytical models&#39;, &#39;Data models&#39;, &#39;Visual analytics&#39;, &#39;Training&#39;, &#39;Space exploration&#39;], &#39;preprint_link&#39;: &#39;https://www.techrxiv.org/doi/full/10.36227/techrxiv.173386520.03447635/v2&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3558074&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_86b81feb-984c-4b76-bda1-202a9c48c25c.html"> Visagreement: Visualizing and Exploring Explanations (Dis)Agreement <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Priscylla Silva, Vitoria Guardieiro, Brian Barr, Claudio Silva, Luis Gustavo Nonato </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Priscylla Silva </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:30:00.000Z &ndash; 2025-11-06T09:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T08:30:00+00:00'
    endTime = '2025-11-06T09:45:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "e1-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>