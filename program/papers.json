[{"UID":"013c31af-6df4-4dea-b01c-3f4cd7af88df","abstract":"Trame is an open-source, Python-based, scalable integration framework for visual analytics. It is the culmination of decades of work\u2014by a large and active community\u2014beginning with the creation of VTK, the growth of ParaView as a premier high-performance, client\u2013server computing system, and more recently the creation of web tools, such as VTK.js and VTK.wasm. As an integration environment, trame relies on open-source standards and tools that can be easily combined into effective computing solutions. We have long recognized that impactful analytics tools must be ubiquitous\u2014meaning they run on all major computing platforms\u2014and integrate/interoperate easily with external packages, such as data systems and processing tools, application UI frameworks, and 2-D/3-D graphical libraries. In this article, we present the architecture and use of trame for applications ranging from simple dashboards to complex workflow-based applications. We also describe examples that readily incorporate external tools and run without coding changes on desktop, mobile, cloud, client\u2013server, and interactive computing notebooks, such as Jupyter.","accessible_pdf":"No","authors":[{"email":null,"name":"S\u00e9bastien Jourdain"},{"email":null,"name":"Patrick O\u2019Leary"},{"email":null,"name":"Will Schroeder"}],"award":"","doi":"10.1109/MCG.2025.3540264","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"013c31af-6df4-4dea-b01c-3f4cd7af88df","image_caption":"","keywords":["Scientific computing","Visual analytics","Computer architecture","Licenses","Libraries","Servers","Security","Software tools","Standards","Open source software"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Developed an open, Python-based framework for visual analytics. It provides interoperability with many open source tools such as Jupyter, VTK, and ParaView.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2025.3540264","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga2","session_room":"Hall E1","session_room_id":"e1","session_title":"Visual Analytics Methods, Tools, and Infrastructure","session_uid":"013c31af-6df4-4dea-b01c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visual Analytics Methods, Tools, and Infrastructure"],"time_stamp":"2025-11-06T11:03:00.000Z","title":"Trame: Platform Ubiquitous, Scalable Integration Framework for Visual Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JfetpDJuh5Q"},{"UID":"02afa2f8-af68-4eb3-b1e6-9f20791ee73d","abstract":"Incorporating automatic style extraction and transfer from existing well-designed graph visualizations can significantly alleviate the designer\u2019s workload. There are many types of graph visualizations. In this paper, our work focuses on node-link diagrams. We present a novel approach to streamline the design process of graph visualizations by automatically extracting visual styles from well-designed examples and applying them to other graphs. Our formative study identifies the key styles that designers consider when crafting visualizations, categorizing them into global and local styles. Leveraging deep learning techniques such as saliency detection models and multi-label classification models, we develop end-to-end pipelines for extracting both global and local styles. Global styles focus on aspects such as color scheme and layout, while local styles are concerned with the finer details of node and edge representations. Through a user study and evaluation experiment, we demonstrate the efficacy and time-saving benefits of our method, highlighting its potential to enhance the graph visualization design process.","accessible_pdf":"No","authors":[{"email":null,"name":"Sicheng Song"},{"email":null,"name":"Yipeng Zhang"},{"email":null,"name":"Yanna Lin"},{"email":null,"name":"Huamin Qu"},{"email":null,"name":"Changbo Wang"},{"email":null,"name":"Chenhui Li"}],"award":"","doi":"10.1109/TVCG.2024.3485701","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"02afa2f8-af68-4eb3-b1e6-9f20791ee73d","image_caption":"","keywords":["Visualization","Data visualization","Data mining","Layout","Image color analysis","Feature extraction","Deep learning","Pipelines","Bars","Image edge detection"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3485701","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full17","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Graphs and Networks","session_uid":"02afa2f8-af68-4eb3-b1e6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Graphs and Networks"],"time_stamp":"2025-11-05T15:21:00.000Z","title":"GVVST: Image-Driven Style Extraction From Graph Visualizations for Visual Style Transfer","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zH5jvaUZHGk"},{"UID":"04cf6d82-a777-4312-a4b0-d3f4b9479602","abstract":"We investigate the impact of visualization methods and interaction mechanisms in Voting Advice Applications (VAAs) by comparing two designs: the traditional design in Kieskompas and an alternative design composed by a visualization -- metaphoric map -- based on dimensionality reduction of opinion data and an interaction mechanism allowing to adjust answers on the questions. In a user study with 382 participants from the Netherlands, we assessed their effects on vote choice, political knowledge, ease of use, and interpretation of opinion space. The results show no evidence that visualization and interaction mechanisms influence vote choice. The metaphoric map led to lower perceived and factual political knowledge and was harder to interpret, likely due to its higher visualization literacy demands. Notably, participants using metaphoric map offered more objective, non-ideological interpretations of their political position, suggesting potential to foster unbiased political comprehension. Supplementary materials are available at: https://osf.io/zvhsg/?view_only=ee2d7d315c254b358b9be077214cc18e.","accessible_pdf":null,"authors":[{"affiliation":"Utrecht University","email":"dverboom@vallei-veluwe.nl","name":"Damion E. Verboom"},{"affiliation":"Utrecht University","email":"t.mtsentlintze@uu.nl","name":"Tamara Mchedlidze"},{"affiliation":"Utrecht University","email":"e.oral@uu.nl","name":"Ba\u015fak Oral"},{"affiliation":"Utrecht University","email":"evanthia.dimara@gmail.com","name":"Evanthia Dimara"},{"affiliation":"Saxion University","email":"anadanielaperesrebelo@gmail.com","name":"Daniela Peres Rebelo"},{"affiliation":"Tilburg University","email":"n.kamoen@tilburguniversity.edu","name":"Naomi Kamoen"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"04cf6d82-a777-4312-a4b0-d3f4b9479602","image_caption":"","keywords":["VAAs","visualization","interaction","dimensionality reduction","social information","voting"],"open_access_supplemental_link":"https://osf.io/zvhsg/?view_only=ee2d7d315c254b358b9be077214cc18e","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1299","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"04cf6d82-a777-4312-a4b0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T08:30:00.000Z","title":"Visualizing Opinion Space in Voting Advice Applications: A User Study","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"055a075f-b15e-43ec-becb-cc09ad944d2c","abstract":"We investigate whether tactile charts support comprehension and learning of complex visualizations for blind and low-vision (BLV) individuals and contribute four tactile chart designs and an interview study. Visualizations are powerful tools for conveying data, yet BLV individuals typically can rely only on assistive technologies\u2014primarily alternative texts\u2014to access this information. Prior research shows the importance of mental models of chart types for interpreting these descriptions, yet BLV individuals have no means to build such a mental model based on images of visualizations. Tactile charts show promise to fill this gap in supporting the process of building mental models. Yet studies on tactile data representations mostly focus on simple chart types, and it is unclear whether they are also appropriate for more complex charts as would be found in scientific publications. Working with two BLV researchers, we designed 3D-printed tactile template charts with exploration instructions for four advanced chart types: UpSet plots, violin plots, clustered heatmaps, and faceted line charts. We then conducted an interview study with 12 BLV participants comparing whether using our tactile templates improves mental models and understanding of charts and whether this understanding translates to novel datasets experienced through alt texts. Thematic analysis shows that tactile models support chart type understanding and are the preferred learning method by BLV individuals. We also report participants' opinions on tactile chart design and their role in BLV education.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Utah","email":"hetingying.hty@gmail.com","name":"Tingying He"},{"affiliation":"University of Utah","email":"maggie.mccracken@psych.utah.edu","name":"Maggie McCracken"},{"affiliation":"University College London","email":"d.hajas@ucl.ac.uk","name":"Daniel Hajas"},{"affiliation":"University of Utah","email":"sarah.creem@psych.utah.edu","name":"Sarah Creem-Regehr"},{"affiliation":"University of Utah","email":"alexander.lex@gmail.com","name":"Alexander Lex"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"055a075f-b15e-43ec-becb-cc09ad944d2c","image_caption":"","keywords":["Accessibility","tactile representations"],"open_access_supplemental_link":"https://osf.io/9dwgq/","open_access_supplemental_question":"We provide an extensive, well-documented 40-page appendix that includes materials and detailed descriptions of the full design and user study processes.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.21462","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1514","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full33","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Encoding & Comprehension","session_uid":"055a075f-b15e-43ec-becb","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Encoding & Comprehension"],"time_stamp":"2025-11-05T08:42:00.000Z","title":"Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"063cc125-d8ef-4935-baa1-e74f6262ea81","abstract":"Analytic provenance can be visually encoded to help users track their ongoing analysis trajectories, recall past interactions, and inform new analytic directions. Despite its significance, provenance is often hardwired into analytics systems, affording limited user control and opportunities for self-reflection. We thus propose modeling provenance as an attribute that is available to users during analysis. We demonstrate this concept by modeling two provenance attributes that track the recency and frequency of user interactions with data. We integrate these attributes into a visual data analysis system prototype, ProvenanceLens, wherein users can visualize their interaction recency and frequency by mapping them to encoding channels (e.g., color, size) or applying data transformations (e.g., filter, sort). Using ProvenanceLens as a design probe, we conduct an exploratory study with sixteen users to investigate how these provenance-tracking affordances are utilized for both decision-making and self-reflection. We find that users can accurately and confidently answer questions about their analysis, and we show that mismatches between the user's mental model and the provenance encodings can be surprising, thereby prompting useful self-reflection. We also report on the user strategies surrounding these affordances, and reflect on their intuitiveness and effectiveness in representing provenance.","accessible_pdf":"No","authors":[{"email":null,"name":"Arpit Narechania"},{"email":null,"name":"Shunan Guo"},{"email":null,"name":"Eunyee Koh"},{"email":null,"name":"Alex Endert"},{"email":null,"name":"Jane Hoffswell"}],"award":"","doi":"10.1109/TVCG.2025.3571708","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"063cc125-d8ef-4935-baa1-e74f6262ea81","image_caption":"","keywords":["Data visualization","Visualization","Analytical models","Encoding","Data models","Data analysis","Image color analysis","History","Time-frequency analysis","Probes"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2505.11784","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0792.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full19","session_room":"Hall E1","session_room_id":"e1","session_title":"Interaction, Provenance, and Collaboration","session_uid":"063cc125-d8ef-4935-baa1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Interaction, Provenance, and Collaboration"],"time_stamp":"2025-11-06T15:45:00.000Z","title":"Utilizing Provenance as an Attribute for Visual Data Analysis: A Design Probe with ProvenanceLens","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/253qu_2o0K0"},{"UID":"06d5d55a-bd4b-4c6f-9f01-60b54e657562","abstract":"Layered network visualizations assign each node to one of several parallel axes. They can convey sequence or flow data, hierarchies, or multiple data classes, but edge crossings and long edges often impair readability. Layout algorithms can reduce edge crossings and shorten edges using quick heuristics or optimal methods that prioritize human readability over computation speed. This work uses an optimization metaheuristic to provide the best of both worlds: high-quality layouts within a predetermined execution time. Our adaptation of the large neighborhood search (LNS) metaheuristic repeatedly selects fixed-sized subgraphs to lay out optimally. We conducted a computational evaluation using 450 synthetic networks to compare five ways of selecting candidate nodes, four ways of selecting their neighboring subgraph, and three criteria for determining subgraph size. LNS generally halved the number of crossings versus the barycentric heuristic while maintaining a reasonable runtime. Our best approach randomly selected candidate nodes, used degree centrality to pick cluster-like neighborhoods, and chose smaller neighborhoods that could be optimally laid out in 0.6 or 1.2 seconds (versus 6 seconds). In a case study visualizing 13 control flow graphs, most with over 1000 nodes, we show that our method can be employed to create visualizations with fewer crossings than Tabu Search, another metaheuristic, and vastly outperforms an ILP solver when runtime is bounded.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Connor Wilson"},{"email":null,"name":"Tarik Crnovrsanin"},{"email":null,"name":"Eduardo Puerta"},{"email":null,"name":"Cody Dunne"}],"award":"","doi":"10.1109/TVCG.2025.3537898","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"06d5d55a-bd4b-4c6f-9f01-60b54e657562","image_caption":"","keywords":["Layout","Metaheuristics","Visualization","Runtime","Optimization models","Search problems","Reverse engineering","Minimization","Heuristic algorithms","Graph drawing"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/fytk7","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-09-0854.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full24","session_room":"Hall M2","session_room_id":"m2","session_title":"Ordering and Layout","session_uid":"06d5d55a-bd4b-4c6f-9f01","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Ordering and Layout"],"time_stamp":"2025-11-05T13:36:00.000Z","title":"Fast and Readable Layered Network Visualizations Using Large Neighborhood Search","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/1kLUiSSxIuM"},{"UID":"0833c9dd-52a3-4a64-bbcd-15c9d10ac8aa","abstract":"We offer a new model of the sensemaking process for data analysis and visualization. Whereas past sensemaking models have been grounded in positivist assumptions about the nature of knowledge, we reframe data sensemaking in critical, humanistic terms by approaching it through an interpretivist lens. Our three-phase process model uses the analogy of an iceberg, where data is the visible tip of underlying schemas. In the Add phase, the analyst acquires data, incorporates explicit schemas from the data, and absorbs the tacit schemas of both data and people. In the Check phase, the analyst interprets the data with respect to the current schemas and evaluates whether the schemas match the data. In the Refine phase, the analyst considers the role of power, articulates what was tacit into explicitly stated schemas, updates data, and formulates findings. Our model has four important distinguishing features: Tacit and Explicit Schemas, Schemas First and Always, Data as a Schematic Artifact, and Schematic Multiplicity. We compare the roles of schemas in past sensemaking models and draw conceptual distinctions based on a historical review of schemas in different academic traditions. We validate the descriptive and prescriptive power of our model through four analysis scenarios: noticing uncollected data, learning to wrangle data, downplaying inconvenient data, and measuring with sensors. We conclude by discussing the value of interpretivism, the virtue of epistemic humility, and the pluralism this sensemaking model can foster.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Charles Berret"},{"email":null,"name":"Tamara Munzner"}],"award":"","doi":"10.1109/TVCG.2024.3486613","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"0833c9dd-52a3-4a64-bbcd-15c9d10ac8aa","image_caption":"","keywords":["Analytical models","Data models","Data analysis","Computational modeling","Data visualization","Icebergs","Shape","Sensor phenomena and characterization","Visual analytics","Tuning"],"open_access_supplemental_link":null,"open_access_supplemental_question":"-","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2204.04758","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3486613","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full21","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Models, Methods, and Typologies","session_uid":"0833c9dd-52a3-4a64-bbcd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Models, Methods, and Typologies"],"time_stamp":"2025-11-07T09:30:00.000Z","title":"Iceberg Sensemaking: A Process Model for Critical Data Analysis and Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/moykjX8QeZI"},{"UID":"085f602b-ba6f-4a24-ab38-7a60af7ad270","abstract":"User interfaces for inspecting spatio-temporal events often allow their users to filter the events by specifying a time window with a time slider. We consider the case that filtered events are visualized on a map using textual or iconic labels. However, to ensure a clear visualization, not all filtered events are annotated with a label. We present algorithms for setting up a data structure that encodes for every possible time window the set of displayed labels. Our algorithms ensure that the displayed labels never overlap and guarantee the stability of the labeling during certain basic interactions with the time slider. Assuming that the labels have different priorities (weights), we aim to maximize the weight of the displayed labels integrated over all possible time windows. As basic interactions, we consider moving the entire time window, symmetrically scaling it, and dragging one of its endpoints. We consider two stability requirements: (1) during a basic interaction, a label should appear and disappear at most once; (2) if a label is displayed for a time window Q, then it is also displayed for all the time windows contained in Q and that contain its timestamp. We prove that finding an optimal solution is NP-hard and propose efficient constant-factor approximation algorithms for unit-square and unit-disk labels, as well as a fast greedy heuristic for arbitrarily shaped labels. In experiments on real-world data, we compare the non-exact algorithms with an exact approach through integer linear programming.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Annika Bonerath"},{"email":null,"name":"Anne Driemel"},{"email":null,"name":"Jan-Henrik Haunert"},{"email":null,"name":"Herman Haverkort"},{"email":null,"name":"Elmar Langetepe"},{"email":null,"name":"Benjamin Niedermann"}],"award":"","doi":"10.1109/TVCG.2025.3527582","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"085f602b-ba6f-4a24-ab38-7a60af7ad270","image_caption":"","keywords":["Labeling","Data visualization","Annotations","Tornadoes","Heuristic algorithms","Data structures","Spatial databases","Reproducibility of results","Earthquakes","Approximation algorithms"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[10.1109/TVCG 2025.3527582 ]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full20","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Maps & Spatial Vis","session_uid":"085f602b-ba6f-4a24-ab38","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Maps & Spatial Vis"],"time_stamp":"2025-11-05T09:18:00.000Z","title":"Algorithms for Consistent Dynamic Labeling of Maps With a Time-Slider Interface","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/nkeAR0PgfSw"},{"UID":"093872c4-bec7-4c27-bb8e-c89465a4381b","abstract":"Color is a powerful tool in data visualization, but for individuals with color vision deficiencies (CVD), hue can become a barrier rather than an aid. In this paper, we examine how real-world visualizations are perceived across vision profiles through three complementary studies. Study 1 assessed how normal vision participants rated 46 visualizations shown in original and simulated red/green colorblind versions. Study 2 collected matched responses from participants with diagnosed CVD. Study 3 involved in-depth interviews exploring how users interpret, adapt to, and evaluate inaccessible designs. Across studies, we find that simulations capture directional perceptual shifts but fail to reflect the interpretive breakdowns and emotional work described by real CVD users. Factor analysis reveals two dominant perceptual dimensions: functional utility and affective experience. While normal vision participants prioritize functional clarity, CVD users rely more on structural cues and emotional resonance, particularly when color is unreliable. Qualitative insights show that perceptual breakdowns occur not only in high-interference charts but also when redundant encoding or layout scaffolding is missing. We synthesize these findings and offer empirically grounded design recommendations to guide inclusive visualization practices. Our results argue that accessibility must go beyond color correction, embracing structural clarity, redundancy, and real-user validation to ensure inclusive visual communication.","accessible_pdf":null,"authors":[{"affiliation":"Arizona State University","email":"zjian115@asu.edu","name":"Zhuojun Jiang"},{"affiliation":"Northeastern University","email":"aarunku5@asu.edu","name":"Anjana Arunkumar"},{"affiliation":"Arizona State University","email":"cbryan16@asu.edu","name":"Chris Bryan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"093872c4-bec7-4c27-bb8e-c89465a4381b","image_caption":"","keywords":["Information Visualization","Perception & Cognition","Colorblindness","Accessibility"],"open_access_supplemental_link":"https://osf.io/ymucn/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1755","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full27","session_room":"Hall M2","session_room_id":"m2","session_title":"The Color and the Shape","session_uid":"093872c4-bec7-4c27-bb8e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The Color and the Shape"],"time_stamp":"2025-11-06T09:18:00.000Z","title":"The Hue-Man Factor: An Empirical Evaluation of Visualization Perception and Accessibility Across Color Vision Profiles","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gHCENa3C9X4"},{"UID":"093d9538-804e-4951-a5bf-dab72e99c35d","abstract":"Voronoi treemaps are used to depict nodes and their hierarchical relationships simultaneously. However, in addition to the hierarchical structure, data attributes, such as co-occurring features or similarities, frequently exist. Examples include geographical attributes like shared borders between countries or contextualized semantic information such as embedding vectors derived from large language models. In this work, we introduce a Voronoi treemap algorithm that leverages data similarity to generate neighborhood-preserving treemaps. First, we extend the treemap layout pipeline to consider similarity during data preprocessing. We then use a Kuhn-Munkres matching of similarities to centroidal Voronoi tessellation (CVT) cells to create initial Voronoi diagrams with equal cell sizes for each level. Greedy swapping is used to improve the neighborhoods of cells to match the data's similarity further. During optimization, cell areas are iteratively adjusted to their respective sizes while preserving the existing neighborhoods. We demonstrate the practicality of our approach through multiple real-world examples drawn from infographics and linguistics. To quantitatively assess the resulting treemaps, we employ treemap metrics and measure neighborhood preservation.","accessible_pdf":null,"authors":[{"affiliation":"University of Konstanz","email":"patrick.paetzold@uni-konstanz.de","name":"Patrick Paetzold"},{"affiliation":"University of Konstanz","email":"rebecca.kehlbeck@uni-konstanz.de","name":"Rebecca Kehlbeck"},{"affiliation":"University of Konstanz","email":"yumeng.xue@uni-konstanz.de","name":"Yumeng Xue"},{"affiliation":"University of Konstanz","email":"bin.chen@uni-konstanz.de","name":"Bin Chen"},{"affiliation":"Renmin University of China","email":"cloudseawang@gmail.com","name":"Yunhai Wang"},{"affiliation":"University of Konstanz","email":"oliver.deussen@uni-konstanz.de","name":"Oliver Deussen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"093d9538-804e-4951-a5bf-dab72e99c35d","image_caption":"","keywords":["Hierarchical data","Treemap","Voronoi diagram","Voronoi treemap"],"open_access_supplemental_link":"https://github.com/cgmi/Neighborhood-Preserving-Voronoi-Treemaps","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.03445","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1598","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full23","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Of Trees and Treemaps","session_uid":"093d9538-804e-4951-a5bf","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Of Trees and Treemaps"],"time_stamp":"2025-11-05T10:15:00.000Z","title":"Neighbourhood-Preserving Voronoi Treemaps","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/yiiG1AZHJ1E"},{"UID":"09fe1b80-d4a6-454e-bd5f-d49b53eae041","abstract":"Designing multiscale visualizations, particularly when the ratio between the largest scale and the smallest item is large, can be challenging, and designers have developed many approaches to overcome this challenge. We present a design space for visualization with multiple scales. The design space includes three dimensions, with eight total subdimensions. We demonstrate its descriptive power by using it to code approaches from a corpus we compiled of 52 examples, created by a mix of academics and practitioners. We demonstrate descriptive power by analyzing and partitioning these examples into four high-level strategies for designing multiscale visualizations, which are shared approaches with respect to design space dimension choices. We demonstrate generative power by analyzing missed opportunities within the corpus of examples, identified through analysis of the design space, where we note how certain examples could have benefited from different choices. We discuss patterns in the use of different dimension and strategy choices in the different visualization contexts of analysis and presentation. \n\nSupplemental materials: https://osf.io/wbrdm/\n\nDesign space website: https://marasolen.github.io/multiscale-vis-ds/","accessible_pdf":"Accessible","authors":[{"affiliation":"The University of British Columbia","email":"marasolen@gmail.com","name":"Mara Solen"},{"affiliation":"The University of British Columbia","email":"moddo@eoas.ubc.ca","name":"Matt Oddo"},{"affiliation":"University of British Columbia","email":"tmm@cs.ubc.ca","name":"Tamara Munzner"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"09fe1b80-d4a6-454e-bd5f-d49b53eae041","image_caption":"","keywords":["Visualization","design space","multiscale."],"open_access_supplemental_link":"https://osf.io/wbrdm/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2404.01485","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1085","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full21","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Models, Methods, and Typologies","session_uid":"09fe1b80-d4a6-454e-bd5f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Models, Methods, and Typologies"],"time_stamp":"2025-11-07T08:30:00.000Z","title":"A Design Space for Multiscale Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/moykjX8QeZI"},{"UID":"0db7ecd3-a51e-4c72-bc36-d06d31c08c7f","abstract":"Upon completing the design and training phases, deploying a deep learning model to specific hardware becomes necessary prior to its implementation in practical applications. To enhance the performance of the model, the developers must optimize it to decrease inference latency. Auto-scheduling, an automated approach that generates optimization schemes, offers a feasible option for large-scale auto-deployment. Nevertheless, the low-level code generated by auto-scheduling closely resembles hardware coding and may present challenges for human comprehension, thereby hindering future manual optimization efforts. In this study, we introduce ASight, a visual analytics system to assist engineers in identifying performance bottlenecks, comprehending the auto-generated low-level code, and obtaining insights from auto-scheduling optimizations. We develop a subgraph matching algorithm capable of identifying graph isomorphism among Intermediate Representations to track performance bottlenecks from low-level metrics to high-level computational graphs. To address the substantial profiling metrics involved in auto-scheduling and derive optimization design principles by summarizing commonalities among auto-scheduling optimizations, we propose an enhanced visualization for the large search space of auto-scheduling. We validate the effectiveness of ASight through two case studies, one focused on a local machine and the other on a data center, along with a quantitative experiment exploring optimization design principles.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Laixin Xie"},{"email":null,"name":"Chenyang Zhang"},{"email":null,"name":"Ruofei Ma"},{"email":null,"name":"Xingxing Xing"},{"email":null,"name":"Wei Wan"},{"email":null,"name":"Quan Li"}],"award":"","doi":"10.1109/TVCG.2025.3574194","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"0db7ecd3-a51e-4c72-bc36-d06d31c08c7f","image_caption":"","keywords":["Optimization","Codes","Hardware","Schedules","Runtime","Measurement","Visual analytics","Optimal scheduling","Aerodynamics","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0651.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full3","session_room":"Hall E1","session_room_id":"e1","session_title":"Analysts, Assemble!","session_uid":"0db7ecd3-a51e-4c72-bc36","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analysts, Assemble!"],"time_stamp":"2025-11-06T13:48:00.000Z","title":"ASight: Fine-tuning Auto-Scheduling Optimizations for Model Deployment via Visual Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/fkfdha6pCeI"},{"UID":"0e4ce5d7-ea72-4776-af1e-e42301ba40b7","abstract":"Deep Learning Super Sampling (DLSS) is a relatively new \nmachine learning technology for accelerating 3D graphics, by \nrendering at lower resolutions, and inferring what the higher \nresolution output should look like. However, DLSS was designed for, \nand trained on, video games. Can this technology, intended for the \nentertainment market, be repurposed to accelerate high-resolution \nscientific and geospatial visualization? This paper evaluates the visual \naccuracy costs of inferring higher resolution data visualizations with \nDLSS upscaling. A related technology, Deep Learning Anti-aliasing \n(DLAA) is similarly evaluated. This evaluation focused on 3D \ngeospatial applications, with (primarily non-photorealistic) terrain \nrendering and point cloud rendering, though the results are applicable \nto a wider range of scientific visualization data types. The results \ndemonstrate that DLSS/DLAA can significantly impact visual \naccuracy, and should be avoided for visualizations requiring accurate \nportrayal of fine details.","accessible_pdf":null,"authors":[{"affiliation":"University of New Hampshire","email":"kindratberegovyi@gmail.com","name":"Kindrat Beregovyi"},{"affiliation":"University of New Hampshire","email":"tbutkie@gmail.com","name":"Thomas Butkiewicz"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"0e4ce5d7-ea72-4776-af1e-e42301ba40b7","image_caption":"","keywords":["Machine Learning Techniques; Computer Graphics Techniques; Guidelines; Geospatial Data; Terrain","Point Clouds; Deep Learning Super Sampling","DLSS","Upscaling; DLAA; Anti-Aliasing"],"open_access_supplemental_link":"https://vislab-ccom.unh.edu/downloads/DLSS_Paper_Data.zip","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1232","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short11","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Multi-modal and Spatial","session_uid":"0e4ce5d7-ea72-4776-af1e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Multi-modal and Spatial"],"time_stamp":"2025-11-06T08:30:00.000Z","title":"Visual Integrity in the Age of AI: An Evaluation of DLSS and DLAA in Geospatial Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"0ecc86cf-63c0-4483-9849-b207a438bb59","abstract":"The recent adoption of artificial intelligence in socio-technical systems raises concerns about the black-box nature of the resulting decisions in fields such as hiring, finance, admissions, etc. If data subjects\u2014such as job applicants, loan applicants, and students\u2014receive an unfavorable outcome, they may be interested in algorithmic recourse, which involves updating certain features to yield a more favorable result when re-evaluated by algorithmic decision-making. Unfortunately, when individuals do not fully understand the incremental steps needed to change their circumstances, they risk following misguided paths that can lead to significant, long-term adverse consequences. Existing recourse approaches focus exclusively on the final recourse goal but neglect the possible incremental steps to reach the goal with real-life constraints, user preferences, and model artifacts. To address this gap, we formulate a visual analytic workflow for incremental recourse planning in collaboration with AI/ML experts and contribute an interactive visualization interface that helps data subjects efficiently navigate the recourse alternatives and make an informed decision. We present a usage scenario and subjective feedback from observational studies with twelve graduate students using a real-world dataset, which demonstrates that our approach can be instrumental for data subjects in choosing a suitable recourse path.","accessible_pdf":"Accessible","authors":[{"affiliation":"Pacific Northwest National Laboratory","email":"kaustavbhatt94@gmail.com","name":"Kaustav Bhattacharjee"},{"affiliation":"New Jersey Institute of Technology","email":"jy448@njit.edu","name":"Jun Yuan"},{"affiliation":"New Jersey Institute of Technology","email":"dasgupta.aritra@gmail.com","name":"Aritra Dasgupta"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"0ecc86cf-63c0-4483-9849-b207a438bb59","image_caption":"","keywords":["Visual analytics","recourse","machine learning"],"open_access_supplemental_link":"https://osf.io/ngzcm/","open_access_supplemental_question":"This is a novel method for designing visual analytic interfaces for algorithmic recourse. Paper will be put on arXiV soon.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1200","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short12","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Viz for AI & AI for Viz","session_uid":"0ecc86cf-63c0-4483-9849","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Viz for AI & AI for Viz"],"time_stamp":"2025-11-05T10:33:00.000Z","title":"ReVise: A Human-AI Interface for Incremental Algorithmic Recourse","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"0f3ea5e8-4bc5-4e8a-8619-1ce981d4e8e1","abstract":"Marksmanship practices are required in various professions, including police, military personnel, hunters, as well as sports shooters, such as Olympic shooting, biathlon, and modern pentathlon. The current form of training and coaching is mostly based on repetition, where the coach does not see through the eyes of the shooter, and analysis is limited to stance and accuracy post-session. In this study, we present a shooting visualization system and evaluate its perceived effectiveness for both novice and expert shooters. To achieve this, five composite visualizations were developed using first-person shooting video recordings enriched with overlaid metrics and graphical summaries. These views were evaluated with 10 participants (5 expert marksmen, 5 novices) through a mixed-methods study including shot-count and aiming interpretation tasks, pairwise preference comparisons, and semi-structured interviews. The results show that a dashboard-style composite view, combining raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases and supported understanding across skill levels. The insights gained from this design study point to the broader value of integrating first-person video with visual analytics for coaching, and we suggest directions for applying this approach to other precision-based sports.","accessible_pdf":null,"authors":[{"affiliation":"Mid Sweden University","email":"emin.zerman@miun.se","name":"Emin Zerman"},{"affiliation":"Mid Sweden University","email":"j_c_kc@live.se","name":"Jonas Carlsson"},{"affiliation":"Mid Sweden University","email":"marten.sjostrom@miun.se","name":"M\u00e5rten Sj\u00f6str\u00f6m"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"0f3ea5e8-4bc5-4e8a-8619-1ce981d4e8e1","image_caption":"","keywords":["Composite visualization","sports coaching","marksmanship training","first-person video","user studies"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.00333","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1143","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"0f3ea5e8-4bc5-4e8a-8619","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T10:51:00.000Z","title":"Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"0ffcd3a5-a31e-4532-9adb-8306bbb1518f","abstract":"In this paper, we present a flexible and probabilistic framework for tracking topological features in time-varying scalar fields using merge trees and partial optimal transport. Merge trees are topological descriptors that record the evolution of connected components in the sublevel sets of scalar fields. We present a new technique for modeling and comparing merge trees using tools from partial optimal transport. In particular, we model a merge tree as a measure network, that is, a network equipped with a probability distribution, and define a notion of distance on the space of merge trees inspired by partial optimal transport. Such a distance offers a new and flexible perspective for encoding intrinsic and extrinsic information in the comparative measures of merge trees. More importantly, it gives rise to a partial matching between topological features in time-varying data, thus enabling flexible topology tracking for scientific simulations. Furthermore, such partial matching may be interpreted as probabilistic coupling between features at adjacent time steps, which gives rise to probabilistic tracking graphs. We derive a stability result for our distance and provide numerous experiments indicating the efficacy of our framework in extracting meaningful feature tracks.","accessible_pdf":"No","authors":[{"email":null,"name":"Mingzhe Li"},{"email":null,"name":"Xinyuan Yan"},{"email":null,"name":"Lin Yan"},{"email":null,"name":"Tom Needham"},{"email":null,"name":"Bei Wang"}],"award":"","doi":"10.1109/TVCG.2025.3561300","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"0ffcd3a5-a31e-4532-9adb-8306bbb1518f","image_caption":"","keywords":["Probabilistic logic","Feature extraction","Vegetation","Topology","Probability distribution","Extraterrestrial measurements","Surveys","Visualization","Vectors","Reviews"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2302.02895","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2025.3561300]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full2","session_room":"Hall E2","session_room_id":"e2","session_title":"Algorithms and Workflows","session_uid":"0ffcd3a5-a31e-4532-9adb","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Algorithms and Workflows"],"time_stamp":"2025-11-06T13:36:00.000Z","title":"Flexible and Probabilistic Topology Tracking with Partial Optimal Transport","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/wUoqSN_s3tw"},{"UID":"10233019-3fcc-4f01-b478-96b0ec01cf29","abstract":"Accurate and reliable visualization of spatiotemporal sensor data such as environmental parameters and meteorological conditions is crucial for informed decision-making. Traditional spatial interpolation methods, however, often fall short of producing reliable interpolation results due to the limited and irregular sensor coverage. This paper introduces a novel spatial interpolation pipeline that achieves reliable interpolation results and produces a novel heatmap representation with uncertainty information encoded. We leverage imputation reference data from Graph Neural Networks (GNNs) to enhance visualization reliability and temporal resolution. By integrating Principal Neighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), our model effectively learns the spatiotemporal dependencies. Furthermore, we propose an extrinsic, static visualization technique for interpolation-based heatmaps that effectively communicates the uncertainties arising from various sources in the interpolated map. Through a set of use cases, extensive evaluations on real-world datasets, and user studies, we demonstrate our model's superior performance for data imputation, the improvements to the interpolant with reference data, and the effectiveness of our visualization design in communicating uncertainties.","accessible_pdf":null,"authors":[{"affiliation":"East China Normal University","email":"jtchen@stu.ecnu.edu.cn","name":"Juntong Chen"},{"affiliation":"East China Normal University","email":"huayuan221@gmail.com","name":"Huayuan Ye"},{"affiliation":"East China Normal University","email":"10215102469@stu.ecnu.edu.cn","name":"He Zhu"},{"affiliation":"Zhejiang University","email":"siwei.fu@zju.edu.cn","name":"Siwei Fu"},{"affiliation":"School of Computer Science and Technology","email":"cbwang@cs.ecnu.edu.cn","name":"Changbo Wang"},{"affiliation":"East China Normal University","email":"chli@cs.ecnu.edu.cn","name":"Chenhui Li"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"10233019-3fcc-4f01-b478-96b0ec01cf29","image_caption":"","keywords":["Spatial interpolation","spatiotemporal data","uncertainty visualization","graph neural network"],"open_access_supplemental_link":"https://github.com/jtchen2k/relmap/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.01240","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1643","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full22","session_room":"Hall E2","session_room_id":"e2","session_title":"Multivariate and Time","session_uid":"10233019-3fcc-4f01-b478","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Multivariate and Time"],"time_stamp":"2025-11-05T08:42:00.000Z","title":"RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-Il9UvlqnNA"},{"UID":"10595e17-8ad8-4932-b661-e5d6764c8082","abstract":"You Only Have Seven Seconds is a cinematic artistic visualization derived from an interactive AI art installation (ReCollection) that explores how machine intelligence can reassemble collective human memory through language input. Motivated by the artist\u2019s personal response to her grandmother\u2019s cognitive decline and informed by current research in Critical Dementia Studies that advocates for reimagining\u2014rather than repairing\u2014memory, the project collects and curates whispered, seven-second recollections and machine-generated image data from installation participants. These selected datasets are transformed into a generative visualization presented as\na cinematic art using custom-designed AI systems and experimental visualization strategies. This paper introduces the conceptual foundations and technical development of this project, with emphasis on data collection, experimental visualization, narrative construction, sound design, and intelligent system integration. Through the design of artificial memory, You Only Have Seven Seconds constructs a dynamic archive of shared, ephemeral recollections for storytelling.","accessible_pdf":null,"authors":[{"affiliation":"university of california, Santa Barbara","email":"zhangweidilydia@gmail.com","name":"weidi zhang"},{"affiliation":"The University of Sheffield","email":"lijiaozi.cheng@gmail.com","name":"Lijiaozi Cheng"},{"affiliation":"Zurich University of the Arts","email":"paulschmidt786@gmail.com","name":"Paul Schmidt"},{"affiliation":"Minus AI","email":"rodgerljl@msn.com","name":"Jieliang Luo"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"10595e17-8ad8-4932-b661-e5d6764c8082","image_caption":"","keywords":["Generative Design","Experimental Data Visualization","Computer Animation","AI Art."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1076","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap2","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Papers","session_uid":"10595e17-8ad8-4932-b661","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Papers"],"time_stamp":"2025-11-06T08:42:00.000Z","title":"You Only Have Seven Seconds: From Intimate Whispers to Shared Worlds in Participatory Data-Driven Cinematic Art","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VhkjwheU9Bc"},{"UID":"12e7281f-5e70-4d16-a9a0-a87a0d46df5c","abstract":"Visualization grammars from ggplot2 to Vega-Lite are based on the Grammar of Graphics (GoG), our most comprehensive formal theory of visualization. The GoG helped expand the expressive gamut of visualization by moving beyond fixed chart types and towards a design space of composable operators. Yet, the resultant design space has surprising limitations, inconsistencies, and cliffs---even seemingly simple charts like mosaics, waffles, and ribbons fall out of scope of most GoG implementations. To author such charts, visualization designers must either rely on overburdened grammar developers to implement purpose-built mark types (thus reintroducing the issues of typologies) or drop to lower-level frameworks. In response, we present GoFish: a declarative visualization grammar that formalizes Gestalt principles (e.g., uniform spacing, containment, and connection) that have heretofore been complected in GoG constructs. These graphical operators achieve greater expressive power than their predecessors by enabling recursive composition: they can be nested and overlapped arbitrarily. Through a diverse example gallery, we demonstrate how graphical operators free users to arrange shapes in many different ways while retaining the benefits of high-level grammars like scale resolution and coordinate transform management. Recursive composition naturally yields an infinite design space that blurs the boundary between an expressive, low-level grammar and a concise, high-level one. In doing so, we point towards an updated theory of visualization, one that is open to an innumerable space of graphic representations instead of limited to a fixed set of good designs.","accessible_pdf":"Accessible","authors":[{"affiliation":"Massachusetts Institute of Technology","email":"jopo@mit.edu","name":"Josh Pollock"},{"affiliation":"MIT","email":"arvindsatya@mit.edu","name":"Arvind Satyanarayan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"12e7281f-5e70-4d16-a9a0-a87a0d46df5c","image_caption":"","keywords":["Grammar of Graphics","Graphical operators","Gestalt principles","Relational paradigm"],"open_access_supplemental_link":"https://github.com/starfish-graphics/gofish-graphics","open_access_supplemental_question":"Our work includes a live website (https://gofish.graphics/) with installation instructions, a tutorial, a live code editor, and 30 examples.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1078","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full5","session_room":"Hall M2","session_room_id":"m2","session_title":"Authoring and Design","session_uid":"12e7281f-5e70-4d16-a9a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Authoring and Design"],"time_stamp":"2025-11-05T14:57:00.000Z","title":"GoFish: A Grammar of More Graphics!","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/S3LGLxyblpM"},{"UID":"135b9aea-b3cc-4bf7-880f-117393bf1f70","abstract":"Encoding uncertainty in timelines can provide more precise and informative visualizations (e.g., visual representations of unsure times or locations in event planning timelines). To evaluate the effectiveness of different temporal and categorical uncertainty representations on timelines, we conducted a mixed-methods user study with 81 participants on uncertainty in activity recall timelines (ARTs). We find that participants' accuracy is better when temporal uncertainty is encoded using transparency instead of dashing, and that a participant's visual encoding preference does not always align with their performance (e.g., they performed better with a less-preferred visual encoding technique). Additionally, qualitative findings show that existing biases of an individual alter their interpretation of ARTs. A copy of our study materials is available at https://osf.io/98p6m/.","accessible_pdf":null,"authors":[{"affiliation":"Northeastern University","email":"potter.v@northeastern.edu","name":"Veronika Potter"},{"affiliation":"Northeastern University","email":"le.ha1@northeastern.edu","name":"Ha Le"},{"affiliation":"Northeastern University","email":"syeda.u@northeastern.edu","name":"Uzma Haque Syeda"},{"affiliation":"Northeastern University","email":"s.intille@northeastern.edu","name":"Stephen Intille"},{"affiliation":"Northeastern University","email":"m.borkin@neu.edu","name":"Michelle Borkin"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"135b9aea-b3cc-4bf7-880f-117393bf1f70","image_caption":"","keywords":["Timelines","Uncertainty Visualization","Evaluation Study."],"open_access_supplemental_link":"https://osf.io/98p6m/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://osf.io/98p6m/","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1068","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"135b9aea-b3cc-4bf7-880f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T08:30:00.000Z","title":"An Evaluation of Temporal and Categorical Uncertainty on Timelines: A Case Study in Human Activity Recall Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"13eab0df-a9ad-4614-bc89-4a645ab9438a","abstract":"Ordering has been extensively studied in many visualization applications, such as axis and matrix reordering, for the simple reason that the order will greatly impact the perceived pattern of data. Many quality metrics concerning data pattern, perception, and aesthetics are proposed, and respective optimization algorithms are developed. However, the optimization problems related to ordering are often difficult to solve (e.g., TSP is NP-complete), and developing specialized optimization algorithms is costly. In this paper, we propose Versatile Ordering Network (VON), which automatically learns the strategy to order given a quality metric. VON uses the quality metric to evaluate its solutions, and leverages reinforcement learning with a greedy rollout baseline to improve itself. This keeps the metric transparent and allows VON to optimize over different metrics. Additionally, VON uses the attention mechanism to collect information across scales and reposition the data points with respect to the current context. This allows VONs to deal with data points following different distributions. We examine the effectiveness of VON under different usage scenarios and metrics. The results demonstrate that VON can produce comparable results to specialized solvers.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Zehua Yu"},{"email":null,"name":"Weihan Zhang"},{"email":null,"name":"Sihan Pan"},{"email":null,"name":"Jun Tao"}],"award":"","doi":"10.1109/TVCG.2024.3520208","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"13eab0df-a9ad-4614-bc89-4a645ab9438a","image_caption":"","keywords":["Measurement","Visualization","Attention mechanisms","Neural networks","Layout","Data visualization","Reinforcement learning","Optimization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.12759","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3520208","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full24","session_room":"Hall M2","session_room_id":"m2","session_title":"Ordering and Layout","session_uid":"13eab0df-a9ad-4614-bc89","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Ordering and Layout"],"time_stamp":"2025-11-05T13:48:00.000Z","title":"Versatile Ordering Network: An Attention-Based Neural Network for Ordering Across Scales and Quality Metrics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/1kLUiSSxIuM"},{"UID":"14bce997-e03b-4057-aa4e-89ac30159f51","abstract":"Modern machine learning often relies on optimizing a neural network\u2019s parameters using a loss function to learn complex features. Beyond training, examining the loss function with respect to a network\u2019s parameters (i.e., as a loss landscape) can reveal insights into the architecture and learning process. While the local structure of the loss landscape surrounding an individual solution can be characterized using a variety of approaches, the global structure of a loss landscape, which includes potentially many local minima corresponding to different solutions, remains far more difficult to conceptualize and visualize. To address this difficulty, we introduce LossLens, a visual analytics framework that explores loss landscapes at multiple scales. LossLens integrates metrics from global and local scales into a comprehensive visual representation, enhancing model diagnostics. We demonstrate LossLens through two case studies: visualizing how residual connections influence a ResNet-20, and visualizing how physical parameters influence a physics-informed neural network solving a simple convection problem.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Tiankai Xie"},{"email":null,"name":"Jiaqing Chen"},{"email":null,"name":"Yaoqing Yang"},{"email":null,"name":"Caleb Geniesse"},{"email":null,"name":"Ge Shi"},{"email":null,"name":"Ajinkya Jeevan Chaudhari"},{"email":null,"name":"John Kevin Cava"},{"email":null,"name":"Michael W. Mahoney"},{"email":null,"name":"Talita Perciano"},{"email":null,"name":"Gunther H. Weber"},{"email":null,"name":"Ross Maciejewski"}],"award":"","doi":"10.1109/MCG.2024.3509374","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"14bce997-e03b-4057-aa4e-89ac30159f51","image_caption":"","keywords":["Measurement","Neural networks","Loss measurement","Training","Computational modeling","Load modeling","Data analysis","Analytical models","Visual analytics","Computer architecture","Machine learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.13321","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3509374","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga2","session_room":"Hall E1","session_room_id":"e1","session_title":"Visual Analytics Methods, Tools, and Infrastructure","session_uid":"14bce997-e03b-4057-aa4e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visual Analytics Methods, Tools, and Infrastructure"],"time_stamp":"2025-11-06T10:51:00.000Z","title":"LossLens: Diagnostics for Machine Learning Through Loss Landscape Visual Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JfetpDJuh5Q"},{"UID":"14e8969b-dbaf-41b3-8330-a0decc6b53b0","abstract":"In this work we study the identification of spatial correlation in distributions of 2D scalar fields, presented across different forms of visual displays. We study simple visual displays that directly show color-mapped scalar fields, namely those drawn from a distribution, and whether humans can identify strongly correlated spatial regions in these displays. In this setting, the recognition of correlation requires making judgments on a set of fields, rather than just one field. Thus, in our experimental design we compare two basic visualization designs: animation-based displays against juxtaposed views of scalar fields, along different choices of color scales. Moreover, we investigate the impacts of the distribution itself, controlling for the level of spatial correlation and discriminability in spatial\nscales. Our study\u2019s results illustrate the impacts of these distribution characteristics, while also highlighting how different visual displays impact the types of judgments made in assessing spatial correlation. Supplemental material is available at https://osf.io/zn4qy/","accessible_pdf":null,"authors":[{"affiliation":"Vanderbilt University","email":"yayan.zhao@vanderbilt.edu","name":"Yayan Zhao"},{"affiliation":"Vanderbilt University","email":"matthew.berger@vanderbilt.edu","name":"Matthew Berger"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"14e8969b-dbaf-41b3-8330-a0decc6b53b0","image_caption":"","keywords":["Evaluation","spatial correlation","color maps"],"open_access_supplemental_link":"https://osf.io/zn4qy","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.17997","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1495","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full33","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Encoding & Comprehension","session_uid":"14e8969b-dbaf-41b3-8330","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Encoding & Comprehension"],"time_stamp":"2025-11-05T08:30:00.000Z","title":"Evaluating judgements of spatial correlation in visual displays of scalar field distributions","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"1513daf8-e0da-48f1-b800-f7b0975869ff","abstract":"We present PiCCL (Pictorial Chart Composition Language), a new language that enables users to easily create pictorial charts using a set of simple operators. To support systematic construction while addressing the main challenge of expressive pictorial chart authoring\u2013manual composition and fine-tuning of visual properties\u2013PiCCL introduces a parametric representation that integrates data-driven chart generation with graphical composition. It also employs a lazy data-binding mechanism that automatically synthesizes charts. PiCCL is grounded in a comprehensive analysis of real-world pictorial chart examples. We describe PiCCL\u2019s design and its implementation as piccl.js, a JavaScript-based library. To evaluate PiCCL, we showcase a gallery that demonstrates its expressiveness and report findings from a user study assessing the usability of piccl.js. We conclude with a discussion of PiCCL\u2019s limitations and potential, as well as future research directions.","accessible_pdf":null,"authors":[{"affiliation":"Shandong University","email":"202315173@mail.sdu.edu.cn","name":"Haoyan Shi"},{"affiliation":"Renmin University of China","email":"cloudseawang@gmail.com","name":"Yunhai Wang"},{"affiliation":"Renmin University of China","email":"chenjunhao11@ruc.edu.cn","name":"Junhao Chen"},{"affiliation":"Microsoft Research","email":"clwang15uw@gmail.com","name":"Chenglong Wang"},{"affiliation":"Yonsei University","email":"b.lee@yonsei.ac.kr","name":"Bongshin Lee"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1513daf8-e0da-48f1-b800-f7b0975869ff","image_caption":"","keywords":["pictorial charts","data-driven composition","chart composition","parametric representation"],"open_access_supplemental_link":"https://osf.io/5eqb7","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2006","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full27","session_room":"Hall M2","session_room_id":"m2","session_title":"The Color and the Shape","session_uid":"1513daf8-e0da-48f1-b800","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The Color and the Shape"],"time_stamp":"2025-11-06T09:30:00.000Z","title":"PiCCL: Data-Driven Mark Composition of Bespoke Pictorial Chart","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gHCENa3C9X4"},{"UID":"1628b4eb-473e-4157-9d19-44990b75efc7","abstract":"Information visualizations are powerful tools that help users quickly identify patterns, trends, and outliers, facilitating informed decision-making.  However, when visualizations incorporate deceptive design elements\u2014such as truncated or inverted axes, unjustified 3D effects, or violations of best practices\u2014they can mislead viewers and distort understanding, spreading misinformation. While some deceptive tactics are obvious, others subtly manipulate perception while maintaining a fa\u00e7ade of legitimacy. As Vision-Language Models (VLMs) are increasingly used to interpret visualizations, especially by non-expert users, it is critical to understand how susceptible these models are to deceptive visual designs. In this study, we conduct an in-depth evaluation of VLMs' ability to interpret misleading visualizations. By analyzing over 16,000  responses from ten different models across eight distinct types of misleading chart designs, we demonstrate that most VLMs are deceived by them. This leads to altered interpretations of charts, despite the underlying data remaining the same. Our findings highlight the need for robust safeguards in VLMs against visual misinformation.","accessible_pdf":null,"authors":[{"affiliation":"York University","email":"ridwanmahbub26@gmail.com","name":"Ridwan Mahbub"},{"affiliation":"York University","email":"saidulis@yorku.ca","name":"Mohammed Saidul Islam"},{"affiliation":"York University","email":"tahmid20@yorku.ca","name":"Md Tahmid Rahman Laskar"},{"affiliation":"York University","email":"mizanurr@yorku.ca","name":"Mizanur Rahman"},{"affiliation":"University of Alberta","email":"mnayeem@ualberta.ca","name":"Mir Tafseer Nayeem"},{"affiliation":"York University","email":"enamulh@yorku.ca","name":"Enamul Hoque"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1628b4eb-473e-4157-9d19-44990b75efc7","image_caption":"","keywords":["Misleading Visualizations","Large Language Models","Vision Language Models","Taxonomy","Evaluation"],"open_access_supplemental_link":"https://github.com/vis-nlp/visDeception","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1327","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short10","session_room":"Hall E","session_room_id":"e1_e2","session_title":"VGTC Awards & Best Short Papers","session_uid":"1628b4eb-473e-4157-9d19","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VGTC Awards & Best Short Papers"],"time_stamp":"2025-11-04T11:24:00.000Z","title":"The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VdSWiJuX_x8"},{"UID":"164ea18d-29f7-49a8-87e4-4d95da79af04","abstract":"Professional visualization design has become an increasingly important area of inquiry, yet much of the field\u2019s discourse remains anchored in researcher-centered contexts. Studies of design practice often focus on individual designers\u2019 decisions and reflections, offering limited insight into the collaborative and systemic dimensions of professional work. In this paper, we propose a systems-level reframing of design judgment grounded in the coordination and adaptation that sustain progress amid uncertainty, constraint, and misalignment. Drawing on sustained engagement across multiple empirical studies\u2014including ethnographic observation of design teams and qualitative studies of individual practitioners\u2014we identify recurring episodes in which coherence was preserved not by selecting an optimal option, but by repairing alignment, adjusting plans, and reframing goals. We interpret these dynamics through the lens of Joint Cognitive Systems, which provide tools for analyzing how judgment emerges as a distributed capacity within sociotechnical activity. This perspective surfaces often-invisible work in visualization design and offers researchers a new conceptual vocabulary for studying how design activity is sustained in practice.","accessible_pdf":null,"authors":[{"affiliation":"Purdue University","email":"parsonsp@purdue.edu","name":"Paul Parsons"},{"affiliation":"Monash University","email":"arranridley@gmail.com","name":"Arran Ridley"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"164ea18d-29f7-49a8-87e4-4d95da79af04","image_caption":"","keywords":["Human-centered Computing [Visualization]"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1237","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"164ea18d-29f7-49a8-87e4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T09:24:00.000Z","title":"Judgment as Coordination: A Joint Systems View of Visualization Design Practice","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"184d62a7-fb82-4d6f-9819-6f552e1c6c28","abstract":"Visualizing data often entails data transformations that can reveal and hide information, operations we dub disclosure tactics. Whether designers hide information intentionally or as an implicit consequence of other design choices, tools and frameworks for visualization offer little explicit guidance on disclosure. To systematically characterize how visualizations can limit access to an underlying dataset, we contribute a content analysis of 425 examples of visualization techniques sampled from academic papers in the visualization literature, resulting in a taxonomy of disclosure tactics. Our taxonomy organizes disclosure tactics based on how they change the data representation underlying a chart, providing a systematic way to reason about design trade-offs in terms of what information is revealed, distorted, or hidden. We demonstrate the benefits of using our taxonomy by showing how it can guide reasoning in design scenarios where disclosure is a first-order consideration. Adopting disclosure as a framework for visualization research offers new perspective on authoring tools, literacy, uncertainty communication, personalization, and ethical design.","accessible_pdf":null,"authors":[{"affiliation":"University of Chicago","email":"krisha@uchicago.edu","name":"Krisha Mehta"},{"affiliation":"University of Chicago","email":"glk@uchicago.edu","name":"Gordon Kindlmann"},{"affiliation":"University of Chicago","email":"kalea@uchicago.edu","name":"Alex Kale"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"184d62a7-fb82-4d6f-9819-6f552e1c6c28","image_caption":"","keywords":["Information disclosure"],"open_access_supplemental_link":"https://github.com/krisha-mehta/DisclosureInDataVis","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1255","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full30","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Trust No One","session_uid":"184d62a7-fb82-4d6f-9819","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Trust No One"],"time_stamp":"2025-11-07T09:06:00.000Z","title":"Designing for Disclosure in Data Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mavB31i3NwM"},{"UID":"18f5b9fb-e80f-415d-a701-6c7256d9711b","abstract":"An essential task in analyzing collaborative design processes, such as those that are part of workshops in design studies,\nis identifying design outcomes and understanding how the collaboration between participants formed the results and led to decision-\nmaking. However, findings are typically restricted to a consolidated textual form based on notes from interviews or observations. A\nchallenge arises from integrating different sources of observations, leading to large amounts and heterogeneity of collected data. To\naddress this challenge we propose a practical, modular, and adaptable framework of workshop setup, multimodal data acquisition,\nAI-based artifact extraction, and visual analysis. Our interactive visual analysis system, reCAPit, allows the flexible combination of\ndifferent modalities, including video, audio, notes, or gaze, to analyze and communicate important workshop findings. A multimodal\nstreamgraph displays activity and attention in the working area, temporally aligned topic cards summarize participants\u2019 discussions,\nand drill-down techniques allow inspecting raw data of included sources. As part of our research, we conducted six workshops across\ndifferent themes ranging from social science research on urban planning to a design study on band-practice visualization. The latter\ntwo are examined in detail and described as case studies. Further, we present considerations for planning workshops and challenges\nthat we derive from our own experience and the interviews we conducted with workshop experts. Our research extends existing\nmethodology of collaborative design workshops by promoting data-rich acquisition of multimodal observations, combined AI-based\nextraction and interactive visual analysis, and transparent dissemination of results.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Stuttgart","email":"maurice.koch@visus.uni-stuttgart.de","name":"Maurice Koch"},{"affiliation":"University of Stuttgart","email":"nelusa.pathmanathan@visus.uni-stuttgart.de","name":"Nelusa Pathmanathan"},{"affiliation":"University of Stuttgart","email":"weiskopf@visus.uni-stuttgart.de","name":"Daniel Weiskopf"},{"affiliation":"University of Stuttgart","email":"kuno.kurzhals@visus.uni-stuttgart.de","name":"Kuno Kurzhals"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"18f5b9fb-e80f-415d-a701-6c7256d9711b","image_caption":"","keywords":["Collaborative workshops","multimodal analysis of design processes","combination of AI and interactive visualization","design study methodology","eye tracking","reCAPit"],"open_access_supplemental_link":"https://doi.org/10.18419/DARUS-5166","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.06117","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1683","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full5","session_room":"Hall M2","session_room_id":"m2","session_title":"Authoring and Design","session_uid":"18f5b9fb-e80f-415d-a701","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Authoring and Design"],"time_stamp":"2025-11-05T15:33:00.000Z","title":"A Multimodal Framework for Understanding Collaborative Design Processes","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/S3LGLxyblpM"},{"UID":"19140b17-2652-4f77-a749-4088ec870052","abstract":"When designed deliberately, data visualizations can become powerful persuasive tools, influencing viewers' opinions, values, and actions. While researchers have begun studying this issue (e.g., to evaluate the effects of persuasive visualization), we argue that a fundamental mechanism of persuasion resides in rhetorical construction, a perspective inadequately addressed in current visualization research. To fill this gap, we present a focused analysis of octopus maps, a visual genre that has maintained persuasive power across centuries and achieved significant social impact. Employing rhetorical schema theory, we collected and analyzed 90 octopus maps spanning from the 19th century to contemporary times. We closely examined how octopus maps implement their persuasive intents and constructed a design space that reveals how visual metaphors are strategically constructed and what common rhetorical strategies are applied to components such as maps, octopus imagery, and text. Through the above analysis, we also uncover a set of interesting findings. For instance, contrary to the common perception that octopus maps are primarily a historical phenomenon, our research shows that they remain a lively design convention in today\u2019s digital age. Additionally, while most octopus maps stem from Western discourse that views the octopus as an evil symbol, some designs offer alternative interpretations, highlighting the dynamic nature of rhetoric across different sociocultural settings. Lastly, drawing from the lessons provided by octopus maps, we discuss the associated ethical concerns of persuasive visualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"Iowa State University","email":"daocheng@iastate.edu","name":"Daocheng Lin"},{"affiliation":"Fudan University","email":"wangyifanlea@gmail.com","name":"Yifan Wang"},{"affiliation":"Shanghai Jiao Tong University","email":"flora20@sjtu.edu.cn","name":"Yutong Yang"},{"affiliation":"Fudan University","email":"xingyulan96@gmail.com","name":"Xingyu Lan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"19140b17-2652-4f77-a749-4088ec870052","image_caption":"","keywords":["Persuasive Visualization","Map","Visual Rhetoric","Visualization Rhetoric","Metaphorical Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.11903","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1249","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full20","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Maps & Spatial Vis","session_uid":"19140b17-2652-4f77-a749","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Maps & Spatial Vis"],"time_stamp":"2025-11-05T08:42:00.000Z","title":"Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/nkeAR0PgfSw"},{"UID":"19655cb7-e771-464e-8d3b-3aa6e9ca1736","abstract":"In temporal (event-based) networks, time is a continuous axis, with real-valued time coordinates for each node and edge. Computing a layout for such graphs means embedding the node trajectories and edge surfaces over time in a 2D+t space, known as the space-time cube. Currently, these space-time cube layouts are visualized through animation or by slicing the cube at regular intervals. However, both techniques present problems such as below-average performance on tasks as well as loss of precision and difficulties in selecting timeslice intervals. In this article, we present TimeLighting, a novel visual analytics approach to visualize and explore temporal graphs embedded in the space-time cube. Our interactive approach highlights node trajectories and their movement over time, visualizes node \u201caging\u201d, and provides guidance to support users during exploration by indicating interesting time intervals (\u201cwhen\u201d) and network elements (\u201cwhere\u201d) are located for a detail-oriented investigation. This combined focus helps to gain deeper insights into the temporal network's underlying behavior. We assess the utility and efficacy of our approach through two case studies and qualitative expert evaluation. The results demonstrate how TimeLighting supports identifying temporal patterns, extracting insights from nodes with high activity, and guiding the exploration and analysis process.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Velitchko Andreev Filipov"},{"email":null,"name":"Davide Ceneda"},{"email":null,"name":"Daniel Archambault"},{"email":null,"name":"Alessio Arleo"}],"award":"","doi":"10.1109/TVCG.2024.3514858","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"19655cb7-e771-464e-8d3b-3aa6e9ca1736","image_caption":"","keywords":["Event detection","Visual analytics","Layout","Animation","Trajectory"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://ieeexplore.ieee.org/abstract/document/10787140","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-02-0154.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full22","session_room":"Hall E2","session_room_id":"e2","session_title":"Multivariate and Time","session_uid":"19655cb7-e771-464e-8d3b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Multivariate and Time"],"time_stamp":"2025-11-05T09:18:00.000Z","title":"TimeLighting: Guided Exploration of 2D Temporal Network Projections","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-Il9UvlqnNA"},{"UID":"1a6e079e-6775-4742-80e6-1bfa9f98605d","abstract":"Adjusting transparency is a common method of mitigating occlusion but is often detrimental for understanding the relative depth relationships between objects as well as removes potentially important information from the occluding object. We propose using dichoptic opacity, a novel method for occlusion management that contrasts the transparency of occluders presented to each eye. This allows for better simultaneous understanding of both occluder and occluded. A user study highlights the technique's potential, showing strong user engagement and a clear preference for dichoptic opacity over traditional presentations. While it does not determine optimal transparency values, it reveals promising trends in both percentage and range that merit further investigation.","accessible_pdf":"Accessible","authors":[{"affiliation":"Newcastle University","email":"g.bell1@newcastle.ac.uk","name":"George Bell"},{"affiliation":"Newcastle University","email":"alma.cantu@ncl.ac.uk","name":"Alma Cantu"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1a6e079e-6775-4742-80e6-1bfa9f98605d","image_caption":"","keywords":["Occlusion","dichoptic presentation","transparency","3D surface","stereoscopy"],"open_access_supplemental_link":"https://github.com/georgebellbell/Dichoptic-Opacity-Experiment-Data.git","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://doi.org/10.48550/arXiv.2506.22841","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1262","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"1a6e079e-6775-4742-80e6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T14:45:00.000Z","title":"Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"1a87c6dd-4c17-4b68-91d5-c59f2bb0fe4c","abstract":"In recent years, data-driven stories have found a firm footing in journalism, business, and education. They leverage visualization and storytelling to convey information to broader audiences. Likewise, immersive technologies, like augmented and virtual reality devices, provide excellent potential for exploring and explaining data, thus inviting research on how data-driven storytelling transfers to immersive environments. To gain a better understanding of this exciting novel research area, we conducted a scoping review on the emerging notion of immersive data-driven storytelling, extended by surveying immersive data journalism and by analyzing immersive games, selected based on community reviews and tags. We present our methodology for the survey and discuss prominent themes that coalesce the knowledge we extracted from the literature, journalism, and games. These themes include, among others, the spatial embodiment of narration, the incorporation of the users and their context into narratives, and the balance between guiding the user versus promoting serendipity. Our discussion of these themes reveals research opportunities and challenges that will inform the design of immersive data-driven stories in the future.","accessible_pdf":"No","authors":[{"email":null,"name":"Juli\u00e1n M\u00e9ndez"},{"email":null,"name":"Weizhou Luo"},{"email":null,"name":"Rufat Rzayev"},{"email":null,"name":"Wolfgang B\u00fcschel"},{"email":null,"name":"Raimund Dachselt"}],"award":"","doi":"10.1109/TVCG.2025.3531138","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"1a87c6dd-4c17-4b68-91d5-c59f2bb0fe4c","image_caption":"","keywords":["Data visualization","Journalism","Reviews","Games","Libraries","Surveys","Media","Data analysis","Codes","Artificial intelligence"],"open_access_supplemental_link":null,"open_access_supplemental_question":"We followed the PRISMA methodology for scoping reviews, and provide both the codebook and all the analyzed papers, journal applications, and games with the corresponding coding to each of them. All the respective filtering steps and reasons for it are thoroughly documented in the supplementary material and in the paper.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://imld.de/idds","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0610]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full26","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Stories and the Journalists Who Tell Them","session_uid":"1a87c6dd-4c17-4b68-91d5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Stories and the Journalists Who Tell Them"],"time_stamp":"2025-11-07T09:06:00.000Z","title":"Immersive Data-Driven Storytelling: Scoping an Emerging Field Through the Lenses of Research, Journalism, and Games","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/j1bJGzUi0jM"},{"UID":"1acd711b-69a4-486f-9955-0f6adb58292b","abstract":"Recent work suggests that shape can encode quantitative data via a mapping between value and spatial frequency (SF). However, the set-size effect when perceiving multiple SF based items remains unclear. While automatic feature extraction has been found to be less affected by set size (number of items in a group), higher-level processes for making perceptual decisions tend to require increased cognitive demand. To investigate the set-size effect on comparing integrated SF based items, we used a risk-based scenario to assess discrimination performance. Participants were asked to discriminate between pairs of maps containing multiple SF glyphs, in which each glyph represents one of four discrete levels (none, low, medium, high), forming an aggregate \u201crisk strength\u201d per map. The set size was also adjusted across conditions, ranging from small (3 items) to large (7 items). Discrimination sensitivity is modeled with a logistic function and response time with a mixed-effect linear model. Results show that smaller set sizes and lower overall strength enable more precise discrimination, with faster response times for larger differences between maps. Incorporating set size and overall strength into the logistic model, we found that these variables both independently and jointly influence discrimination sensitivity. We suggest these results point towards capacity-limited processes rather than purely automatic ensemble coding. Our findings highlight the importance of set size and overall signal strength when presenting multiple SF glyphs in data visualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"King's College London","email":"yiran.2.li@kcl.ac.uk","name":"Yiran Li"},{"affiliation":"King's College London","email":"shan.shao@kcl.ac.uk","name":"Shan Shao"},{"affiliation":"King's College London","email":"peter.baudains@kcl.ac.uk","name":"Peter Baudains"},{"affiliation":"King's College London","email":"andrew.meso@kcl.ac.uk","name":"Andrew Meso"},{"affiliation":"King's College London","email":"nickholliman@gmail.com","name":"Nick Holliman"},{"affiliation":"King's College London","email":"alfie.abdulrahman@gmail.com","name":"Alfie Abdul-Rahman"},{"affiliation":"Kings College London","email":"rita.borgo@kcl.ac.uk","name":"Rita Borgo"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1acd711b-69a4-486f-9955-0f6adb58292b","image_caption":"","keywords":["Radial spatial frequency","Vizent glyph","set size","visual discrimination","aggregation","psychometric function"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://kclpure.kcl.ac.uk/portal/en/publications/set-size-matters-capacity-limited-perception-of-grouped-spatial-f","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1557","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full27","session_room":"Hall M2","session_room_id":"m2","session_title":"The Color and the Shape","session_uid":"1acd711b-69a4-486f-9955","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The Color and the Shape"],"time_stamp":"2025-11-06T08:54:00.000Z","title":"Set Size Matters: Capacity-Limited Perception of Grouped Spatial-Frequency Glyphs","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gHCENa3C9X4"},{"UID":"1af413bb-b7e1-4e7a-bbba-ec896ec760e7","abstract":"Various computational approaches predict chromatin structure, yielding concrete models that position genomic loci in physical space and help reveal genome organization and function. While prior visualization research has explored data and task abstractions for genomics, the design space for depicting these three-dimensional (3D) genome models---and associated genome-mapped data---remains unclear. In this paper, we investigate the visualization of genomic data with a spatial component. First, we systematically survey how 3D genome models are used and depicted in computational biology. We analyze over 300 papers with figures that visualize 3D genomic data and categorize the methods for visual representation. From this survey, we derive a design space for visualizing 3D genome data, identifying common patterns and key properties such as representation, visual channels, and composition. We position these findings within an existing genomics visualization taxonomy, refining and extending existing classifications. Second, we augment Gosling, a declarative visualization grammar for genomics, to support 3D genomic data. Our integration enables expressive authoring of visualizations that connect traditional genome-mapped information with 3D genome models, emphasizing their spatial characteristics. To demonstrate its utility, we employ our extended grammar to recreate interactive examples, showcasing its ability to represent complex visual designs. Comprehensive examples and an interactive editor are available at https://3d.gosling-lang.org.","accessible_pdf":"Accessible","authors":[{"affiliation":"Harvard Medical School","email":"david_kouril@hms.harvard.edu","name":"David Kou\u0159il"},{"affiliation":"Harvard Medical School","email":"trevor_manz@g.harvard.edu","name":"Trevor Manz"},{"affiliation":"Harvard Medical School","email":"sehi_lyi@hms.harvard.edu","name":"Sehi L'Yi"},{"affiliation":"Harvard Medical School","email":"nils@hms.harvard.edu","name":"Nils Gehlenborg"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1af413bb-b7e1-4e7a-bbba-ec896ec760e7","image_caption":"","keywords":["genomic data","3D visualization","declarative grammars"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://doi.org/10.31219/osf.io/dtr6u_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1080","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full32","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Volumes & 3D","session_uid":"1af413bb-b7e1-4e7a-bbba","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Volumes & 3D"],"time_stamp":"2025-11-06T10:15:00.000Z","title":"Design Space and Declarative Grammar for 3D Genomic Data Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"1b09b6a7-1b8c-49d2-9726-095246b41f5b","abstract":"We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite for applying and evaluating material layout strategies to generate stiff, lightweight designs in 3D domains. SGLDBench provides a seamlessly integrated simulation and analysis framework, including six reference strategies and a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results. This facilitates the systematic analysis and comparison of design strategies based on the mechanical properties they achieve. SGLDBench enables the evaluation of diverse load conditions and, through the tight integration of the solver, supports high-resolution designs and stiffness analysis. Additionally, SGLDBench emphasizes visual analysis to explore the relationship between the geometric structure of a design and the distribution of stresses, offering insights into the specific properties and behaviors of different design strategies. SGLDBench\u2019s specific features are highlighted through several experiments, comparing the results of reference strategies with respect to geometric and mechanical properties.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Junpeng Wang"},{"email":null,"name":"Dennis Bukenberger"},{"email":null,"name":"Simon Niedermayr"},{"email":null,"name":"Christoph Neuhauser"},{"email":null,"name":"Jun Wu"},{"email":null,"name":"R\u00fcdiger Westermann"}],"award":"","doi":"10.1109/TVCG.2025.3573774","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"1b09b6a7-1b8c-49d2-9726-095246b41f5b","image_caption":"","keywords":["Stress","Lattices","Three-dimensional printing","Visualization","MATLAB","Finite element analysis","Layout","Benchmark testing","Solids","Boundary conditions"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2501.03068","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-10-0949.R1 ]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full29","session_room":"Hall M1","session_room_id":"m1","session_title":"Transportation, Buildings, and Urban Vis","session_uid":"1b09b6a7-1b8c-49d2-9726","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Transportation, Buildings, and Urban Vis"],"time_stamp":"2025-11-06T13:48:00.000Z","title":"SGLDBench: A Benchmark Suite for Stress-Guided Lightweight 3D Designs","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/hoMn73KnXCg"},{"UID":"1b3b1b15-e378-4c65-8984-717a7f7baa3a","abstract":"Systems relying on ML have become ubiquitous, but so has biased behavior within them. Research shows that bias significantly affects stakeholders\u2019 trust in systems and how they use them. Further, stakeholders of different backgrounds view and trust the same systems differently. Thus, how ML models\u2019 behavior is explained plays a key role in comprehension and trust. We survey explainability visualizations, creating a taxonomy of design characteristics. We conduct user studies to evaluate five state-of-the-art visualization tools (LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how taxonomy characteristics affect comprehension, bias perception, and trust for non-expert ML users. Surprisingly, we find an inverse relationship between comprehension and trust: the better users understand the models, the less they trust them. We investigate the cause and find that this relationship is strongly mediated by bias perception: more comprehensible visualizations increase people\u2019s perception of bias, and increased bias perception reduces trust. We confirm this relationship is causal: Manipulating explainability visualizations to control comprehension, bias perception, and trust, we show that visualization design can significantly (p < 0.001) increase comprehension, increase perceived bias, and reduce trust. Conversely, reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust even when comprehension remains high. Our work advances understanding of how comprehension affects trust and systematically investigates visualization\u2019s role in facilitating responsible ML applications.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Massachusetts Amherst","email":"zhannakaufma@umass.edu","name":"Zhanna Kaufman"},{"affiliation":"University of Massachusetts Amherst","email":"mendres@umass.edu","name":"Madeline Endres"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"},{"affiliation":"University of Massachusetts","email":"brun@cs.umass.edu","name":"Yuriy Brun"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1b3b1b15-e378-4c65-8984-717a7f7baa3a","image_caption":"","keywords":["Visualization design","explainability","trust","bias in machine learning."],"open_access_supplemental_link":"https://osf.io/c87xm/?view_only=31dfc1f2a7624f5cb20b0f07d3730df3","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.00140","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1446","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full12","session_room":"Hall E1","session_room_id":"e1","session_title":"Explanation, Exploration, and Model Configuration","session_uid":"1b3b1b15-e378-4c65-8984","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explanation, Exploration, and Model Configuration"],"time_stamp":"2025-11-06T08:54:00.000Z","title":"Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/h8CsAxh4LWg"},{"UID":"1e9e6140-6939-402f-8781-8420814824be","abstract":"Visualizations support critical decision making in domains like health risk communication. This is particularly important for those at higher health risks and their care providers, allowing for better risk interpretation which may lead to more informed decisions. However, the kinds of visualizations used to represent data may impart biases that influence data interpretation and decision making. Both continuous representations using bar charts and discrete representations using icon arrays are pervasive in health risk communication, but express the same quantities using fundamentally different visual paradigms. We conducted a series of studies to investigate how bar charts, icon arrays, and their layout (juxtaposed, explicit encoding, explicit encoding plus juxtaposition) affect the perception of value comparison and subsequent decision-making in health risk communication. Our results suggest that icon arrays and explicit encoding combined with juxtaposition can optimize for both accurate difference estimation and perceptual biases in decision making. We also found misalignment between estimation accuracy and decision making, as well as between low and high literacy groups, emphasizing the importance of tailoring visualization approaches to specific audiences and evaluating visualizations beyond perceptual accuracy alone. This research contributes empirically-grounded design recommendations to improve comparison in health risk communication and support more informed decision-making across domains.","accessible_pdf":null,"authors":[{"affiliation":"University of North Carolina at Chapel Hill","email":"jadekandel@gmail.com","name":"Jade Kandel"},{"affiliation":"The University of North Carolina at Chapel Hill","email":"liujiayi@unc.edu","name":"Jiayi Liu"},{"affiliation":"University of North Carolina-Chapel Hill","email":"zeyuwang@cs.unc.edu","name":"Arran Zeyu Wang"},{"affiliation":"University of North Carolina-Chapel Hill","email":"chint@cs.unc.edu","name":"Chin Tseng"},{"affiliation":"University of North Carolina-Chapel Hill","email":"danielle.szafir@cs.unc.edu","name":"Danielle Szafir"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1e9e6140-6939-402f-8781-8420814824be","image_caption":"","keywords":["Information Visualization","Graphical Perception","Health Risk Communication","Bar Chart","Icon Array","Composition"],"open_access_supplemental_link":"https://osf.io/4nhtf/?view_only=b5168623250a44aa826ab027df42d87b","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1933","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full15","session_room":"Hall E2","session_room_id":"e2","session_title":"From Data to Meaning","session_uid":"1e9e6140-6939-402f-8781","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["From Data to Meaning"],"time_stamp":"2025-11-06T11:15:00.000Z","title":"Graphical Perception of Icon Arrays versus Bar Charts for Value Comparisons in Health Risk Communication","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/v-iY-PKyXHs"},{"UID":"1ee915a9-ec02-4dd8-85c3-f03ce8b9bb54","abstract":"Kaleidoscope of Thoughts is a 360\u00b0 immersive visualization project that explores the multilayered dynamics of human cognition and emotion. Using participatory data collection and experimental visualization strategies, the project captures and translates raw thoughts in multiple languages into an immersive, multisensory environment. Visual motifs, such as cyanotype textures, Perlin noise patterns, and algorithmically generated butterfly effects based on the Lorenz attractor, function as representations of emotional turbulence and transformation. The installation foregrounds how minor shifts in cognitive perspective can significantly alter emotional well-being, fostering a shared experience that encourages reflection and connection among diverse audiences (see Figure 1).","accessible_pdf":null,"authors":[{"affiliation":"Arizona State University","email":"meghasachdeva135@gmail.com","name":"Megha Sachdeva"},{"affiliation":"Media and Immersive Experience (MIX) Center","email":"cami.gregory02@gmail.com","name":"Cambelle Gregory"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1ee915a9-ec02-4dd8-85c3-f03ce8b9bb54","image_caption":"","keywords":["Experimental visualization; Participatory data collection; Immersive storytelling; Emotional data representation; Multilingual art installation; Interactive media; Cognitive visualization; Collective experience"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1053","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap1","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Pictorial","session_uid":"1ee915a9-ec02-4dd8-85c3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Pictorial"],"time_stamp":"2025-11-06T10:51:00.000Z","title":"Kaleidoscope of Thoughts: Experimental Visualization of Cognitive Turbulence","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/NZ0Wwrqv6fA"},{"UID":"1f23efb5-deca-4391-a7c4-e455fce74cb9","abstract":"Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification \u21d2 Interpretation \u21d2 Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.","accessible_pdf":null,"authors":[{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"lihaoxuan@zju.edu.cn","name":"Haoxuan Li"},{"affiliation":"Zhejiang University","email":"wenzhen@zju.edu.cn","name":"Zhen Wen"},{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"jiangqiqi348284@gmail.com","name":"Qiqi Jiang"},{"affiliation":"Zhejiang University, Hangzhou, China","email":"3220101835@zju.edu.cn","name":"Chenxiao Li"},{"affiliation":"Zhejiang University","email":"wuyw0701@foxmail.com","name":"Yuwei Wu"},{"affiliation":"Zhejiang University","email":"yyc_yang@zju.edu.cn","name":"Yuchen Yang"},{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"wangyiyao@zju.edu.cn","name":"Yiyao Wang"},{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"huangxiuqi@zju.edu.cn","name":"Xiuqi Huang"},{"affiliation":"Zhejiang University","email":"minfeng_zhu@zju.edu.cn","name":"Minfeng Zhu"},{"affiliation":"Zhejiang University","email":"chenvis@zju.edu.cn","name":"Wei Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"1f23efb5-deca-4391-a7c4-e455fce74cb9","image_caption":"","keywords":["Large Language Models","Mechanistic Interpretability","Visual Analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1254","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full31","session_room":"Hall E1","session_room_id":"e1","session_title":"VA for AI","session_uid":"1f23efb5-deca-4391-a7c4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VA for AI"],"time_stamp":"2025-11-05T08:30:00.000Z","title":"ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LajPfGQe32k"},{"UID":"1ff943c3-7fda-4abd-a68d-9831af7b78f2","abstract":"We introduce DashSpace, a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar. To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants\u2014whose presence is shown using 3D avatars and webcam feeds\u2014to interact with the shared space synchronously, both co-located and remotely. We present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.","accessible_pdf":"No","authors":[{"email":null,"name":"Marcel Borowski"},{"email":null,"name":"Peter W. S. Butcher"},{"email":null,"name":"Janus Bager Kristensen"},{"email":null,"name":"Jonas Oxenb\u00f8ll Petersen"},{"email":null,"name":"Panagiotis D. Ritsos"},{"email":null,"name":"Clemens N. Klokmose"},{"email":null,"name":"Niklas Elmqvist"}],"award":"","doi":"10.1109/TVCG.2025.3537679","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"1ff943c3-7fda-4abd-a68d-9831af7b78f2","image_caption":"","keywords":["Data visualization","Three-dimensional displays","Collaboration","Grammar","Software","Visualization","Hardware","Data analysis","Mobile handsets","Media"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://pure.au.dk/portal/en/publications/dashspace-a-live-collaborative-platform-for-immersive-and-ubiquit","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-06-0442.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full19","session_room":"Hall E1","session_room_id":"e1","session_title":"Interaction, Provenance, and Collaboration","session_uid":"1ff943c3-7fda-4abd-a68d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Interaction, Provenance, and Collaboration"],"time_stamp":"2025-11-06T15:21:00.000Z","title":"DashSpace: A Live Collaborative Platform for Immersive and Ubiquitous Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/253qu_2o0K0"},{"UID":"200b7b6d-abf5-4cb3-b8b4-3da334833db3","abstract":"Data Humanism is a human-centered design approach that emphasizes the personal, contextual, and imperfect nature of data. Despite its growing influence among practitioners, the 13 principles outlined in Giorgia Lupi\u2019s visual manifesto remain loosely defined in research contexts, creating a gap between design practice and systematic application. Through a mixed-methods approach, including a systematic literature review, multimedia analysis, and expert interviews, we present a characterization of Data Humanism principles for visualization researchers. Our characterization provides concrete definitions that maintain interpretive flexibility in operationalizing design choices. We  validate our work through direct consultation with Lupi. Moreover, we leverage the characterization to decode a visualization work, mapping Data Humanism principles\nto specific visual design choices. Our work creates a common language for human-centered visualization, bridging the gap between practice and research for future applications and evaluations.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Zurich","email":"alhazwani@ifi.uzh.ch","name":"Ibrahim Al-Hazwani"},{"affiliation":"University of Bergen","email":"ke.zhang@uib.no","name":"Ke Er Zhang"},{"affiliation":"University of Bergen","email":"laura.garrison@uib.no","name":"Laura Garrison"},{"affiliation":"University of Zurich","email":"bernard@ifi.uzh.ch","name":"J\u00fcrgen Bernard"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"200b7b6d-abf5-4cb3-b8b4-3da334833db3","image_caption":"","keywords":["Data Humanism","Critical Data Visualization","Human-Centered Visualization"],"open_access_supplemental_link":"https://osf.io/tmnra/?view_only=06edc12deed64382a354dac3a7a62049","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1208","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"200b7b6d-abf5-4cb3-b8b4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T08:39:00.000Z","title":"Data Humanism Decoded: A Characterization of its Principles to Bridge Data Visualization Researchers and Practitioners","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"21590a9a-129a-4f34-b457-0e490dc57bce","abstract":"Transformer-based language models have demonstrated remarkable capabilities across various tasks, yet their internal mechanisms\u2014such as layered representations, distributed attention, and evolving token semantics\u2014remain challenging to interpret. We present Semantic Pathway, an interactive visual analytics tool designed to reveal how token representations evolve across layers in autoregressive Transformer models such as GPT-2. The system integrates layerwise semantic trajectories, attention overlays, and output probability views into a unified interface, enabling users to trace how meaning accumulates and decisions emerge during generation. To reduce visual and interaction complexity, Semantic Pathway incorporates attention-based influence filtering, optional nearest-token projections, and a Compare Mode for analyzing divergence across alternate outputs. The design prioritizes interpretability and usability, supporting both fine-grained inspection and high-level exploration of sequence modeling behavior. This work contributes to ongoing efforts to make language models more interpretable, educationally accessible, and open to diagnostic insight.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"mithilesh.singh@stonybrook.edu","name":"Mithilesh Kumar Singh"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"21590a9a-129a-4f34-b457-0e490dc57bce","image_caption":"","keywords":["Large Language Models","Interpretability","Hidden States","Attention Weights","Semantic Pathway Visualization","Token Influence","Interactive Visualization","Transformer Models"],"open_access_supplemental_link":"https://vimeo.com/1080448380/2f38ba5dfc","open_access_supplemental_question":"We provide a detailed walkthrough video demonstrating all features of our interactive tool, and highlight our design decisions for interpretability. The system and visual encodings are thoroughly documented, supporting reproducibility and educational reuse.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1322","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short11","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Multi-modal and Spatial","session_uid":"21590a9a-129a-4f34-b457","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Multi-modal and Spatial"],"time_stamp":"2025-11-06T09:06:00.000Z","title":"Semantic Pathway: An Interactive Visualization of Hidden States and Token Influence in LLMs","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"22ccb527-7a1d-49e1-81b8-9e4f335089e1","abstract":"Accounting for individual differences can improve the effectiveness of visualization design. While the role of visual attention in visualization interpretation is well recognized, existing work often overlooks how this behavior varies based on visual literacy levels. Based on data from a 235-participant user study covering three visualization tests (mini-VLAT, CALVI, and SGL), we show that distinct attention patterns in visual data exploration can correlate with participants' literacy levels: While experts (high-scorers) generally show a strong attentional focus, novices (low-scorers) focus less and explore more. We then propose two computational models leveraging these insights: Lit2Sal -- a novel visual saliency model that predicts observer attention given their visualization literacy level, and Sal2Lit -- a model to predict visual literacy from human visual attention data. Our quantitative and qualitative evaluation demonstrates that Lit2Sal outperforms state-of-the-art saliency models with literacy-aware considerations. Sal2Lit predicts literacy with 86% accuracy using a single attention map, providing a time-efficient supplement to literacy assessment that only takes less than a minute. Taken together, our unique approach to consider individual differences in salience models and visual attention in literacy assessments paves the way for new directions in personalized visual data communication to enhance understanding.","accessible_pdf":null,"authors":[{"affiliation":"Georgia Institute of Technology","email":"minsuk@gatech.edu","name":"Minsuk Chang"},{"affiliation":"University of Stuttgart","email":"yao.wang@vis.uni-stuttgart.de","name":"Yao Wang"},{"affiliation":"University of Washington","email":"wwill@cs.washington.edu","name":"Huichen Wang"},{"affiliation":"Georgia Institute of Technology","email":"zhouyuanhong0510@gmail.com","name":"Yuanhong Zhou"},{"affiliation":"University of Stuttgart","email":"andreas.bulling@vis.uni-stuttgart.de","name":"Andreas Bulling"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"22ccb527-7a1d-49e1-81b8-9e4f335089e1","image_caption":"","keywords":["Visualization Literacy Assessment","Visual Attention and Saliency","Visual Saliency Models"],"open_access_supplemental_link":"https://osf.io/2crb9/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.03713","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1490","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full35","session_room":"Room 0.11 + 0.12","session_room_id":"0_11_0_12","session_title":"Visualization Literacy","session_uid":"22ccb527-7a1d-49e1-81b8","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization Literacy"],"time_stamp":"2025-11-07T08:42:00.000Z","title":"Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/R3RWFseiZmo"},{"UID":"258e8105-5eb0-4316-99bc-40522d2b6d15","abstract":"Causality helps people reason about and understand complex systems, particularly through what-if analyses that explore how interventions might alter outcomes. Although existing methods embrace causal reasoning using interventions and counterfactual analysis, they primarily focus on effects at the population level. These approaches often fall short in systems characterized by significant heterogeneity, where the impact of an intervention can vary widely across subgroups. To address this challenge, we present XplainAct, a visual analytics framework that supports simulating, explaining, and reasoning interventions at the individual level within subpopulations. We demonstrate the effectiveness of XplainAct through two case studies: investigating opioid-related deaths in epidemiology and analyzing voting inclinations in the presidential election.","accessible_pdf":"Accessible","authors":[{"affiliation":"Stony Brook University","email":"yanming.zhang@stonybrook.edu","name":"Yanming Zhang"},{"affiliation":"Stony Brook University","email":"khegde@cs.stonybrook.edu","name":"Krishnakumar Hegde"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"258e8105-5eb0-4316-99bc-40522d2b6d15","image_caption":"","keywords":["Explainable AI","Causality","Visual Analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1350","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short12","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Viz for AI & AI for Viz","session_uid":"258e8105-5eb0-4316-99bc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Viz for AI & AI for Viz"],"time_stamp":"2025-11-05T10:51:00.000Z","title":"XplainAct: Visualization for Personalized Intervention Insights","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"25ece7fd-1a08-463e-b526-9e6a16a62b59","abstract":"Volumetric medical imaging technologies produce detailed 3D representations of anatomical structures. However, effective medical data visualization and exploration pose significant challenges, especially for individuals with limited medical expertise. We introduce a novel XR-based system with two key innovations: (1) a coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models and (2) a multimodal interaction framework combining hand gestures with LLM-enabled voice commands. We conduct preliminary evaluations, including a 15-participant user study and expert interviews, to demonstrate the system's abilities to enhance spatial understanding and reduce cognitive load. Experimental results show notable improvements in task completion times, usability metrics, and interaction effectiveness enhanced by LLM-driven voice control. While identifying areas for future refinement, our findings highlight the potential of this immersive visualization system to advance medical training and clinical practice. Our demo application and supplemental materials are available for download at: https://osf.io/bpjq5/.","accessible_pdf":null,"authors":[{"affiliation":"CUHK","email":"1155203213@link.cuhk.edu.hk","name":"Qixuan Liu"},{"affiliation":"CUHK","email":"shiqiu@cuhk.edu.hk","name":"Shi Qiu"},{"affiliation":"CUHK","email":"yqwang@cse.cuhk.edu.hk","name":"Yinqiao Wang"},{"affiliation":"CUHK","email":"xiwenwu@cuhk.edu.hk","name":"Xiwen Wu"},{"affiliation":"CUHK","email":"kennethchok@surgery.cuhk.edu.hk","name":"Kenneth Siu Ho Chok"},{"affiliation":"The Chinese University of Hong Kong","email":"cwfu@cse.cuhk.edu.hk","name":"Chi-Wing Fu"},{"affiliation":"The Chinese University of Hong Kong","email":"pheng@cse.cuhk.edu.hk","name":"Pheng Ann Heng"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"25ece7fd-1a08-463e-b526-9e6a16a62b59","image_caption":"","keywords":["Medical Visualization","Medical XR","Multimodal Interaction"],"open_access_supplemental_link":"https://osf.io/bpjq5/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"http://arxiv.org/abs/2506.22926","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1094","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"25ece7fd-1a08-463e-b526","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T15:03:00.000Z","title":"Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"266aaadd-b874-4660-9061-3746f4ce4747","abstract":"Railroad networks are complex systems where a single disruption can have cumulative effects, impacting services' planned schedules and degrading the network's performance. To support railroad planners in identifying these disruptions, we propose a Visual Analytics approach with multiple coordinated views: a map, calendar heatmap, Marey graph, and a multi-line chart. Our proposed approach enables the exploration and analysis of the disruptions' spatial and temporal patterns and delay accumulation to identify vulnerable segments in the railroad network. We assess the effectiveness of our approach through an expert interview highlighting how the accumulation of delays and disruptions is lucidly communicated and provides valuable insights into their spread across the network. We discuss the outcomes of the expert interview alongside the limitations we identified and how we resolve these. Finally, we illustrate directions for future work, including online data to assist railroad planners with real-time monitoring for proactive decision-making and improved operations.","accessible_pdf":"Accessible","authors":[{"affiliation":"Institute of Visual Computing and Human-Centered Technology","email":"sandhya.rajendran@tuwien.ac.at","name":"Sandhya Rajendran"},{"affiliation":"Eindhoven University of Technology","email":"a.arleo@tue.nl","name":"Alessio Arleo"},{"affiliation":"University of Cologne","email":"landesberger@cs.uni-koeln.de","name":"Tatiana von Landesberger"},{"affiliation":"Institute of Visual Computing and Human-Centered Technology","email":"miksch@ifs.tuwien.ac.at","name":"Silvia Miksch"},{"affiliation":"University of Cologne","email":"max.sondag93@gmail.com","name":"Max Sondag"},{"affiliation":"TU Wien","email":"michaela.tuscher@tuwien.ac.at","name":"Michaela Tuscher"},{"affiliation":"Institute of Visual Computing and Human-Centered Technology","email":"velitchko.filipov@tuwien.ac.at","name":"Velitchko Filipov"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"266aaadd-b874-4660-9061-3746f4ce4747","image_caption":"","keywords":["Spatio-temporal Data","Networks"],"open_access_supplemental_link":"https://osf.io/d2fzh/?view_only=2715e41e0e6c4e23812ec2465f4da48e","open_access_supplemental_question":"thoroughly documented source code, particularly substantial supplemental material","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1171","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"266aaadd-b874-4660-9061","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:00:00.000Z","title":"Don't Stop Me Now: Visualizing Disruptions in Railroad Networks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"2686600c-1989-405f-bf3e-34862f75517c","abstract":"The high performance of tree ensemble classifiers benefits from a large set of rules, which, in turn, makes the models hard to understand. To improve interpretability, existing methods extract a subset of rules for approximation using model reduction techniques. However, by focusing on the reduced rule set, these methods often lose fidelity and ignore anomalous rules that, despite their infrequency, play crucial roles in real-world applications. This paper introduces a scalable visual analysis method to explain tree ensemble classifiers that contain tens of thousands of rules. The key idea is to address the issue of losing fidelity by adaptively organizing the rules as a hierarchy rather than reducing them. To ensure the inclusion of anomalous rules, we develop an anomaly-biased model reduction method to prioritize these rules at each hierarchical level. Synergized with this hierarchical organization of rules, we develop a matrix-based hierarchical visualization to support exploration at different levels of detail. Our quantitative experiments and case studies demonstrate how our method fosters a deeper understanding of both common and anomalous rules, thereby enhancing interpretability without sacrificing comprehensiveness.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Zhen Li"},{"email":null,"name":"Weikai Yang"},{"email":null,"name":"Jun Yuan"},{"email":null,"name":"Jing Wu"},{"email":null,"name":"Changjian Chen"},{"email":null,"name":"Yao Ming"},{"email":null,"name":"Fan Yang"},{"email":null,"name":"Hui Zhang"},{"email":null,"name":"Shixia Liu"}],"award":"","doi":"10.1109/TVCG.2024.3514115","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"2686600c-1989-405f-bf3e-34862f75517c","image_caption":"","keywords":["Random forests","Visualization","Reduced order systems","Analytical models","Predictive models","Decision trees","Scalability","Credit cards","Organizations","Medical services"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2409.03164","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3514115","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full23","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Of Trees and Treemaps","session_uid":"2686600c-1989-405f-bf3e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Of Trees and Treemaps"],"time_stamp":"2025-11-05T10:51:00.000Z","title":"RuleExplorer: A Scalable Matrix Visualization for Understanding Tree Ensemble Classifiers","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/yiiG1AZHJ1E"},{"UID":"26915272-41cc-4235-9570-43e77fa756af","abstract":"The burgeoning online video game industry has sparked intense competition among providers to both expand their user base and retain existing players, particularly within social interaction genres. To anticipate player churn, there is an increasing reliance on machine learning (ML) models that focus on social interaction dynamics. However, the prevalent opacity of most ML algorithms poses a significant hurdle to their acceptance among domain experts, who often view them as \u201copaque models\u201d. Despite the availability of eXplainable Artificial Intelligence (XAI) techniques capable of elucidating model decisions, their adoption in the gaming industry remains limited. This is primarily because non-technical domain experts, such as product managers and game designers, encounter substantial challenges in deciphering the \u201cexplicit\u201d and \u201cimplicit\u201d features embedded within computational models. This study proposes a reliable, interpretable, and actionable solution for predicting player churn by restructuring model inputs into explicit and implicit features. It explores how establishing a connection between explicit and implicit features can assist experts in understanding the underlying implicit features. Moreover, it emphasizes the necessity for XAI techniques that not only offer implementable interventions but also pinpoint the most crucial features for those interventions. Two case studies, including expert feedback and a within-subject user study, demonstrate the efficacy of our approach.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Xiyuan Wang"},{"email":null,"name":"Laixin Xie"},{"email":null,"name":"He Wang"},{"email":null,"name":"Xingxing Xing"},{"email":null,"name":"Wei Wan"},{"email":null,"name":"Ziming Wu"},{"email":null,"name":"Xiaojuan Ma"},{"email":null,"name":"Quan Li"}],"award":"","doi":"10.1109/TVCG.2024.3487974","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"26915272-41cc-4235-9570-43e77fa756af","image_caption":"","keywords":["Games","Predictive models","Social networking (online)","Prediction algorithms","Computational modeling","Industries","Visual analytics","Reliability","Interviews","Explainable AI"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Github repository  contains the source code of the framework, including the Interface: Contains frontend code. Backend: Contains backend code. The counterfactual generation in the backend is inspired by and builds upon the DECE framework: https://github.com/ChengFR/DECE-research.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0603.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full16","session_room":"Hall M1","session_room_id":"m1","session_title":"Games, Sports, and Music","session_uid":"26915272-41cc-4235-9570","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Games, Sports, and Music"],"time_stamp":"2025-11-06T10:51:00.000Z","title":"Deciphering Explicit and Implicit Features for Reliable, Interpretable, and Actionable User Churn Prediction in Online Video Games","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/w72IpVgxklQ"},{"UID":"28a61ed9-3f15-4835-a688-e72b1fd6fa0c","abstract":"Despite the widespread use of Uniform Manifold Approximation and Projection (UMAP), the impact of its stochastic optimization process on the results remains underexplored. We observed that it often produces unstable results where the projections of data points are determined mostly by chance rather than reflecting neighboring structures. To address this limitation, we introduce (r,d)-stability to UMAP: a framework that analyzes the stochastic positioning of data points in the projection space. To assess how stochastic elements\u2014specifically, initial projection positions and negative sampling\u2014impact UMAP results, we introduce \u201cghosts\u201d, or duplicates of data points representing potential positional variations due to stochasticity. We define a data point\u2019s projection as (r,d)-stable if its ghosts perturbed within a circle of radius r in the initial projection remain confined within a circle of radius d for their final positions. To efficiently compute the ghost projections, we develop an adaptive dropping scheme that reduces a runtime up to 60% compared to an unoptimized baseline while maintaining approximately 90% of unstable points. We also present a visualization tool that supports the interactive exploration of the (r,d)-stability of data points. Finally, we demonstrate the effectiveness of our framework by examining the stability of projections of real-world datasets and present usage guidelines for the effective use of our framework.","accessible_pdf":"Accessible","authors":[{"affiliation":"Sungkyunkwan University","email":"mw.jung@skku.edu","name":"Myeongwon Jung"},{"affiliation":"Link\u00f6ping University","email":"tfujiwara@ucdavis.edu","name":"Takanori Fujiwara"},{"affiliation":"Sungkyunkwan University","email":"jmjo@skku.edu","name":"Jaemin Jo"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"28a61ed9-3f15-4835-a688-e72b1fd6fa0c","image_caption":"","keywords":["Dimensionality reduction","manifold learning","stochastic optimization","reliability","visualization","WebGPU"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.17174","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1641","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full11","session_room":"Hall E2","session_room_id":"e2","session_title":"Embeddings & Metrics","session_uid":"28a61ed9-3f15-4835-a688","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Embeddings & Metrics"],"time_stamp":"2025-11-05T13:00:00.000Z","title":"GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gvbjZE1CgLQ"},{"UID":"28a70097-2b4f-4ac5-9bb0-7b452093c16b","abstract":"In-vitro fertilization (IVF) has become standard practice to address infertility, which affects more than one in ten couples in the US. However, current protocols yield relatively low success rates of about 20% per treatment cycle. A critical but complex and time-consuming step is the grading and selection of embryos for implantation. Although incubators with time-lapse microscopy have enabled computational analysis of embryo development, existing automated approaches either require extensive manual annotations or use opaque deep learning models that are hard for clinicians to validate and trust. We present EmbryoProfiler, a visual analytics system collaboratively developed with embryologists, biologists, and machine learning researchers to support clinicians in visually assessing embryo viability from time-lapse microscopy imagery. Our system incorporates a deep learning pipeline that automatically annotates microscopy images and extracts clinically interpretable features relevant for embryo grading. Our contributions include: (1) a semi-automatic, visualization-based workflow that guides clinicians through fertilization assessment, developmental timing evaluation, morphological inspection, and comparative analysis of embryos; (2) innovative interactive visualizations, such as cell-shape plots, designed to facilitate efficient analysis of morphological and developmental characteristics; and (3) an integrated, explainable machine learning classifier offering transparent, clinically-informed embryo viability scoring to predict live birth outcomes. Quantitative evaluation of our classifier and qualitative case studies conducted with practitioners demonstrate that EmbryoProfiler enables clinicians to make better-informed embryo selection decisions, potentially leading to improved clinical outcomes in IVF treatments.","accessible_pdf":null,"authors":[{"affiliation":"Harvard University","email":"jknittel@seas.harvard.edu","name":"Johannes Knittel"},{"affiliation":"Harvard University","email":"simonwarchol@g.harvard.edu","name":"Simon Warchol"},{"affiliation":"Harvard University","email":"jakob.troidl@googlemail.com","name":"Jakob Troidl"},{"affiliation":"Tufts University","email":"camelia_daniela.brumar@tufts.edu","name":"Camelia D. Brumar"},{"affiliation":"Harvard University","email":"yuy068@g.harvard.edu","name":"Helen Yang"},{"affiliation":"Harvard Medical School","email":"eric.moerth@gmx.at","name":"Eric M\u00f6rth"},{"affiliation":"New York University","email":"rk4815@nyu.edu","name":"Robert Kr\u00fcger"},{"affiliation":"Harvard University","email":"dan.needleman@gmail.com","name":"Daniel Needleman"},{"affiliation":"Tel Aviv University","email":"dalitb@tasmc.health.gov.il","name":"Dalit Ben-Yosef"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"28a70097-2b4f-4ac5-9bb0-7b452093c16b","image_caption":"","keywords":["in-vitro fertilization","embryo selection","visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1893","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full8","session_room":"Hall M1","session_room_id":"m1","session_title":"Biomedical Visualization","session_uid":"28a70097-2b4f-4ac5-9bb0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Biomedical Visualization"],"time_stamp":"2025-11-06T15:45:00.000Z","title":"EmbryoProfiler: A Visual Clinical Decision Support System for IVF","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rgq3gdIQGxk"},{"UID":"29656e00-00d7-4080-b589-2d7c838c72be","abstract":"Design studies aim to develop visualization solutions for real-world problems across various application domains. Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies. In this paper, we conducted a multi-stage qualitative study to fill this gap, which involved 30 design study researchers from diverse backgrounds and expertise levels. Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them. We further compiled the roles that LLMs can play across different stages of the design study process. Our findings highlight practical implications to inform visualization practitioners, and also provide a framework for leveraging LLMs to facilitate the design study process in visualization research.","accessible_pdf":"Accessible","authors":[{"affiliation":"Singapore Management University","email":"haywardryan@foxmail.com","name":"Shaolun Ruan"},{"affiliation":"The Hong Kong University of Science and Technology","email":"rshengac@connect.ust.hk","name":"Rui Sheng"},{"affiliation":"Nanyang Technological University","email":"xiaolin004@e.ntu.edu.sg","name":"Xiaolin Wen"},{"affiliation":"Zhejiang University","email":"wangjiachen@zju.edu.cn","name":"Jiachen Wang"},{"affiliation":"Singapore Management University","email":"tianyizhang.2023@phdcs.smu.edu.sg","name":"Tianyi Zhang"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"},{"affiliation":"Monash University","email":"tgdwyer@gmail.com","name":"Tim Dwyer"},{"affiliation":"Singapore Management University","email":"jiannanli@smu.edu.sg","name":"Jiannan Li"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"29656e00-00d7-4080-b589-2d7c838c72be","image_caption":"","keywords":["Design Study","Large Language Models (LLMs)","Qualitative Study","Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.10024","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1207","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full28","session_room":"Hall E1","session_room_id":"e1","session_title":"The VIS in GenAI","session_uid":"29656e00-00d7-4080-b589","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The VIS in GenAI"],"time_stamp":"2025-11-05T13:00:00.000Z","title":"Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zLchh59XyM4"},{"UID":"29b8a92c-ef97-44df-b315-a903fe54fa53","abstract":"The rapid growth and availability of event sequence data across domains requires effective analysis and exploration methods to facilitate decision-making. Visual analytics combines computational techniques with interactive visualizations, enabling the identification of patterns, anomalies, and attribute interactions. However, existing approaches frequently overlook the interplay between temporal and multivariate attributes. We introduce EventBox, a novel data representation and visual encoding approach for analyzing groups of events and their multivariate attributes. We have integrated EventBox into Sequen-C, a visual analytics system for the analysis of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we have added user-driven transformations, including alignment, sorting, substitution and aggregation. To enhance analytical depth, we incorporate automatically generated statistical analyses, providing additional insight into the significance of attribute interactions. We evaluated our approach involving 21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T framework to assess visualization value, user performance metrics completing a series of tasks, and interactive sessions with domain experts. We also present three case studies with real-world healthcare data demonstrating how EventBox and its integration into Sequen-C reveal meaningful patterns, anomalies, and insights. These results demonstrate that our work advances visual analytics by providing a flexible solution for exploring temporal and multivariate attributes in event sequences.","accessible_pdf":null,"authors":[{"affiliation":"University of Sheffield","email":"l.montanagonzalez@sheffield.ac.uk","name":"Luis Rene Montana Gonzalez"},{"affiliation":"University of Sheffield","email":"jgmagallanescastaneda1@sheffield.ac.uk","name":"Jessica Magallanes"},{"affiliation":"University of Sheffield","email":"m.juarez@sheffield.ac.uk","name":"Miguel A Juarez"},{"affiliation":"University of Sheffield","email":"s.mason@sheffield.ac.uk","name":"Suzanne Mason"},{"affiliation":"University of Sheffield","email":"a.j.narracott@sheffield.ac.uk","name":"Andrew Narracott"},{"affiliation":"Sheffield Teaching Hospitals Foundation Trust","email":"lindsey.vangemeren@nhs.net","name":"Lindsey van Gemeren"},{"affiliation":"Sheffield Teaching Hospitals Foundation Trust","email":"steven.wood8@nhs.net","name":"Steven Wood"},{"affiliation":"University of Sheffield","email":"m.villa-uriol@sheffield.ac.uk","name":"Maria-Cruz Villa-Uriol"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"29b8a92c-ef97-44df-b315-a903fe54fa53","image_caption":"","keywords":["Temporal event sequences","multivariate attribute analysis","temporal analysis","visual analytics","interactive visualization."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.14685","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1663","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full22","session_room":"Hall E2","session_room_id":"e2","session_title":"Multivariate and Time","session_uid":"29b8a92c-ef97-44df-b315","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Multivariate and Time"],"time_stamp":"2025-11-05T08:54:00.000Z","title":"EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-Il9UvlqnNA"},{"UID":"2a0c8e77-b8b1-4b7d-ab16-0034e10e1dee","abstract":"Grounded image generation enables precise spatial control over pre-trained diffusion models, making it possible to use chart images as visual guides during the image generation process. This paper presents a novel approach that generates cohesive and natural illustrations of vertical bar charts by integrating real-world object images as visual embellishments. The proposed pipeline takes an object image and a reference bar chart as input and produces an embellished bar chart that follows the structure of the input chart. To preserve chart integrity by maintaining the count, position, size, and order of data values, we introduce a strategy that anchors the top and bottom parts of the object image to the top and bottom of each bar while allowing the middle section to be filled by the generation model. We demonstrate the efficacy of the pipeline through the generation of 4,725 chart images followed by evaluation based on three integrity metrics. The results show that generation success rate is affected by various factors. Finally, we discuss future directions for generalization and better usability of our pipeline, and limitations of evaluation used in our approach.","accessible_pdf":null,"authors":[{"affiliation":"KAIST","email":"ksg_0320@kaist.ac.kr","name":"Seon Gyeom Kim"},{"affiliation":"KAIST","email":"jaeyoungchoi@kaist.ac.kr","name":"Jae Young Choi"},{"affiliation":"KAIST","email":"phillip0701@kaist.ac.kr","name":"Yuseung Lee"},{"affiliation":"KAIST","email":"jhyun513@kaist.ac.kr","name":"Jaeryung Chung"},{"affiliation":"Adobe Research","email":"ryrossi@adobe.com","name":"Ryan Rossi"},{"affiliation":"Adobe Research","email":"jkil@adobe.com","name":"Jihyung Kil"},{"affiliation":"Adobe Research","email":"eunyee@adobe.com","name":"Eunyee Koh"},{"affiliation":"KAIST","email":"takyeonlee@kaist.ac.kr","name":"Tak Yeon Lee"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2a0c8e77-b8b1-4b7d-ab16-0034e10e1dee","image_caption":"","keywords":["Chart Embellishment","Grounded Image Generation"],"open_access_supplemental_link":"https://groundedchartgeneration.github.io/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1326","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"2a0c8e77-b8b1-4b7d-ab16","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T10:15:00.000Z","title":"Grounded Generation of Embellished Bar Chart Ensuring Chart Integrity","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"2b485865-8584-417a-bd55-5a961dcff5ef","abstract":"Annotation is often a time-consuming but fruitful activity in data analysis contexts. The manual labor required to create useful annotations is a barrier that keeps users from documenting their analysis, especially intermediate results. To address the needs of exploration and annotation alike, we propose integrating annotation with lens-based interactions, combining both with guidance. We investigate the exploration-annotation requirement space, identifying challenges and extracting five design requirements for annotation in exploration contexts. Based on this investigation, we designed ANNOLENS\u2014a concrete instantiation of such a system that lets users explore and annotate dimensionality-reduced multivariate data. It employs a dual-lens approach for contrastive exploration, using guidance to steer users toward interesting data subsets and attributes. Annotation is directly integrated into the lenses, letting users quickly annotate hunches and discoveries. Automated merging and linking serve to simplify annotation management and reduce disruptions. In a pilot study, we conducted a preliminary evaluation of our approach, which indicated that users find it easy to annotate data and were able to incorporate their knowledge and unique perspective into the process. A free copy of this paper and all supplemental materials are available at https://osf.io/zpu6c/.","accessible_pdf":null,"authors":[{"affiliation":"University of Stuttgart","email":"franziska.becker@vis.uni-stuttgart.de","name":"Franziska Becker"},{"affiliation":"University of Stuttgart","email":"steffen.koch@vis.uni-stuttgart.de","name":"Steffen Koch"},{"affiliation":"University of Stuttgart","email":"research@blascheck.eu","name":"Tanja Blascheck"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2b485865-8584-417a-bd55-5a961dcff5ef","image_caption":"","keywords":["Annotation","exploration","guidance","visual analytics."],"open_access_supplemental_link":"https://osf.io/zpu6c/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1333","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"2b485865-8584-417a-bd55","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T09:24:00.000Z","title":"AnnoLens: Exploration and Annotation through Lens-Based Guidance","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"2b7f7862-26fb-4ce7-81a9-01537d804ead","abstract":"In data-driven storytelling contexts such as data journalism and data videos, data visualizations are often presented alongside real-world imagery to support narrative context. However, these visualizations and contextual images typically remain separated, limiting their combined narrative expressiveness and engagement. Achieving this is challenging due to the need for fine-grained alignment and creative ideation. To address this, we present SceneLoom, a Vision-Language Model (VLM)-powered system that facilitates the coordination of data visualization with real-world imagery based on narrative intents. Through a formative study, we investigated the design space of coordination relationships between data visualization and real-world scenes from the perspectives of visual alignment and semantic coherence. Guided by the derived design considerations, SceneLoom leverages VLMs to extract visual and semantic features from scene images and data visualization, and perform design mapping through a reasoning process that incorporates spatial organization, shape similarity, layout consistency, and semantic binding. The system generates a set of contextually expressive, image-driven design alternatives that achieve coherent alignments across visual, semantic, and data dimensions. Users can explore these alternatives, select preferred mappings, and further refine the design through interactive adjustments and animated transitions to support expressive data communication. A user study and an example gallery validate SceneLoom's effectiveness in inspiring creative design and facilitating design externalization.","accessible_pdf":null,"authors":[{"affiliation":"Fudan University","email":"lgao.lynne@gmail.com","name":"Lin Gao"},{"affiliation":"The Hong Kong University of Science and Technology","email":"lshenaj@connect.ust.hk","name":"Leixian Shen"},{"affiliation":"Fudan Univerisity","email":"yuhengzhao_cn@163.com","name":"Yuheng Zhao"},{"affiliation":"Fudan University","email":"22307130107@m.fudan.edu.cn","name":"Jiexiang Lan"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"},{"affiliation":"Fudan University","email":"simingchen3@gmail.com","name":"Siming Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2b7f7862-26fb-4ce7-81a9-01537d804ead","image_caption":"","keywords":["Creativity Support","Data Communication","Scene Context","Vision-Language Model"],"open_access_supplemental_link":"https://lynnegao.me/scene-loom/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.16466","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1058","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full26","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Stories and the Journalists Who Tell Them","session_uid":"2b7f7862-26fb-4ce7-81a9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Stories and the Journalists Who Tell Them"],"time_stamp":"2025-11-07T08:30:00.000Z","title":"SceneLoom: Communicating Data with Scene Context","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/j1bJGzUi0jM"},{"UID":"2b8f4be2-cade-4f52-8af3-5d874bdb4b77","abstract":"We present interactive data-driven compute overlays for native and\nweb-based 3D geographic map applications based on WebGPU.\nOur data-driven overlays are generated in a multi-step compute\nworkfow from multiple data sources on the GPU. We demonstrate\ntheir potential by showing results from snow cover and avalanche\nsimulations, where simulation parameters can be adjusted interac-\ntively and results are visualized instantly. Benchmarks show that\nour approach can compute large-scale avalanche simulations in mil-\nliseconds to seconds, depending on the size of the terrain and the\nsimulation parameters, which is multiple orders of magnitude faster\nthan a state-of-the-art Python implementation.","accessible_pdf":"Accessible","authors":[{"affiliation":"TU Wien","email":"e11808210@student.tuwien.ac.at","name":"Patrick Komon"},{"affiliation":"TU Vienna","email":"e1326608@student.tuwien.ac.at","name":"Gerald Kimmersdorfer"},{"affiliation":"TU Wien","email":"celarek@cg.tuwien.ac.at","name":"Adam Celarek"},{"affiliation":"TU Wien","email":"waldner@cg.tuwien.ac.at","name":"Manuela Waldner"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2b8f4be2-cade-4f52-8af3-5d874bdb4b77","image_caption":"","keywords":["3D geographic visualization","geographic simulation","WebGPU"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2506.23364","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1251","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"2b8f4be2-cade-4f52-8af3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:27:00.000Z","title":"Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"2b91d1cf-ebf6-46b2-9cbc-cc51da3a56ed","abstract":"Tens of thousands of people have represented data by creating data-encoding textile pieces like blankets, scarves, and more. A prototypical example is the temperature blanket, which represents the weather through rows or blocks of different colors mapped to temperature ranges. While researchers have used fiber arts mediums to create exploratory projects, data visualization and physicalization research has largely not engaged with examples from this enormous and diverse community. We explore the pace of data textiles, or fiber arts that encode information, by surveying creators (i.e., data fiber artists) on their projects and processes. We create a corpus of 159 examples of data textiles and present a schema characterizing the data encoding methods used in these projects. We also gather insights into creators\u2019 data workflows as well as their motives and discoveries through making with their data. Creators of data textiles use distinct processes to map their data, building fabric from component structures and substructures while using material properties like color and texture. From diverse data-tracking procedures, creators use and relate to data in varied ways. Working on these pieces also contributes to the creators\u2019 personal growth and data understanding. Our findings point to new opportunities for visualization, including opportunities to support fiber artists with tools formatted to their needs and opportunities to incorporate concepts from data textiles into other types of visualization (e.g., using texture, structural layouts, colorways).","accessible_pdf":"Accessible","authors":[{"affiliation":"Northeastern University","email":"purdue.sy@northeastern.edu","name":"Sydney Purdue"},{"affiliation":"Northeastern University","email":"eduardopuertac@gmail.com","name":"Eduardo Puerta"},{"affiliation":"Northeastern University","email":"e.bertini@northeastern.edu","name":"Enrico Bertini"},{"affiliation":"Northeastern University","email":"m.tory@northeastern.edu","name":"Melanie Tory"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2b91d1cf-ebf6-46b2-9cbc-cc51da3a56ed","image_caption":"","keywords":["Data physicalization","textiles","data textiles","craft","VisArt","data practices","visual encoding design"],"open_access_supplemental_link":"https://osf.io/t9jhu/","open_access_supplemental_question":"We collected a corpus of 159 examples of data textiles, and we provide this corpus, including all submitted images for the pieces as well as a summary of the description provided, as a part of our supplemental materials. Notably, given the nature of these pieces as artistic objects, we asked our participants for their preferred attribution, and we attribute 142 projects to the name of that creator's choice, with an additional 14 attributed anonymously, and 3 held confidential.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/eyw2r_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1367","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full15","session_room":"Hall E2","session_room_id":"e2","session_title":"From Data to Meaning","session_uid":"2b91d1cf-ebf6-46b2-9cbc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["From Data to Meaning"],"time_stamp":"2025-11-06T10:15:00.000Z","title":"Stitching Meaning: Practices of Data Textile Creators","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/v-iY-PKyXHs"},{"UID":"2bf2b86a-9987-4815-8c2a-8911b3588071","abstract":"We propose a design space for embedded data visualizations in urban environments, structured across multiple dimensions that articulate key contextual and representational characteristics. The design space is grounded in situated and immersive visualization theory, urban informatics, and co-creative ideation workshops. Its dimensions describe different aspects of how data visualizations relate to the physical urban environment and to the viewer. We illustrate the applicability of the design space using mappings to speculative embedded urban visualizations. This conceptual contribution is intended to support designers and researchers in structuring, analyzing, and generating embedded urban visualizations, and serves as a basis for future extensions.","accessible_pdf":null,"authors":[{"affiliation":"Technische Hochschule Mannheim","email":"t.nagel@hs-mannheim.de","name":"Till Nagel"},{"affiliation":"Technische Hochschule Mannheim","email":"c.huber@hs-mannheim.de","name":"Christoph Huber"},{"affiliation":"Technische Hochschule Mannheim","email":"3000176@stud.hs-mannheim.de","name":"Mona Eder"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2bf2b86a-9987-4815-8c2a-8911b3588071","image_caption":"","keywords":["Situated Visualization","Urban Data","Framework."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1271","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"2bf2b86a-9987-4815-8c2a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T09:06:00.000Z","title":"Toward a Design Space for Embedded Urban Data Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"2c73a979-fd41-4019-98ea-12a5581a53f1","abstract":"This article presents a systematic study of the generalization of convolutional neural networks (CNNs) and humans on relational reasoning tasks with bar charts. We first revisit previous experiments on graphical perception and update the benchmark performance of CNNs. We then test the generalization performance of CNNs on a classic relational reasoning task: estimating bar length ratios in a bar chart, by progressively perturbing the standard visualizations. We further conduct a user study to compare the performance of CNNs and humans. Our results show that CNNs outperform humans only when the training and test data have the same visual encodings. Otherwise, they may perform worse. We also find that CNNs are sensitive to perturbations in various visual encodings, regardless of their relevance to the target bars. Yet, humans are mainly influenced by bar lengths. Our study suggests that robust relational reasoning with visualizations is challenging for CNNs. Improving CNNs\u2019 generalization performance may require training them to better recognize task-related visual properties.","accessible_pdf":"No","authors":[{"email":null,"name":"Zhenxing Cui"},{"email":null,"name":"Lu Chen"},{"email":null,"name":"Yunhai Wang"},{"email":null,"name":"Daniel Haehn"},{"email":null,"name":"Yong Wang"},{"email":null,"name":"Hanspeter Pfister"}],"award":"","doi":"10.1109/TVCG.2024.3463800","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"2c73a979-fd41-4019-98ea-12a5581a53f1","image_caption":"","keywords":["Data visualization","Bars","Visualization","Cognition","Standards","Encoding","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":"This work is notable for systematically evaluating convolutional neural networks\u2019 ability to perform relational reasoning on bar charts, a fundamental but underexplored task in visual understanding. It introduces carefully designed synthetic datasets and benchmarks that rigorously test model generalization beyond training distributions, providing valuable insights into current model limitations and directions for improvement.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2503.00086","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3463800","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full25","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception","session_uid":"2c73a979-fd41-4019-98ea","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception"],"time_stamp":"2025-11-06T13:48:00.000Z","title":"Generalization of CNNs on Relational Reasoning with Bar Charts","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/D_j0t7Fy0w0"},{"UID":"2dfe369c-3743-4695-aeba-b0bc4de4429d","abstract":"As biodiversity loss and climate change accelerate, botanical gar-\ndens serve as vital infrastructures for research, education, and con-\nservation. This project focuses on the Arnold Arboretum of Har-\nvard University, a 281-acre living museum founded in 1872 in\nBoston, understood as a hybrid site where scientific inquiry, envi-\nronmental stewardship, and interspecies encounters meet. Drawing\non more than a century of curatorial data, the research combines\nhistorical analysis with computational methods to visualize the in-\ntertwined biographies of plants and people. The resulting digital\nplatform reveals patterns of care and scientific observation, along\nwith the ethical, infrastructural, and collective dimensions embed-\nded in botanical data. Using techniques from artificial intelligence,\ngeospatial mapping, and information design, the project frames the\narboretum as a system of shared agency\u2014an active archive of more-\nthan-human affinities that records the layered memory of curatorial\nlabor, the situated nature of knowledge production, and the poten-\ntial of design to bridge archival record and future care.","accessible_pdf":null,"authors":[{"affiliation":"GRIDH","email":"johan.malmstedt@gmail.com","name":"Johan Malmstedt"},{"affiliation":"University of Groningen","email":"d.rodighiero@rug.nl","name":"Dario Rodighiero"},{"affiliation":"metaLAB (at) Berlin","email":"gn.nann@gmail.com","name":"Giacomo Nanni"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"2dfe369c-3743-4695-aeba-b0bc4de4429d","image_caption":"","keywords":["Index Terms: botanical data","deep learning","digital archives","ethnobotany","historical visualization","interspecies relations","science\nand technology studies (STS)"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1057","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap2","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Papers","session_uid":"2dfe369c-3743-4695-aeba","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Papers"],"time_stamp":"2025-11-06T09:18:00.000Z","title":"Living Library of Trees: Mapping Knowledge Ecology in Arnold Arboretum","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VhkjwheU9Bc"},{"UID":"2e99a9c7-1244-48c4-a5fa-7b3f3df0aa71","abstract":"In this article, we assess the visualization literacy of two prominent Large Language Models (LLMs): OpenAI\u2019s Generative Pretrained Transformers (GPT), the backend of ChatGPT, and Google\u2019s Gemini, previously known as Bard, to establish benchmarks for assessing their visualization capabilities. While LLMs have shown promise in generating chart descriptions, captions, and design suggestions, their potential for evaluating visualizations remains under-explored. Collecting data from humans for evaluations has been a bottleneck for visualization research in terms of both time and money, and if LLMs were able to serve, even in some limited role, as evaluators, they could be a significant resource. To investigate the feasibility of using LLMs in the visualization evaluation process, we explore the extent to which LLMs possess visualization literacy\u2014a crucial factor for their effective utility in the field. We conducted a series of experiments using a modified 53-item Visualization Literacy Assessment Test (VLAT) for and . Our findings indicate that the LLMs we explored currently fail to achieve the same levels of visualization literacy when compared to data from the general public reported in VLAT, and LLMs heavily relied on their pre-existing knowledge to answer questions instead of utilizing the information provided by the visualization when answering questions.","accessible_pdf":"No","authors":[{"email":null,"name":"Jiayi Hong"},{"email":null,"name":"Christian Seto"},{"email":null,"name":"Arlen Fan"},{"email":null,"name":"Ross Maciejewski"}],"award":"","doi":"10.1109/TVCG.2025.3536358","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"2e99a9c7-1244-48c4-a5fa-7b3f3df0aa71","image_caption":"","keywords":["Data visualization","Visualization","Costs","Data models","Benchmark testing","Codes","Training","Data mining","Computational modeling","Cognition"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Our Github repository (https://github.com/VADERASU/llm4viz-experiments) contains detailed instructions on how we generated our charts for testing as well as how we ran our experiments with detailed links for each experiment, thus making our work highly replicatable.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2501.16277v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0543.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full35","session_room":"Room 0.11 + 0.12","session_room_id":"0_11_0_12","session_title":"Visualization Literacy","session_uid":"2e99a9c7-1244-48c4-a5fa","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization Literacy"],"time_stamp":"2025-11-07T09:18:00.000Z","title":"Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/R3RWFseiZmo"},{"UID":"30cf9291-b159-4a33-95bd-47f194b893a0","abstract":"Creating aesthetically pleasing data visualizations remains challenging for users without design expertise or familiarity with visualization tools. To address this gap, we present DataWink, a system that enables users to create custom visualizations by adapting high-quality examples. Our approach combines large multimodal models (LMMs) to extract data encoding from existing SVG-based visualization examples, featuring an intermediate representation of visualizations that bridges primitive SVG and visualization programs. Users may express adaptation goals to a conversational agent and control the visual appearance through widgets generated on demand. With an interactive interface, users can modify both data mappings and visual design elements while maintaining the original visualization's aesthetic quality. To evaluate DataWink, we conduct a user study (N=12) with replication and free-form exploration tasks. As a result, DataWink is recognized for its learnability and effectiveness in personalized authoring tasks. Our results demonstrate the potential of example-driven approaches for democratizing visualization creation.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology","email":"liwenhan.xie@connect.ust.hk","name":"Liwenhan Xie"},{"affiliation":"The Hong Kong University of Science and Technology","email":"ylindg@connect.ust.hk","name":"Yanna Lin"},{"affiliation":"Nanyang Technological University","email":"can.liu.1996@gmail.com","name":"Can Liu"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"},{"affiliation":"Newcastle University","email":"xinhuan.shu@gmail.com","name":"Xinhuan Shu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"30cf9291-b159-4a33-95bd-47f194b893a0","image_caption":"","keywords":["Visualization template","Lazy data binding","Visualization by example","Dynamic abstractions"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.17734","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1029","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full34","session_room":"Hall M2","session_room_id":"m2","session_title":"Vis & Language","session_uid":"30cf9291-b159-4a33-95bd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Vis & Language"],"time_stamp":"2025-11-06T10:15:00.000Z","title":"DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JQEgOnSg18o"},{"UID":"31b94088-e58d-4b8b-993f-c942e3dbe846","abstract":"The design of urban road networks significantly influences traffic conditions, underscoring the importance of informed traffic planning. Traffic planning experts rely on specialized platforms to simulate traffic systems, assessing the efficacy of the road network across various states of modifications. Nevertheless, a prevailing issue persists: many existing traffic planning platforms exhibit inefficiencies in flexibly interacting with the road network\u2019s structure and attributes and intuitively comparing multiple states during the iterative planning process. This paper introduces TraSculptor, an interactive planning decision-making system. To develop TraSculptor, we identify and address two challenges: interactive modification of road networks and intuitive comparison of multiple network states. For the first challenge, we establish flexible interactions to enable experts to easily and directly modify the road network on the map. For the second challenge, we design a comparison view with a history tree of multiple states and a road-state matrix to facilitate intuitive comparison of road network states. To evaluate TraSculptor, we provided a usage scenario where the Braess\u2019s paradox was showcased, invited experts to perform a case study on the Sioux Falls network, and collected expert feedback through interviews.","accessible_pdf":"No","authors":[{"email":null,"name":"Zikun Deng"},{"email":null,"name":"Yuanbang Liu"},{"email":null,"name":"Mingrui Zhu"},{"email":null,"name":"Da Xiang"},{"email":null,"name":"Haiyue Yu"},{"email":null,"name":"Zicheng Su"},{"email":null,"name":"Qinglong Lu"},{"email":null,"name":"Tobias Schreck"},{"email":null,"name":"Yi Cai"}],"award":"","doi":"10.1109/TVCG.2025.3532498","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"31b94088-e58d-4b8b-993f-c942e3dbe846","image_caption":"","keywords":["Roads","Planning","Data visualization","Iterative methods","Visual analytics","Decision making","Road traffic","Traffic control","History","Transportation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.09489","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0604.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full29","session_room":"Hall M1","session_room_id":"m1","session_title":"Transportation, Buildings, and Urban Vis","session_uid":"31b94088-e58d-4b8b-993f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Transportation, Buildings, and Urban Vis"],"time_stamp":"2025-11-06T14:00:00.000Z","title":"TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/hoMn73KnXCg"},{"UID":"32bd145e-06bf-411a-af2e-109192108a7e","abstract":"While many visualizations are build for domain users (e.g., biologists, machine learning developers), understanding how visualizations are used in the domain has long been a challenging task. Previous research has relied on either interviewing a limited number of domain users or reviewing relevant application papers in the visualization community, neither of which provides comprehensive insight into visualizations in the wild of a specific domain. This paper aims to fill this gap by examining the potential of using Large Language Models (LLM) to analyze visualization usage in domain literature. We use high-dimension (HD) data visualization in sing-cell transcriptomics as a test case, analyzing 1,203 papers that describe 2,056 HD visualizations with highly specialized domain terminologies (e.g., biomarkers, cell lineage). To facilitate this analysis, we introduce a multi-step, human-in-the-loop LLM workflow. Instead of relying solely on LLMs for end-to-end analysis, our workflow enhances analytical quality through 1) integrating image processing and traditional NLP methods to prepare well-structured inputs for three targeted LLM subtasks (i.e., translating domain terminology, summarizing analysis tasks, and performing categorization), and 2) establishing checkpoints for human involvement and validation throughout the process.\nThe analysis results was validated with expert interviews and a test set, revealing three often overlooked aspects in HD visualization: trajectories in HD spaces, inter-cluster relationships, and dimension clustering.\nThis research provides a stepping stone for future studies seeking to use LLMs to bridge the gap between visualization design and domain-specific usage.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"},{"affiliation":"Northeastern University","email":"xinyi.liu@utexas.edu","name":"Xinyi Liu"},{"affiliation":"Harvard Medical School","email":"nils@hms.harvard.edu","name":"Nils Gehlenborg"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"32bd145e-06bf-411a-af2e-109192108a7e","image_caption":"","keywords":["High dimensional visualization; LLM-supported literature review; Visualization in the wild"],"open_access_supplemental_link":"https://hdvis.github.io","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/qtsak_v2?view_only=","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1905","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full28","session_room":"Hall E1","session_room_id":"e1","session_title":"The VIS in GenAI","session_uid":"32bd145e-06bf-411a-af2e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The VIS in GenAI"],"time_stamp":"2025-11-05T14:00:00.000Z","title":"Can LLMs Bridge Domain and Visualization? A Case Study on High-Dimension Data Visualization in Single-Cell Transcriptomics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zLchh59XyM4"},{"UID":"32cc9f8f-5e23-4433-94fa-aa2688eb0aeb","abstract":"Detecting complex behavioral patterns in temporal data, such as moving object trajectories, often relies on precise formal specifications derived from vague domain concepts. However, such methods are sensitive to noise and minor fluctuations, leading to missed pattern occurrences. Conversely, machine learning (ML) approaches require abundant labeled examples, posing practical challenges. Our visual analytics approach enables domain experts to derive, test, and combine interval-based features to discriminate patterns and generate training data for ML algorithms. Visual aids enhance recognition and characterization of expected patterns and discovery of unexpected ones. Case studies demonstrate feasibility and effectiveness of the approach, which offers a novel framework for integrating human expertise and analytical reasoning with ML techniques, advancing data analytics.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Natalia Andrienko"},{"email":null,"name":"Gennady Andrienko"},{"email":null,"name":"Alexander Artikis"},{"email":null,"name":"Periklis Mantenoglou"},{"email":null,"name":"Salvatore Rinzivillo"}],"award":"","doi":"10.1109/MCG.2024.3379851","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"32cc9f8f-5e23-4433-94fa-aa2688eb0aeb","image_caption":"","keywords":["Trajectory","Data models","Pattern recognition","Time series analysis","Task analysis","Visual analytics","Training","Human in the loop","Object recognition"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3379851","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga2","session_room":"Hall E1","session_room_id":"e1","session_title":"Visual Analytics Methods, Tools, and Infrastructure","session_uid":"32cc9f8f-5e23-4433-94fa","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visual Analytics Methods, Tools, and Infrastructure"],"time_stamp":"2025-11-06T10:27:00.000Z","title":"Human-in-the-Loop: Visual Analytics for Building Models Recognizing Behavioral Patterns in Time Series","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JfetpDJuh5Q"},{"UID":"32ebace7-cf30-462d-b6a4-40a3e39c6226","abstract":"Fast loading and responsive interaction lead to more effective web-based visualizations. While run-time optimizations such as caching and data tiling improve interaction latency, these approaches leave initial load performance unoptimized. In this work, we investigate _publish-time optimizations_ that shift computational work ahead of user sessions to accelerate both loading and interaction. We organize the space of publish-time optimizations into categories of data preparation, pre-computation of data assets for optimization, and pre-rendering; and then reason about tradeoffs in terms of time-to-render (TTR), time-to-activation (TTA), and storage cost (SC). To assess their effectiveness, we implement publish-time optimizations for the open-source Mosaic architecture and evaluate their impact across varied visualizations and dataset sizes. On average, publish-time strategies reduced rendering latency by 83.7% and activation latency by 33.3%, demonstrating their value for improving the performance of web-based visualizations.","accessible_pdf":null,"authors":[{"affiliation":"University of Washington","email":"rpechuk@uw.edu","name":"Ron Pechuk"},{"affiliation":"University of Washington","email":"jheer@uw.edu","name":"Jeffrey Heer"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"32ebace7-cf30-462d-b6a4-40a3e39c6226","image_caption":"","keywords":["scalable visualization","web visualization","visualization optimization","visualization systems","user interfaces"],"open_access_supplemental_link":"https://github.com/uwdata/mosaic-publish","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1295","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"32ebace7-cf30-462d-b6a4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T08:57:00.000Z","title":"Publish-Time Optimizations for Web-Based Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"333107f7-56b6-4215-823a-2550e2a1c1e1","abstract":"Edge bundling reduces the visual complexity of drawings of dense graphs by clustering compatible edges. However, existing edge bundling methods often have high computational complexity, leading to scalability issues. This paper presents a new framework for fast edge bundling and faithfulness metrics for large and complex graphs using spectral sparsification, which sparsifies a graph G into a subgraph G' with O(nlogn) edges, preserving the spectrum of G. We first present a general framework, FEB (Fast Edge Bundling), utilizing spectral sparsification to improve the efficiency of existing bundling methods while maintaining a similar quality of bundling. We then present the FBQ (Fast Bundling Quality) framework for proxy bundle faithfulness metrics, to measure how FEB faithfully preserves the ground truth structure in the original edge bundling, with two variants, FBQ_JS (utilizing Jaccard Similarity) and  FBQ_SQ (utilizing sampling quality metrics). Extensive experiments using various real-world networks demonstrate the efficiency of the FEB framework, with 61% runtime improvement over the original edge bundling methods without sparsification, while maintaining a similar quality by FBQ quality metrics and visual comparison.","accessible_pdf":null,"authors":[{"affiliation":"University of Sydney","email":"xjia9238@uni.sydney.edu.au","name":"Xingjue Jiang"},{"affiliation":"University of Sydney","email":"seokhee.hong@sydney.edu.au","name":"Seok-Hee Hong"},{"affiliation":"University of Sydney","email":"amei2916@uni.sydney.edu.au","name":"Amyra Meidiana"},{"affiliation":"University of Sydney","email":"xzen6984@uni.sydney.edu.au","name":"Xianyuan Zeng"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"333107f7-56b6-4215-823a-2550e2a1c1e1","image_caption":"","keywords":["Edge bundling","Spectral sparsification","Faithfulness metrics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1132","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"333107f7-56b6-4215-823a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T10:42:00.000Z","title":"Fast and Faithful Edge Bundling for Large and Complex Networks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"33454738-a9a9-4944-a27f-262d1da9f35d","abstract":"Embodiment shapes how users verbally express intent when interacting with data through speech interfaces in immersive analytics. Despite growing interest in Natural Language Interactions (NLIs) for visual analytics in immersive environments, users\u2019 speech patterns and their use of embodiment cues in speech remain underexplored. Understanding their interplay is crucial to bridging the gap between users\u2019 intent and an immersive analytic system. To address this, we report the results from 15 participants in a user study conducted using the Wizard of Oz method. We performed axial coding on 1,280 speech acts derived from 734 utterances, examining how analysis tasks are carried out with embodiment and linguistic features. Next, we measured Speech Input Uncertainty for each analysis task using the semantic entropy of utterances, estimating how uncertain users\u2019 speech inputs appear to an analytic system. Through these analyses, we identified five speech input patterns, showing that users dynamically blend embodied and non-embodied speech acts depending on data analysis tasks, phases, and Embodiment Reliance driven by the counts and types of embodiment cues in each utterance. We then examined how these patterns align with user reflections on factors that challenge speech interaction during the study. Finally, we propose design implications aligned with the five patterns.","accessible_pdf":null,"authors":[{"affiliation":"University of Maryland","email":"hsong02@cs.umd.edu","name":"Hyemi Song"},{"affiliation":"University of Maryland","email":"mjohns28@terpmail.umd.edu","name":"Matthew Johnson"},{"affiliation":"Department of Defense","email":"visual.tycho@gmail.com","name":"Kirsten Whitley"},{"affiliation":"Department of Defense","email":"ericpkrokos@gmail.com","name":"Eric Krokos"},{"affiliation":"University of Maryland","email":"varshney@cs.umd.edu","name":"Amitabh Varshney"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"33454738-a9a9-4944-a27f-262d1da9f35d","image_caption":"","keywords":["Embodiment","Natural Language Interaction (NLI)","Immersive Analytics","Speech Patterns","Semantic Entropy","User Intent","Speech Acts"],"open_access_supplemental_link":"https://osf.io/sv4fn/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1747","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full18","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive & Ubiquitous Analytics","session_uid":"33454738-a9a9-4944-a27f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive & Ubiquitous Analytics"],"time_stamp":"2025-11-06T13:24:00.000Z","title":"Embodied Natural Language Interaction (NLI): Speech Input Patterns in Immersive Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/6hTTrGMsPK8"},{"UID":"33761c1b-3649-4d86-8e2f-41cdb361e6a6","abstract":"Affective visualization design is an emerging research direction focused on communicating and influencing emotion through visualization. However, as revealed by previous research, this area is highly interdisciplinary and involves theories and practices from diverse fields and disciplines, thus awaiting analysis from more fine-grained angles. To address this need, this work focuses on a pioneering and relatively mature sub-area, affective geovisualization design, to further the research in this direction and provide more domain-specific insights. Through an analysis of a curated corpus of affective geovisualization designs using the Person-Process-Place (PPP) model from geographic theory, we derived a design taxonomy that characterizes a variety of methods for eliciting and enhancing emotions through geographic visualization. We also identified four underlying high-level design paradigms of affective geovisualization design (e.g., computational, anthropomorphic) that guide distinct approaches to linking geographic information with human experience. By extending existing affective visualization design frameworks with geographic specificity, we provide additional design examples, domain-specific analyses, and insights to guide future research and practices in this underexplored yet highly innovative domain.","accessible_pdf":null,"authors":[{"affiliation":"Fudan University","email":"xingyulan96@gmail.com","name":"Xingyu Lan"},{"affiliation":"Shanghai Jiao Tong University","email":"flora20@sjtu.edu.cn","name":"Yutong Yang"},{"affiliation":"Fudan University","email":"wangyifanlea@gmail.com","name":"Yifan Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"33761c1b-3649-4d86-8e2f-41cdb361e6a6","image_caption":"","keywords":["Affective Visualization Design","Geographic Visualization","User Experience"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.11841","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1175","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full20","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Maps & Spatial Vis","session_uid":"33761c1b-3649-4d86-8e2f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Maps & Spatial Vis"],"time_stamp":"2025-11-05T08:30:00.000Z","title":"Mapping What I Feel: Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/nkeAR0PgfSw"},{"UID":"349234b8-5d8e-40a9-87db-36db8f367bb3","abstract":"Transparency is commonly utilized in visualizations to overlay color-coded histograms or sets, thereby facilitating the visual comparison of categorical data. However, these charts often suffer from significant overlap between objects, resulting in substantial color interactions. Existing color blending models struggle in these scenarios, frequently leading to ambiguous color mappings and the introduction of false colors. To address these challenges, we propose an automated approach for generating optimal color encodings to enhance the perception of translucent charts. Our method harnesses color nameability to maximize the association between composite colors and their respective class labels. We introduce a color-name aware (CNA) optimization framework that generates maximally coherent color assignments and transparency settings while ensuring perceptual discriminability for all segments in the visualization. We demonstrate the effectiveness of our technique through crowdsourced experiments with composite histograms, showing how our technique can significantly outperform both standard and visualization-specific color blending models. Furthermore, we illustrate how our approach can be generalized to other visualizations, including parallel coordinates and Venn diagrams. We provide an open-source implementation of our technique as a web-based tool.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Kecheng Lu"},{"email":null,"name":"Lihang Zhu"},{"email":null,"name":"Yunhai Wang"},{"email":null,"name":"Qiong Zeng"},{"email":null,"name":"Weitao Song"},{"email":null,"name":"Khairi Reda"}],"award":"","doi":"10.1109/TVCG.2024.3520219","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"349234b8-5d8e-40a9-87db-36db8f367bb3","image_caption":"","keywords":["Image color analysis","Optimization","Histograms","Data visualization","Visualization","Standards","Semantics","Rendering (computer graphics)","Data models","Color"],"open_access_supplemental_link":null,"open_access_supplemental_question":"By reducing false color artifacts and improving the whole-from-parts perception, the method improves user accuracy in tasks like identifying distributions, estimating categories, or reading overlaps.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.16242","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-09-0882]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full25","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception","session_uid":"349234b8-5d8e-40a9-87db","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception"],"time_stamp":"2025-11-06T13:36:00.000Z","title":"Color-Name Aware Optimization to Enhance the Perception of Transparent Overlapped Charts","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/D_j0t7Fy0w0"},{"UID":"34e579da-1d6a-4ba7-a6e1-4644a0d2c0b7","abstract":"Ensemble datasets are ever more prevalent in various scientific domains. In climate science, ensemble datasets are used to capture variability in projections under plausible future conditions including greenhouse and aerosol emissions. Each ensemble model run produces projections that are fundamentally similar yet meaningfully distinct. Understanding this variability among ensemble model runs and analyzing its magnitude and patterns is a vital task for climate scientists. In this paper, we present ClimateSOM, a visual analysis workflow that leverages a self organizing map (SOM) and Large Language Models (LLMs) to support interactive exploration and interpretation of climate ensemble datasets. The workflow abstracts climate ensemble model runs\u2014spatiotemporal time series\u2014into a distribution over a 2D space that captures the variability among the ensemble model runs using a SOM. LLMs are integrated to assist in sensemaking of this SOM-defined 2D space, the basis for the visual analysis tasks. In all, ClimateSOM enables users to explore the variability among ensemble model runs, identify patterns, compare and cluster the ensemble model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to an ensemble dataset of precipitation projections over California and the Northwestern United States. Furthermore, we conduct a short evaluation of our LLM integration, and conduct an expert review of the visual workflow and the insights from the case studies with six domain experts to evaluate our approach and its utility.","accessible_pdf":null,"authors":[{"affiliation":"University of California, Davis","email":"ykawakami@ucdavis.edu","name":"Yuya Kawakami"},{"affiliation":"Scripps Institution of Oceanography","email":"dcayan@ucsd.edu","name":"Daniel Cayan"},{"affiliation":"University of California at Davis","email":"dyuliu@ucdavis.edu","name":"Dongyu Liu"},{"affiliation":"University of California at Davis","email":"ma@cs.ucdavis.edu","name":"Kwan-Liu Ma"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"34e579da-1d6a-4ba7-a6e1-4644a0d2c0b7","image_caption":"","keywords":["Climate visual analytics","ensemble visualization","self-organizing maps"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1773","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full10","session_room":"Hall E2","session_room_id":"e2","session_title":"Dimensionality Reduction and Parameter Space Analysis","session_uid":"34e579da-1d6a-4ba7-a6e1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dimensionality Reduction and Parameter Space Analysis"],"time_stamp":"2025-11-05T14:57:00.000Z","title":"ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/lJXNA9vtCPA"},{"UID":"362f56c3-81b6-4af1-aeb4-022ee36079d5","abstract":"Bayesian Neural Networks (BNNs) offer a principled approach to modeling uncertainty in addition to providing predictions, making them particularly valuable for high-stake domains where uncertainty quantification is required. However, their adoption remains low, partly due to the difficulty in tuning and interpreting these models and their results. To address this limitation, we introduce BNNVis, a visual analytics tool designed to visualize BNNs and their results. BNNVis allows the user to understand the architecture and learned posterior weight distributions of their BNN at a glance and how these distributions differ from their prior. Additionally, the system helps them understand the distribution and magnitude of the accompanying uncertainties of the model's predictions. BNNVis provides insight into the final predictions and the model, helping practitioners tune and interpret BNNs and their results. We describe a usage scenario to demonstrate how the features of BNNVis come together to support a practitioner in using a BNN.","accessible_pdf":null,"authors":[{"affiliation":"National Renewable Energy Laboratory","email":"gabriel.appleby@gmail.com","name":"Gabriel Appleby"},{"affiliation":"National Renewable Energy Laboratory","email":"malik.hassanaly@nrel.gov","name":"Malik Hassanaly"},{"affiliation":"Idaho National Lab","email":"jennifer.rogers@inl.gov","name":"Jen Rogers"},{"affiliation":"NREL","email":"juliane.mueller@nrel.gov","name":"Juliane Mueller"},{"affiliation":"National Renewable Energy Laboratory","email":"kristi.potter@nrel.gov","name":"Kristi Potter"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"362f56c3-81b6-4af1-aeb4-022ee36079d5","image_caption":"","keywords":["Visual Analytics","Visualization","Machine Learning","Bayesian Neural-Networks","Uncertainty Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1239","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short12","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Viz for AI & AI for Viz","session_uid":"362f56c3-81b6-4af1-aeb4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Viz for AI & AI for Viz"],"time_stamp":"2025-11-05T10:24:00.000Z","title":"BNNVis: Towards Visual Analytics for Bayesian Neural Networks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"367cd3fe-b8ff-4bee-bd9f-ae8e51feeb9e","abstract":"Principal Component Analysis (PCA) is perhaps the most popular linear projection technique for dimensionality reduction. We consider PCA under the assumption that the high-dimensional data points are equipped with Gaussian uncertainty. Several approaches to such uncertainty-aware PCA have been developed recently in the visualization community. Since PCA is a discontinuous map, a small uncertainty in the data points can result in a huge uncertainty in the projected points. We show that the uncertainty of the data points also creates uncertainty in the eigenvectors of the covariance matrix that defines the PCA projection. We present a closed-form expression to quantify eigenvector uncertainty. Based on this, we propose a 3D glyph that supports the decision whether existing solutions for uncertainty-aware PCA are sufficient, or whether a more expensive sampling-based approach is required. We apply our approach to several test data sets.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Magdeburg","email":"lukas.friesecke@ovgu.de","name":"Lukas Friesecke"},{"affiliation":"University of Magdeburg","email":"christian.braune@ovgu.de","name":"Christian Braune"},{"affiliation":"University of Magdeburg","email":"roessl@isg.cs.uni-magdeburg.de","name":"Christian Roessl"},{"affiliation":"University of Magdeburg","email":"theisel@ovgu.de","name":"Holger Theisel"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"367cd3fe-b8ff-4bee-bd9f-ae8e51feeb9e","image_caption":"","keywords":["PCA","dimensionality reduction","uncertainty visualization"],"open_access_supplemental_link":"https://github.com/lfriesecke/uncertainty-aware-pca-revisited","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://vc.cs.ovgu.de/assets/publications/2025/Friesecke_2025_VIS.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1838","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full11","session_room":"Hall E2","session_room_id":"e2","session_title":"Embeddings & Metrics","session_uid":"367cd3fe-b8ff-4bee-bd9f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Embeddings & Metrics"],"time_stamp":"2025-11-05T13:12:00.000Z","title":"Uncertainty-Aware PCA Revisited","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gvbjZE1CgLQ"},{"UID":"39db4241-5a8e-4bf7-abaf-b638cfbfee08","abstract":"Structural analysis is essential in modern industrial design, where engineers iteratively refine geometry models based on stress simulations to achieve optimized designs. However, comparing stress distributions across multiple model variants remains challenging due to the complexity of stress fields, which are high-dimensional, unevenly distributed, and dependent on intricate geometric structures. Existing tools primarily support single-model analysis and lack dedicated functionalities for multi-model comparison. As a result, engineers must rely on manual, cognitively demanding visual inspections, making it difficult to systematically identify and interpret stress variations across design iterations. To address these limitations, we propose StressDiffVis, a visual analytics approach that facilitates stress field comparison across multiple structural models. StressDiffVis employs a volumetric representation to encode stress distributions while minimizing occlusion, enabling voxel-wise difference analysis for model comparison. To support localized analysis, we introduce model segmentation, grouping voxels with similar stress patterns across models. StressDiffVis integrates these techniques into an interactive interface with a tree view, organizing models by the iterative design process, and a comparison view, using a matrix layout for detailed comparisons. We demonstrate the effectiveness of StressDiffVis through two case studies illustrating its utility in comparative stress analysis. In addition, expert interviews confirm its potential to enhance engineering workflows.","accessible_pdf":null,"authors":[{"affiliation":"South China University of Technology","email":"jiabaoh20@outlook.com","name":"Jiabao Huang"},{"affiliation":"South China University of Technology","email":"zkdeng@scut.edu.cn","name":"Zikun Deng"},{"affiliation":"South China University of Technology","email":"sehanlinsong@mail.scut.edu.cn","name":"Hanlin Song"},{"affiliation":"South China University of Technology","email":"xiangch.05@outlook.com","name":"Xiang Chen"},{"affiliation":"Greater Bay Area National Center of Technology Innovation","email":"gaoshaowu@ncti-gba.cn","name":"Shaowu Gao"},{"affiliation":"South China University of Technology","email":"ycai@scut.edu.cn","name":"Yi Cai"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"39db4241-5a8e-4bf7-abaf-b638cfbfee08","image_caption":"","keywords":["Stress analysis","comparative visualization","volume visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1351","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full29","session_room":"Hall M1","session_room_id":"m1","session_title":"Transportation, Buildings, and Urban Vis","session_uid":"39db4241-5a8e-4bf7-abaf","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Transportation, Buildings, and Urban Vis"],"time_stamp":"2025-11-06T13:00:00.000Z","title":"StressDiffVis: Visual Analytics for Multi-Model Stress Comparison","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/hoMn73KnXCg"},{"UID":"3a293c7e-cb00-40ce-a2b2-f87aa7d1a07b","abstract":"We present FLINT (learning-based FLow estimation and temporal INTerpolation), a novel deep learning-based approach to estimate flow fields for 2D+time and 3D+time scientific ensemble data. FLINT can flexibly handle different types of scenarios with (1) a flow field being partially available for some members (e.g., omitted due to space constraints) or (2) no flow field being available at all (e.g., because it could not be acquired during an experiment). The design of our architecture allows to flexibly cater to both cases simply by adapting our modular loss functions, effectively treating the different scenarios as flow-supervised and flow-unsupervised problems, respectively (with respect to the presence or absence of ground-truth flow). To the best of our knowledge, FLINT is the first approach to perform flow estimation from scientific ensembles, generating a corresponding flow field for each discrete timestep, even in the absence of original flow information. Additionally, FLINT produces high-quality temporal interpolants between scalar fields. FLINT employs several neural blocks, each featuring several convolutional and deconvolutional layers. We demonstrate performance and accuracy for different usage scenarios with scientific ensembles from both simulations and experiments.","accessible_pdf":null,"authors":[{"email":null,"name":"Hamid Gadirov"},{"email":null,"name":"Jos B.T.M. Roerdink"},{"email":null,"name":"Steffen Frey"}],"award":"","doi":"10.1109/TVCG.2025.3561091","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"3a293c7e-cb00-40ce-a2b2-f87aa7d1a07b","image_caption":"","keywords":["Flow estimation","interpolation","deep learning","spatiotemporal data"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2409.19178","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3561091","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full37","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Rescheduled Papers","session_uid":"3a293c7e-cb00-40ce-a2b2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Rescheduled Papers"],"time_stamp":"2025-11-06T13:12:00.000Z","title":"FLINT: Learning-Based Flow Estimation and Temporal Interpolation for Scientific Ensemble Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"3b5e9042-feed-4d23-814b-7242a997aea1","abstract":"Large language models (LLMs) are increasingly used to support data analysis and visualization tasks, but remain prone to hallucinations. Recent work suggests that multi-agent systems (MAS) can mitigate hallucinations by enabling internal validation and cross-verification. However, learning effective MAS coordination strategies to mitigate hallucination remains challenging, particularly for newcomers, due to the wide range of coordination strategies and the lack of interactive, hands-on learning tools. To address this, we present The Agentopia Times, an educational game that teaches hallucination mitigation through active experimentation with MAS coordination strategies. The Agentopia Times simulates a newsroom where LLM agents collaborate to create data-driven narratives, with users tasked with adjusting communication protocols to manage hallucinated content. The game features a structured mapping between MAS coordination and familiar gameplay mechanics, providing immediate feedback on hallucination outcomes. Through use cases and preliminary user feedback, we demonstrate how The Agentopia Times enables users to explore and mitigate hallucination in MAS.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota, Twin Cities","email":"lu000661@umn.edu","name":"Yilin Lu"},{"affiliation":"University of Minnesota","email":"du000288@umn.edu","name":"Shurui Du"},{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3b5e9042-feed-4d23-814b-7242a997aea1","image_caption":"","keywords":["LLM","visualization generation","educational game","LLM hallucination","Multi-Agent"],"open_access_supplemental_link":"https://github.com/Visual-Intelligence-UMN/The-Agentopia-Times","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1222","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short12","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Viz for AI & AI for Viz","session_uid":"3b5e9042-feed-4d23-814b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Viz for AI & AI for Viz"],"time_stamp":"2025-11-05T10:42:00.000Z","title":"The Agentopia Times: Understanding and Mitigating Hallucinations in Multi-Agent LLM Systems via Data Journalism Gameplay","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"3ba85378-08f5-4533-b7d1-a8734f3bf319","abstract":"Dashboards serve as effective visualization tools for conveying complex information. However, there exists a knowledge gap regarding how dashboard designs impact user engagement, necessitating designers to rely on their design expertise. Saliency has been used to comprehend viewing behaviors and assess visualizations, yet existing saliency models are primarily designed for single-view visualizations. To address this, we conduct an eye-tracking study to quantify participants\u2019 viewing patterns on dashboards. We collect eye-movement data from 60 participants, each viewing 36 dashboards (16 representative dashboards shared across all and 20 unique to each participant), totaling 1,216 dashboards and 2,160 eye-movement data instances. Analysis of the data from 16 dashboards viewed by all participants provides insights into how dashboard objects and layout designs influence viewing behaviors. Our analysis confirms known viewing patterns and reveals new patterns related to dashboard layout designs. Using the eye-movement data and identified patterns, we develop a saliency model to predict viewing behaviors with dashboards. Compared to state-of-the-art models for single-view visualizations, our model demonstrates overall improvement in prediction performance for dashboards. Finally, we propose potential dashboard design guidelines, illustrate an application case, and discuss general scanning strategies along with limitations and future work.","accessible_pdf":"No","authors":[{"email":null,"name":"Manling Yang"},{"email":null,"name":"Yihan Hou"},{"email":null,"name":"Ling Li"},{"email":null,"name":"Remco Chang"},{"email":null,"name":"Wei Zeng"}],"award":"","doi":"10.1109/TVCG.2025.3532497","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"3ba85378-08f5-4533-b7d1-a8734f3bf319","image_caption":"","keywords":["Data visualization","Layout","Visualization","Data models","Solid modeling","Gaze tracking","Predictive models","Multitasking","Biological system modeling","Computational modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":"We have the eye-tracking dataset and the code for model, which have publiced on OSF","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG3532497]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full9","session_room":"Hall M2","session_room_id":"m2","session_title":"Dastardly Dashboards","session_uid":"3ba85378-08f5-4533-b7d1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dastardly Dashboards"],"time_stamp":"2025-11-05T08:54:00.000Z","title":"Dashboard Vision: Using Eye-Tracking to Understand and Predict Dashboard Viewing Behaviors","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/Ga7NSmKd54M"},{"UID":"3c025133-4562-4d9a-aff2-725dd723a2a2","abstract":"To understand multiplayer gameplay behavior in a bomb-laying game, this paper explores the visual representation of player movement and events using space-time cubes. Complementing a previous two-dimensional event visualization, the approach focuses on contextualizing the player trajectories on the board with important events. Leveraging extended reality technology, the three-dimensional space-time-cube representation of a game session can be placed like columns in the virtual space. Various techniques support the interactive exploration of the spatiotemporal data. We demonstrate insights that can be found through the analysis of AI agent play behavior.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Bamberg","email":"niko-sebastian.nannemann@stud.uni-bamberg.de","name":"Niko S. Nannemann"},{"affiliation":"University of Bamberg","email":"shivamworking@gmail.com","name":"Shivam Agarwal"},{"affiliation":"University of Bamberg","email":"fabian.beck@uni-bamberg.de","name":"Fabian Beck"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3c025133-4562-4d9a-aff2-725dd723a2a2","image_caption":"","keywords":["Game analytics","space-time cube","event visualization","immersive analytics","extended reality."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1213","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"3c025133-4562-4d9a-aff2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T15:21:00.000Z","title":"Visualizing Player Movement and Game Events through Space-Time Cubes in Extended Reality","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"3ca02fd1-d387-44fa-b576-413ea269d949","abstract":"Pathology images are crucial for cancer diagnosis and treatment. Although artificial intelligence has driven rapid advancements in pathology image analysis, the interpretation of ultra-large and multi-scale pathology images in clinical practice still heavily relies on physicians' experience. Clinicians need to repeatedly zoom in and out on individual slides to compare and assess pathological details \u2014 a process that is both time-consuming and prone to visual fatigue. The system first employs a diffusion model to perform tissue segmentation on pathology images, then calculates pathological tissue proportions and morphological metrics. Finally, through multi-scale dynamic comparison and multi-level visual evaluation, the system facilitates comprehensive and precise analysis of pathology images. The system provides clinicians with an intelligent and interactive tool for pathology image interpretation, enabling efficient visualization and precise analysis of pathological details, thereby reducing the effort require for detailed analysis.","accessible_pdf":null,"authors":[{"affiliation":"Hangzhou City University","email":"xucq@hzcu.edu.cn","name":"Chaoqing Xu"},{"affiliation":"Zhejiang University","email":"rickyyang0113@gmail.com","name":"Ruiqi Yang"},{"affiliation":"Zhejiang University","email":"leewh@zju.edu.cn","name":"Weihan Li"},{"affiliation":"School of Computer and Computing Science, Hangzhou City University","email":"xinyuanfu0421@gmail.com","name":"Xinyuan Fu"},{"affiliation":"School of Computer and Computing Science, Hangzhou City University","email":"flting.amanda@gmail.com","name":"Liting Fang"},{"affiliation":"Zhejiang University","email":"zunleifeng@zju.edu.cn","name":"Zunlei Feng"},{"affiliation":"Zhejiang University","email":"wcan@zju.edu.cn","name":"Can Wang"},{"affiliation":"Hangzhou City University","email":"songml@zju.edu.cn","name":"Mingli Song"},{"affiliation":"Zhejiang University","email":"chenvis@zju.edu.cn","name":"Wei Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3ca02fd1-d387-44fa-b576-413ea269d949","image_caption":"","keywords":["Pathology Image","Diffusion Model","Large-Scale","Visual Analytics","Interactive Exploration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1787","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full8","session_room":"Hall M1","session_room_id":"m1","session_title":"Biomedical Visualization","session_uid":"3ca02fd1-d387-44fa-b576","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Biomedical Visualization"],"time_stamp":"2025-11-06T15:33:00.000Z","title":"An Intelligent Interactive Visual Analytics System for Exploring Large and Multi-Scale Pathology Images","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rgq3gdIQGxk"},{"UID":"3d30c2f2-5e29-4f5f-a3bc-05c9fb3e4ea0","abstract":"Selecting the appropriate dimensionality reduction (DR) technique and determining its optimal hyperparameter settings that maximize the accuracy of the output projections typically involves extensive trial and error, often resulting in unnecessary computational overhead. To address this challenge, we propose a dataset-adaptive approach to DR optimization guided by structural complexity metrics. These metrics quantify the intrinsic complexity of a dataset, predicting whether higher-dimensional spaces are necessary to represent it accurately. Since complex datasets are often inaccurately represented in two-dimensional projections, leveraging these metrics enables us to predict the maximum achievable accuracy of DR techniques for a given dataset, eliminating redundant trials in optimizing DR. We introduce the design and theoretical foundations of these structural complexity metrics. We quantitatively verify that our metrics effectively approximate the ground truth complexity of datasets and confirm their suitability for guiding dataset-adaptive DR workflow. Finally, we empirically show that our dataset-adaptive workflow significantly enhances the efficiency of DR optimization without compromising accuracy.","accessible_pdf":null,"authors":[{"affiliation":"Seoul National University","email":"hj@hcil.snu.ac.kr","name":"Hyeon Jeon"},{"affiliation":"Seoul National University","email":"parkjeong02@snu.ac.kr","name":"Jeongin Park"},{"affiliation":"Seoul National University","email":"dtngus0111@gmail.com","name":"Soohyun Lee"},{"affiliation":"Yonsei University","email":"dhkim16@yonsei.ac.kr","name":"Dae Hyun Kim"},{"affiliation":"Inria-Saclay, Universit\u00e9 Paris-Saclay","email":"sbshin90@cs.umd.edu","name":"Sungbok Shin"},{"affiliation":"Seoul National University","email":"jseo@snu.ac.kr","name":"Jinwook Seo"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3d30c2f2-5e29-4f5f-a3bc-05c9fb3e4ea0","image_caption":"","keywords":["imensionality reduction","Structural complexity","High-dimensional data","Optimization","Dataset-adaptive workflow"],"open_access_supplemental_link":null,"open_access_supplemental_question":"We publicized the source code","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.11984","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1019","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full10","session_room":"Hall E2","session_room_id":"e2","session_title":"Dimensionality Reduction and Parameter Space Analysis","session_uid":"3d30c2f2-5e29-4f5f-a3bc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dimensionality Reduction and Parameter Space Analysis"],"time_stamp":"2025-11-05T14:45:00.000Z","title":"Dataset-Adaptive Dimensionality Reduction","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/lJXNA9vtCPA"},{"UID":"3d33e0d2-031b-4bd9-beae-dd6146407196","abstract":"Many real-world datasets \u2014 from an artist's body of work to a person's social media history \u2014 exhibit meaningful semantic changes over time that are difficult to capture with existing dimensionality reduction methods. To address this gap, we introduce a visualization technique that combines force-based projection and streaming clustering methods to build a spatial-temporal map of embeddings. Applying this technique, we create Chronotome, a tool for interactively exploring evolving themes in time-based data \u2014 in real time. We demonstrate the utility of our approach through use cases on text and image data, showing how it offers a new lens for understanding the aesthetics and semantics of temporal datasets.","accessible_pdf":"Accessible","authors":[{"affiliation":"Harvard University","email":"mattelim@gsd.harvard.edu","name":"Matte Lim"},{"affiliation":"Harvard University","email":"catherineyeh@g.harvard.edu","name":"Catherine Yeh"},{"affiliation":"Harvard","email":"wattenberg@gmail.com","name":"Martin Wattenberg"},{"affiliation":"Harvard","email":"viegas@google.com","name":"Fernanda Viegas"},{"affiliation":"Harvard University","email":"pmichala@gsd.harvard.edu","name":"Panagiotis Michalatos"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3d33e0d2-031b-4bd9-beae-dd6146407196","image_caption":"","keywords":["Dynamic topic modeling","embedding visualization","clustering methods","temporal data","spring force models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1131","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"3d33e0d2-031b-4bd9-beae","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:18:00.000Z","title":"Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"3d649242-7596-4300-84fe-793b6a69b69c","abstract":"Examining vision-language alignment in multimodal embeddings is crucial for various tasks, such as evaluating generative models and filtering pretraining data. The intricate nature of high-dimensional features necessitates dimensionality reduction (DR) methods to explore alignment of multimodal embeddings. However, existing DR methods fail to account for cross-modal alignment metrics, resulting in severe occlusion of points with divergent metrics clustered together, inaccurate contour maps from over-aggregation, and insufficient support for multi-scale exploration. To address these problems, this paper introduces DKMap, a novel DR visualization technique for interactive exploration of multimodal embeddings through Dynamic Kernel enhanced projection. First, rather than performing dimensionality reduction and contour estimation sequentially, we introduce a kernel regression supervised t-SNE that directly integrates post-projection contour mapping into the projection learning process, ensuring cross-modal alignment mapping accuracy. Second, to enable multi-scale exploration with dynamic zooming and progressively enhanced local detail, we integrate validation-constrained \u03b1 refinement of a generalized t-kernel with quad-tree-based multi-resolution technique, ensuring reliable kernel parameter tuning without overfitting. DKMap is implemented as a multi-platform visualization tool, featuring a web-based system for interactive exploration and a Python package for computational notebook analysis. Quantitative comparisons with baseline DR techniques demonstrate DKMap\u2019s superiority in accurately mapping cross-modal alignment metrics. We further demonstrate generalizability and scalability of DKMap with three usage scenarios, including visualizing million-scale text-to-image corpus, comparatively evaluating generative models, and exploring a billion-scale pretraining dataset.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yyebd@connect.ust.hk","name":"Yilin Ye"},{"affiliation":"South China University of Technology","email":"chenxiruan64@gmail.com","name":"Chenxi Ruan"},{"affiliation":"University of Oxford","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"South China University of Technology","email":"zkdeng@scut.edu.cn","name":"Zikun Deng"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"zengwei81@gmail.com","name":"Wei Zeng"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3d649242-7596-4300-84fe-793b6a69b69c","image_caption":"","keywords":["Kernel Regression","Vision-language Alignment","Multimodal Embeddings","Interactive Exploration"],"open_access_supplemental_link":"https://github.com/HKUST-CIVAL/DKMap","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1781","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full24","session_room":"Hall M2","session_room_id":"m2","session_title":"Ordering and Layout","session_uid":"3d649242-7596-4300-84fe","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Ordering and Layout"],"time_stamp":"2025-11-05T13:24:00.000Z","title":"DKMap: Interactive Exploration of Vision-Language Alignment in Multimodal Embeddings via Dynamic Kernel Enhanced Projection","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/1kLUiSSxIuM"},{"UID":"3d97e8c0-6d8b-4fe5-aa06-3ddf284b1a45","abstract":"Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.","accessible_pdf":null,"authors":[{"affiliation":"University of Notre Dame","email":"syao2@nd.edu","name":"Siyuan Yao"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3d97e8c0-6d8b-4fe5-aa06-3ddf284b1a45","image_caption":"","keywords":["Volume visualization","novel view synthesis","scene segmentation","segment tracking","deformable Gaussian splatting"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.12667","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1535","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full36","session_room":"Hall M1","session_room_id":"m1","session_title":"Volume","session_uid":"3d97e8c0-6d8b-4fe5-aa06","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Volume"],"time_stamp":"2025-11-05T13:12:00.000Z","title":"VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-M293vb-AIg"},{"UID":"3dd6a593-74c1-4ea0-b7bc-249654187e88","abstract":"Evaluating the accuracy of dimensionality reduction (DR) projections in preserving the structure of high-dimensional data is crucial for reliable visual analytics. Diverse evaluation metrics targeting different structural characteristics have thus been developed. However, evaluations of DR projections can become biased if highly correlated metrics\u2014those measuring similar structural characteristics\u2014are inadvertently selected, favoring DR techniques that emphasize those characteristics. To address this issue, we propose a novel workflow that reduces bias in the selection of evaluation metrics by clustering metrics based on their empirical correlations rather than on their intended design characteristics alone. Our workflow works by computing metric similarity using pairwise correlations, clustering metrics to minimize overlap, and selecting a representative metric from each cluster. Quantitative experiments demonstrate that our approach improves the stability of DR evaluation, which indicates that our workflow contributes to mitigating evaluation bias.","accessible_pdf":"Accessible","authors":[{"affiliation":"Seoul National University","email":"bjy7266@gmail.com","name":"Jiyeon Bae"},{"affiliation":"Seoul National University","email":"hj@hcil.snu.ac.kr","name":"Hyeon Jeon"},{"affiliation":"Seoul National University","email":"jseo@snu.ac.kr","name":"Jinwook Seo"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3dd6a593-74c1-4ea0-b7bc-249654187e88","image_caption":"","keywords":["Dimensionality reduction","Evaluation metrics","Correlation analysis","Benchmarking","Visual analytics"],"open_access_supplemental_link":"https://github.com/JiyeonBae/dr-metric-selection.git","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1049","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"3dd6a593-74c1-4ea0-b7bc","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T11:00:00.000Z","title":"Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"3ee80ea0-1bd7-49c1-82f5-fd9b9dd2a2ba","abstract":"This paper presents a typology of concepts and contexts to inform the design of medical data visualizations for clinician-patient communication. As the accessibility, ubiquity and complexity of personal medical data grows, so does the need for patients to understand and interpret it correctly. Based on interviews with 19 healthcare professionals and analysis of current patient communication materials, our typology captures i) the diverse types of medical data communicated; ii) the features clinicians need patients to recognise and understand about these data; iii) contextual information required; iv) motivation for communicating data to patients; and v) current common visualization techniques. In doing so it identifies and defines the key dimensions of data-communication scenarios relevant to data visualizers working with patient data. We apply the typology to classify real world clinical scenarios and through doing so, illustrate how the typology can inform data visualization when designing communication aids for clinicians.","accessible_pdf":null,"authors":[{"affiliation":"University of Edinburgh","email":"s0678276@ed.ac.uk","name":"Sarah Dunn"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3ee80ea0-1bd7-49c1-82f5-fd9b9dd2a2ba","image_caption":"","keywords":["Data visualization","health communication","typologies"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1352","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"3ee80ea0-1bd7-49c1-82f5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T09:33:00.000Z","title":"A Typology of Data Concepts and Contexts in Clinician-Patient Communication","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"3eefd0dc-4542-4d3b-8a9e-bd1530cdebab","abstract":"Dimensionality reduction techniques help analysts make sense of complex, high-dimensional spatial datasets, such as multiplexed tissue imaging, satellite imagery, and astronomical observations, by projecting data attributes into a two-dimensional space. However, these techniques typically abstract away crucial spatial, positional, and morphological contexts, complicating interpretation and limiting insights. To address these limitations, we present SEAL, an interactive visual analytics system designed to bridge the gap between abstract 2D embeddings and their rich spatial imaging context. SEAL introduces a novel hybrid-embedding visualization that preserves image and morphological information while integrating critical high-dimensional feature data. By adapting set visualization methods, SEAL allows analysts to identify, visualize, and compare selections\u2014defined manually or algorithmically\u2014in both the embedding and original spatial views, facilitating a deeper understanding of the spatial arrangement and morphological characteristics of entities of interest. To elucidate differences between selected sets of items, SEAL employs a scalable surrogate model to calculate feature importance scores, identifying the most influential features governing the position of objects within embeddings. These importance scores are visually summarized across selections, with mathematical set operations enabling detailed comparative analyses. We demonstrate SEAL's effectiveness and versatility through three case studies: colorectal cancer tissue analysis with a pharmacologist, melanoma investigation with a cell biologist, and exploration of sky survey data with an astronomer. These studies underscore the importance of integrating image context into embedding spaces when interpreting complex imaging datasets. Implemented as a standalone tool while also integrating seamlessly with computational notebooks, SEAL provides an interactive platform for spatially informed exploration of high-dimensional datasets, significantly enhancing interpretability and insight generation.","accessible_pdf":null,"authors":[{"affiliation":"Harvard University","email":"simonwarchol@g.harvard.edu","name":"Simon Warchol"},{"affiliation":"Harvard University","email":"gguo31@g.harvard.edu","name":"Grace Guo"},{"affiliation":"Harvard University","email":"jknittel@seas.harvard.edu","name":"Johannes Knittel"},{"affiliation":"Harvard Medical School","email":"dfreeman@g.harvard.edu","name":"Dan Freeman"},{"affiliation":"Harvard University","email":"usha_bhalla@g.harvard.edu","name":"Usha Bhalla"},{"affiliation":"Harvard Medical School","email":"jeremy_muhlich@hms.harvard.edu","name":"Jeremy Muhlich"},{"affiliation":"Harvard University","email":"peter_sorger@hms.harvard.edu","name":"Peter Sorger"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3eefd0dc-4542-4d3b-8a9e-bd1530cdebab","image_caption":"","keywords":["Dimensionality Reduction","Data Visualization","Spatial Image Analysis","Explainable AI","Feature Importance Modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://www.biorxiv.org/content/10.1101/2025.07.19.665696","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1936","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full10","session_room":"Hall E2","session_room_id":"e2","session_title":"Dimensionality Reduction and Parameter Space Analysis","session_uid":"3eefd0dc-4542-4d3b-8a9e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dimensionality Reduction and Parameter Space Analysis"],"time_stamp":"2025-11-05T15:09:00.000Z","title":"SEAL: Spatially-resolved Embedding Analysis with Linked Imaging Data","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/lJXNA9vtCPA"},{"UID":"3f0a5ad9-2440-47b4-a741-f6b92c58bf8d","abstract":"Eligibility criteria play a critical role in clinical trials by determining the target patient population, which significantly influences the outcomes of medical interventions. However, current approaches for designing eligibility criteria have limitations to support interactive exploration of the large space of eligibility criteria. They also ignore incorporating detailed characteristics from the original electronic health record (EHR) data for criteria refinement. To address these limitations, we proposed TrialCompass, a visual analytics system integrating a novel workflow, which can empower clinicians to iteratively explore the vast space of eligibility criteria through knowledge-driven and outcome-driven approaches. TrialCompass supports history-tracking to help clinicians trace the evolution of their adjustments and decisions when exploring various forms of data (i.e., eligibility criteria, outcome metrics, and detailed characteristics of original EHR data) through these two approaches. This feature can help clinicians comprehend the impact of eligibility criteria on outcome metrics and patient characteristics, which facilitates systematic refinement of eligibility criteria. Using a real-world dataset, we demonstrated the effectiveness of TrialCompass in providing insights into designing eligibility criteria for septic shock and sepsis-associated acute kidney injury. We also discussed the research prospects of applying visual analytics to clinical trials.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology","email":"rshengac@connect.ust.hk","name":"Rui Sheng"},{"affiliation":"Bosch Research North America & Bosch Center for Artificial Intelligence (BCAI)","email":"wangxbzb@gmail.com","name":"Xingbo Wang"},{"affiliation":"Zhejiang University","email":"wangjiachen@zju.edu.cn","name":"Jiachen Wang"},{"affiliation":"The Hong Kong University of Science and Technology","email":"suffvier@gmail.com","name":"Xiaofu Jin"},{"affiliation":"The Hong Kong University of Science and Technology","email":"szh@connect.ust.hk","name":"Zhonghua SHENG"},{"affiliation":"Weill Cornell Medical College","email":"zhx2005@med.cornell.edu","name":"Zhenxing Xu"},{"affiliation":"Weill Cornell Medical College","email":"sur4002@med.cornell.edu","name":"Suraj Rajendran"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"},{"affiliation":"Weill Cornell Medicine","email":"few2001@med.cornell.edu","name":"Fei Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3f0a5ad9-2440-47b4-a741-f6b92c58bf8d","image_caption":"","keywords":["Visual Analytics","Healthcare","Clinical Trials","Decision Making","Electronic Health Record (EHR)"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2507.12298","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1291","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full8","session_room":"Hall M1","session_room_id":"m1","session_title":"Biomedical Visualization","session_uid":"3f0a5ad9-2440-47b4-a741","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Biomedical Visualization"],"time_stamp":"2025-11-06T15:09:00.000Z","title":"TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rgq3gdIQGxk"},{"UID":"3f463b80-5efc-4150-8b6f-3f5fbb1aed72","abstract":"Exploring volumetric data is crucial for interpreting scientific datasets. However, selecting optimal viewpoints for effective navigation can be challenging, particularly for users without extensive domain expertise or familiarity with 3D navigation. In this paper, we propose a novel framework that leverages natural language interaction to enhance volumetric data exploration. Our approach encodes volumetric blocks to capture and differentiate underlying structures. It further incorporates a CLIP Score mechanism, which provides semantic information to the blocks to guide navigation. The navigation is empowered by a reinforcement learning framework that leverage these semantic cues to efficiently search for and identify desired viewpoints that align with the user\u2019s intent. The selected viewpoints are evaluated using CLIP Score to ensure that they best reflect the user queries. By automating viewpoint selection, our method improves the efficiency of volumetric data navigation and enhances the interpretability of complex scientific phenomena.","accessible_pdf":null,"authors":[{"affiliation":"Sun Yat-sen University","email":"zhaox269@mail2.sysu.edu.cn","name":"Xuan Zhao"},{"affiliation":"Sun Yat-sen University","email":"taoj23@mail.sysu.edu.cn","name":"Jun Tao"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"3f463b80-5efc-4150-8b6f-3f5fbb1aed72","image_caption":"","keywords":["Volume rendering","Viewpoint navigation","Natural language interaction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2050","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full34","session_room":"Hall M2","session_room_id":"m2","session_title":"Vis & Language","session_uid":"3f463b80-5efc-4150-8b6f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Vis & Language"],"time_stamp":"2025-11-06T11:15:00.000Z","title":"Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JQEgOnSg18o"},{"UID":"4009fec8-7d38-4e00-9e7d-f9493fb32941","abstract":"Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders.","accessible_pdf":null,"authors":[{"affiliation":"Hunan University","email":"changjianchen@hnu.edu.cn","name":"Changjian Chen"},{"affiliation":"Hunan University","email":"wangpengcheng@hnu.edu.cn","name":"Pengcheng Wang"},{"affiliation":"Hunan University","email":"feilv@hnu.edu.cn","name":"Fei Lyu"},{"affiliation":"Hunan University","email":"ztang@hnu.edu.cn","name":"Zhuo Tang"},{"affiliation":"Hunan University","email":"yanglixt@hnu.edu.cn","name":"Li Yang"},{"affiliation":"Hunan University","email":"wanglong8591@hnu.edu.cn","name":"Long Wang"},{"affiliation":"Hunan University","email":"caiyong911@hnu.edu.cn","name":"Yong Cai"},{"affiliation":"Hunan University","email":"feng_yu@hnu.edu.cn","name":"Feng Yu"},{"affiliation":"Hunan University","email":"lkl@hnu.edu.cn","name":"Kenli Li"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"4009fec8-7d38-4e00-9e7d-f9493fb32941","image_caption":"","keywords":["Hybrid rice breeding","dual projection","genomic prediction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.11848","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1645","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full3","session_room":"Hall E1","session_room_id":"e1","session_title":"Analysts, Assemble!","session_uid":"4009fec8-7d38-4e00-9e7d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analysts, Assemble!"],"time_stamp":"2025-11-06T13:36:00.000Z","title":"Interactive Hybrid Rice Breeding with Parametric Dual Projection","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/fkfdha6pCeI"},{"UID":"4161480d-1e23-4eed-be76-3953fac207b7","abstract":"Large-scale scientific simulations require significant resources to generate high-resolution time-varying data (TVD). While super-resolution is an efficient post-processing strategy to reduce costs, existing methods rely on a large amount of HR training data, limiting their applicability to diverse simulation scenarios. To address this constraint, we proposed CD-TVD, a novel framework that combines contrastive learning and an improved diffusion-based super-resolution model to achieve accurate 3D super-resolution from limited time-step high-resolution data. During pre-training on historical simulation data, the contrastive encoder and diffusion super-resolution modules learn degradation patterns and detailed features of high-resolution and low-resolution samples. In the training phase, the improved diffusion model with a local attention mechanism is fine-tuned using only one newly generated high-resolution timestep, leveraging the degradation knowledge learned by the encoder. This design minimizes the reliance on large-scale high-resolution datasets while maintaining the capability to recover fine-grained details. Experimental results on fluid and atmospheric simulation datasets confirm that CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a significant advancement in data augmentation for large-scale scientific simulations. The code is available at https://github.com/Xin-Gao-private/CD-TVD.","accessible_pdf":"Accessible","authors":[{"affiliation":"Tianjin University","email":"bichongke@tju.edu.cn","name":"Chongke Bi"},{"affiliation":"Tianjin University","email":"gao_xin_private@163.com","name":"Xin Gao"},{"affiliation":"Tianjin University","email":"closernh@163.com","name":"Jiakang Deng"},{"affiliation":"Computer Network Information Center, Chinese Academy of Sciences","email":"liguan@sccas.cn","name":"Guan Li"},{"affiliation":"The Hong Kong University of Science and Technology","email":"junhanvis@outlook.com","name":"Jun Han"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"4161480d-1e23-4eed-be76-3953fac207b7","image_caption":"","keywords":["Time-varying data visualization","deep learning","super-resolution","diffusion model"],"open_access_supplemental_link":null,"open_access_supplemental_question":"This work introduces a two-stage framework for 3D super-resolution of time-varying scientific data, which integrates contrastive representation learning with a diffusion-based reconstruction module. Notably, it achieves accurate reconstruction using only a single high-resolution timestep in the fine-tuning phase, enabled by degradation-aware feature encoding during pretraining. This design significantly reduces reliance on high-resolution data while maintaining generalizability across diverse physical simulation scenarios.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.08173","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1782","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full14","session_room":"Hall M1","session_room_id":"m1","session_title":"Flow and Topology","session_uid":"4161480d-1e23-4eed-be76","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Flow and Topology"],"time_stamp":"2025-11-05T14:57:00.000Z","title":"CD-TVD:Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ar3ziIYBxhc"},{"UID":"41c4a87c-ba18-4af0-9aa1-b97ddd58a669","abstract":"An \u2018invariant descriptor\u2019 captures meaningful structural features of networks, useful where traditional visualizations, like node-link views, face challenges like the \u2019hairball phenomenon\u2019 (inscrutable overlap of points and lines). Designing invariant descriptors involves balancing abstraction and information retention, as richer data summaries demand more storage and computational resources. Building on prior work, chiefly the BMatrix\u2014a matrix descriptor visualized as the invariant \u2019network portrait\u2019 heatmap\u2014we introduce BFS-Census, a new algorithm computing our Census data structures: Census-Node, Census-Edge, and Census-Stub. Our experiments show Census-Stub, which focuses on \u2019stubs\u2019 (half-edges), has orders of magnitude greater discerning power (ability to tell non-isomorphic graphs apart) than any other descriptor in this study, without a difficult trade-off: the substantial increase in resolution doesn't come at a commensurate cost in storage space or computation power. We also present new visualizations\u2014our Hop-Census polylines and Census-Census trajectories\u2014and evaluate them using real-world graphs, including a sensitivity analysis that shows graph topology change maps to visual Census change.","accessible_pdf":"No","authors":[{"email":null,"name":"Matt Oddo"},{"email":null,"name":"Stephen Kobourov"},{"email":null,"name":"Tamara Munzner"}],"award":"","doi":"10.1109/TVCG.2024.3513275","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"41c4a87c-ba18-4af0-9aa1-b97ddd58a669","image_caption":"","keywords":["Visualization","Data structures","Layout","Encoding","Topology","Vectors","Sensitivity analysis","Network topology","Trajectory","Pipelines"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Very extensive supplemental material: in addition to source code and benchmark dataset of 81 networks, we have scripts to produce every figure in paper, and thefigures themselves. These include high-resolution images grouped by plot type and by network: full-page collages for each plot type across all 81 networks, and conversely full-page views for each of the featured network showing all plot types for it.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.04582","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-05-0342.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full17","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Graphs and Networks","session_uid":"41c4a87c-ba18-4af0-9aa1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Graphs and Networks"],"time_stamp":"2025-11-05T15:33:00.000Z","title":"The Census-Stub Graph Invariant Descriptor","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zH5jvaUZHGk"},{"UID":"424d3365-a7d7-4908-b3da-516f6bb9c7a7","abstract":"We built a spatial hybrid system that combines a personal computer (PC) and virtual reality (VR) for visual sensemaking, addressing limitations in both environments. Although VR offers immense potential for interactive data visualization (e.g., large display space and spatial navigation), it can also present challenges such as imprecise interactions and user fatigue. At the same time, a PC offers precise and familiar interactions but has limited display space and interaction modality. Therefore, we iteratively designed a spatial hybrid system (PC+VR) to complement these two environments by enabling seamless switching between PC and VR environments. To evaluate the system's effectiveness and user experience, we compared it to using a single computing environment (i.e., PC-only and VR-only). Our study results (N=18) showed that spatial PC+VR could combine the benefits of both devices to outperform user preference for VR-only without a negative impact on performance from device switching overhead. Finally, we discussed future design implications.","accessible_pdf":"No","authors":[{"email":null,"name":"Wai Tong"},{"email":null,"name":"Haobo Li"},{"email":null,"name":"Meng Xia"},{"email":null,"name":"Kam Kwai Wong"},{"email":null,"name":"Ting-Chuen Pong"},{"email":null,"name":"Huamin Qu"},{"email":null,"name":"Yalong Yang"}],"award":"","doi":"10.1109/TVCG.2025.3538771","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"424d3365-a7d7-4908-b3da-516f6bb9c7a7","image_caption":"","keywords":["Data visualization","Visualization","Three-dimensional displays","Switches","Navigation","Keyboards","Hands","User experience","Rendering (computer graphics)","Visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2502.00853","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0533]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full18","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive & Ubiquitous Analytics","session_uid":"424d3365-a7d7-4908-b3da","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive & Ubiquitous Analytics"],"time_stamp":"2025-11-06T13:48:00.000Z","title":"Exploring Spatial Hybrid User Interface for Visual Sensemaking","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/6hTTrGMsPK8"},{"UID":"426f36f0-a9c4-4a2a-9452-9b771ab2c535","abstract":"PhDs\u2014Portugal has Doctors is an interactive installation presenting Portuguese doctoral theses from 1970 to 2022, tracking their historical evolution and distribution across universities and research sectors. This work resulted in an installation that served a dual purpose: to raise awareness and value the work of national doctorates and to reduce the communication gap on this topic, encouraging a public engagement with the subject, fostering discussions beyond the data, and prompting reflection on how this lesser-known reality has impacted Portugal with its significantly growing presence. Drawing from recent research on aesthetics and its impact on data perception, we integrated these insights to make the visualization approach accessible to the general public, emphasizing a visually minimalist narrative. The story initially prompts viewers to reflect on the temporal evolution of the academic landscape over the last decades. The visualization then encourages viewers to actively engage with the data, facilitating a more in-depth exploration. The installation garnered positive feedback, provoking amazement and surprise, and revealed an unknown reality, even within the scientific community, further justifying the need for this type of dissemination works.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Pedro Silva"},{"email":null,"name":"Pedro Martins"},{"email":null,"name":"Penousal Machado"}],"award":"","doi":"10.1109/MCG.2024.3405656","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"426f36f0-a9c4-4a2a-9452-9b771ab2c535","image_caption":"","keywords":["Data visualization","Medical services","Art","Visualization","Cultural differences","Technological innovation","Industries"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://cdv.dei.uc.pt/projects/phds","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3405656","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga1","session_room":"Room 0.96 + 0.97","session_room_id":"0_96_0_97","session_title":"Reflections and Looking Forward","session_uid":"426f36f0-a9c4-4a2a-9452","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Reflections and Looking Forward"],"time_stamp":"2025-11-07T08:54:00.000Z","title":"PhDs\u2014Portugal has Doctors: A Visualization of Academia Achievements in Portugal From 1970 to 2022","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/29PgqT6NWUQ"},{"UID":"436e7b45-c89c-41ee-a1a7-7eb5e807dc28","abstract":"We investigate the concept of Augmented Dynamic Data Physicalization, the combination of shape-changing physical data representations with high-resolution virtual content. Tangible data sculptures, for example using mid-air shape-changing interfaces, are aesthetically appealing and persistent, but also technically and spatially limited. Blending them with Augmented Reality overlays such as scales, labels, or other contextual information opens up new possibilities. We explore the potential of this promising combination and propose a set of essential visualization components and interaction principles. They facilitate sophisticated hybrid data visualizations, for example Overview & Detail techniques or 3D view aggregations. We discuss three implemented applications that demonstrate how our approach can be used for personal information hubs, interactive exhibitions, and immersive data analytics. Based on these use cases, we conducted hands-on sessions with external experts, resulting in valuable feedback and insights. They highlight the potential of combining dynamic physicalizations with dynamic AR overlays to create rich and engaging data experiences.","accessible_pdf":"No","authors":[{"email":null,"name":"Severin Engert"},{"email":null,"name":"Andreas Peetz"},{"email":null,"name":"Konstantin Klamka"},{"email":null,"name":"Pierre Surer"},{"email":null,"name":"Tobias Isenberg"},{"email":null,"name":"Raimund Dachselt"}],"award":"","doi":"10.1109/TVCG.2025.3547432","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"436e7b45-c89c-41ee-a1a7-7eb5e807dc28","image_caption":"","keywords":["Data visualization","Visualization","Augmented reality","Data analysis","Electronic mail","Training","Shape","Resists","Museums","Meteorology"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://imld.de/addp","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023-09-0571.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full18","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive & Ubiquitous Analytics","session_uid":"436e7b45-c89c-41ee-a1a7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive & Ubiquitous Analytics"],"time_stamp":"2025-11-06T13:36:00.000Z","title":"Augmented Dynamic Data Physicalization: Blending Shape-changing Data Sculptures with Virtual Content for Interactive Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/6hTTrGMsPK8"},{"UID":"440d12ea-914c-49b0-a380-163dab6a3e41","abstract":"Understanding user interactions in digital systems is essential in analyzing user behaviors and improving system usability. However, a collection of interaction sequences is often large and unstructured, making it challenging to uncover interaction patterns. To address this challenge, we introduce a visual analytics approach that integrates hierarchical clustering and process mining techniques to support analysts in exploring unstructured, large interaction sequence data. Our system employs a tailored dynamic time warping-based similarity measure to enable comparison of interaction sequences. Based on the sequence similarities, we provide stepwise, interactive navigation of clustering results with contextual visual cues for refinement and validation. We further apply process mining to characterize derived clusters. Through these hierarchical clustering and process mining steps, analysts can progressively uncover meaningful interaction patterns while utilizing visual guidance and incorporating domain expertise. We demonstrate our system's effectiveness and applicability through two case studies involving system designers, developers, and domain experts.","accessible_pdf":"Accessible","authors":[{"affiliation":"Link\u00f6ping University","email":"peilin.yu@liu.se","name":"Peilin Yu"},{"affiliation":"Link\u00f6ping University","email":"aida.vitoria@liu.se","name":"Aida Nordman"},{"affiliation":"Link\u00f6ping University","email":"tfujiwara@ucdavis.edu","name":"Takanori Fujiwara"},{"affiliation":"Link\u00f6ping University","email":"marta.koc-januchta@liu.se","name":"Marta Koc-Januchta"},{"affiliation":"Link\u00f6ping University","email":"konrad.schonborn@liu.se","name":"Konrad Sch\u00f6nborn"},{"affiliation":"Link\u00f6ping University","email":"lonni.besancon@gmail.com","name":"Lonni Besan\u00e7on"},{"affiliation":"Link\u00f6ping University","email":"katerina.vrotsou@liu.se","name":"Katerina Vrotsou"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"440d12ea-914c-49b0-a380-163dab6a3e41","image_caption":"","keywords":["Pattern discovery in interaction logs","visual analytics","dynamic time warping","hierarchical clustering","process mining"],"open_access_supplemental_link":"https://osf.io/6az29/","open_access_supplemental_question":"Our work provides sufficient supplemental materials.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://doi.org/10.31219/osf.io/n5dxe_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1170","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full19","session_room":"Hall E1","session_room_id":"e1","session_title":"Interaction, Provenance, and Collaboration","session_uid":"440d12ea-914c-49b0-a380","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Interaction, Provenance, and Collaboration"],"time_stamp":"2025-11-06T14:45:00.000Z","title":"Visual Extraction of Interaction Patterns Guided by Hierarchical Clustering and Process Mining","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/253qu_2o0K0"},{"UID":"470f58d3-7a7a-47fd-baf3-82d155ac725b","abstract":"Simulacra Naturae is a data-driven media installation that explores collective care through the entanglement of biological computation, material ecologies, and generative systems. The work translates pre-recorded neural activity from brain organoids, lab-grown three-dimensional clusters of neurons, into a multi-sensory environment composed of generative visuals, spatial audio, living plants, and fabricated clay artifacts. These biosignals, streamed through a real-time system, modulate emergent agent behaviors inspired by natural systems such as termite colonies and slime molds. Rather than using biosignals as direct control inputs, Simulacra Naturae treats organoid activity as a co-creative force, allowing neural rhythms to guide the growth, form, and atmosphere of a generative ecosystem. The installation features computationally fabricated clay prints embedded with solenoids, adding physical sound resonances to the generative surround composition. The spatial environment, filled with live tropical plants and a floor-level projection layer featuring real-time generative AI visuals, invites participants into a sensory field shaped by nonhuman cognition. By grounding abstract data in living materials and embodied experience, Simulacra Naturae reimagines visualization as a practice of care, one that decentralizes human agency and opens new spaces for ethics, empathy, and ecological attunement within hybrid computational systems.","accessible_pdf":null,"authors":[{"affiliation":"University of California, Santa Barbara","email":"nefeli@ucsb.edu","name":"Nefeli Manoudaki"},{"affiliation":"University of California Santa Barbara","email":"merttoka@ucsb.edu","name":"Mert Toka"},{"affiliation":"University of California, Santa Barbara","email":"iason@ucsb.edu","name":"Iason Paterakis"},{"affiliation":"University of California Santa Barbara","email":"diarmid@ucsb.edu","name":"Diarmid Flatley"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"470f58d3-7a7a-47fd-baf3-82d155ac725b","image_caption":"","keywords":["Artificial life","symbiosis","brain organoids","collective intelligence","generative ecosystem."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1116","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap2","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Papers","session_uid":"470f58d3-7a7a-47fd-baf3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Papers"],"time_stamp":"2025-11-06T09:06:00.000Z","title":"Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VhkjwheU9Bc"},{"UID":"47c46990-8edb-4014-be27-c1c07f128d60","abstract":"Visual analytics (VA) is typically applied to complex data, thus requiring complex tools.\nWhile visual analytics empowers analysts in data analysis, analysts may get lost in the complexity occasionally. This highlights the need for intelligent assistance mechanisms.\nHowever, even the latest LLM-assisted VA systems only provide help when explicitly requested by the user, making them insufficiently intelligent to offer suggestions when analysts need them the most. We propose a ProactiveVA framework in which LLM-powered UI agent monitors user interactions and delivers context-aware assistance proactively. To design effective proactive assistance, we first conducted a formative study analyzing help-seeking behaviors in user interaction logs, identifying when users need proactive help, what assistance they require, and how the agent should intervene. Based on this analysis, we distilled key design requirements in terms of intent recognition, solution generation, interpretability and controllability. Guided by these requirements, we develop a three-stage UI agent pipeline including perception, reasoning, and acting.  The agent autonomously perceives users' needs from VA interaction logs, providing tailored suggestions and intuitive guidance through interactive exploration of the system. We implemented the framework in two representative types of VA systems, demonstrating its generalizability, and evaluated the effectiveness through an algorithm evaluation, case and expert study and a user study. We also discuss current design trade-offs of proactive VA and areas for further exploration.","accessible_pdf":null,"authors":[{"affiliation":"Fudan Univerisity","email":"yuhengzhao_cn@163.com","name":"Yuheng Zhao"},{"affiliation":"Fudan University","email":"3504936154@qq.com","name":"Xueli Shu"},{"affiliation":"Beijing Institute of Technology","email":"flwfdd@gmail.com","name":"Liwen Fan"},{"affiliation":"Fudan University","email":"lgao.lynne@gmail.com","name":"Lin Gao"},{"affiliation":"University of Oxford","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"Fudan University","email":"simingchen3@gmail.com","name":"Siming Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"47c46990-8edb-4014-be27-c1c07f128d60","image_caption":"","keywords":["Visual analytics","mixed-initiative","large language model","interface agent","proactive agent","human-AI collaboration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://www.arxiv.org/abs/2507.18165","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1045","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full1","session_room":"Hall E1","session_room_id":"e1","session_title":"Agentic Visualization and Intelligent Systems","session_uid":"47c46990-8edb-4014-be27","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Agentic Visualization and Intelligent Systems"],"time_stamp":"2025-11-05T14:45:00.000Z","title":"ProactiveVA: Proactive Visual Analytics with LLM-based UI Agent","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/2z0_8lHbM04"},{"UID":"494d7752-3810-46fa-906c-610bb1c013ef","abstract":"As AI systems become increasingly integrated into high-stakes domains, enabling users to accurately interpret model behavior is critical. While AI explanations are readily available, users often struggle to reason effectively with these explanations, limiting their ability to validate or learn from AI decisions. To address this gap, we introduce Reverse Mapping, a novel approach that enhances visual explanations by incorporating user-derived insights back into the explanation workflow. Our system extracts structured insights from free-form user interpretations using a large language model and maps them back onto visual explanations through interactive annotations and coordinated multi-view visualizations. Inspired by the verification loop in the visualization knowledge generation model, this design aims to foster more deliberate, reflective interaction with AI explanations. We demonstrate our approach in a prototype system with two use cases and qualitative user feedback.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"nutha010@umn.edu","name":"Aniket Nuthalapati"},{"affiliation":"University of Minnesota Twin Cities","email":"hinds084@umn.edu","name":"Nicholas Hinds"},{"affiliation":"National University of Singapore","email":"brianlim@comp.nus.edu.sg","name":"Brian Lim"},{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"494d7752-3810-46fa-906c-610bb1c013ef","image_caption":"","keywords":["XAI Visualization","reverse mapping","insight verification","explainable AI"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1124","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"494d7752-3810-46fa-906c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T10:51:00.000Z","title":"Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"49b63c40-81c7-41ec-a38a-9d433769c1c6","abstract":"This paper presents Visualization Badges, graphical labels shown alongside visualizations to communicate provenance and design considerations to enhance understandability and transparency. Badges may, for example, highlight a major finding, disclose that an axis has been truncated, or warn of possible visual artifacts. Inspired by nutrition and energy labels on product packaging, visualization badges aim (i) to allow visualization authors to justify and disclose analysis and design decisions and (ii) to make readers aware of important information when viewing and interpreting visualizations. Collectively, visualization badges aim to foster trust in visualizations and prevent readers from drawing incorrect conclusions. Based on a series of co-design workshops, we define and evaluate the concept of visualization badges and formulate a conceptual framework for analysis, application, and further research. Our framework includes a catalog of 132 visualization badges, categorization schemes, design options for their visual representations, applied visualization examples, and guidelines for their use. We hope that visualization badges will help communicate data and collectively improve communication, visualization literacy, and the quality of visualization techniques. Our badges, workshops, and guidelines can be found online https://vis-badges.github.io.","accessible_pdf":null,"authors":[{"affiliation":"Inria","email":"valentin.edelsbrunner@inria.fr","name":"Valentin Edelsbrunner"},{"affiliation":"The University of Edinburgh","email":"jinrui.w@outlook.com","name":"Jinrui Wang"},{"affiliation":"University of Edinburgh","email":"alexis.pister@hotmail.com","name":"Alexis Pister"},{"affiliation":"School of Law (PeaceRep)","email":"tvancisi@ed.ac.uk","name":"Tomas Vancisin"},{"affiliation":"The University of Edinburgh","email":"sian.phillips@ed.ac.uk","name":"Sian Phillips"},{"affiliation":"University of Oxford","email":"min.chen@oerc.ox.ac.uk","name":"Min Chen"},{"affiliation":"Inria","email":"bbach@inf.ed.ac.uk","name":"Benjamin Bach"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"49b63c40-81c7-41ec-a38a-9d433769c1c6","image_caption":"","keywords":["Data Visualization","Communication","Transparency"],"open_access_supplemental_link":"https://vis-badges.github.io/#/about","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://hal.science/view/index/docid/5199752","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1844","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full19","session_room":"Hall E1","session_room_id":"e1","session_title":"Interaction, Provenance, and Collaboration","session_uid":"49b63c40-81c7-41ec-a38a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Interaction, Provenance, and Collaboration"],"time_stamp":"2025-11-06T14:57:00.000Z","title":"Visualization Badges: Communicating Design and Provenance through Graphical Labels Alongside Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/253qu_2o0K0"},{"UID":"4a794224-d35b-4998-91b4-ec7982dbd26c","abstract":"Fuzzy clusters, where ambiguous samples belong to multiple clusters, are common in real-world applications. Analyzing such ambiguous samples in large-scale datasets is crucial for practical applications, such as diagnosing machine learning models. A promising method to support such analysis is through hierarchical cluster-aware grid visualizations, which offer high space efficiency and clear cluster perception. However, existing cluster-aware grid layout methods cannot clarify ambiguity among fuzzy clusters, which limits their effectiveness in fuzzy cluster analysis. To tackle this issue, we introduce a hierarchical fuzzy-cluster-aware grid layout method that supports hierarchical exploration of large-scale datasets. Throughout the hierarchical exploration, it is crucial to facilitate fuzzy cluster analysis while maintaining visual continuity for users. To achieve this, we propose a two-step optimization strategy for enhancing cluster perception, clarifying ambiguity, and preserving stability during the exploration. The first step is to create cluster-aware partitions, where each partition corresponds to a cluster. This step focuses on enhancing cluster perception and maintaining the previous shapes and positions of clusters to preserve stability at the cluster level. The second step is to generate a grid layout for each partition. In addition to placing similar samples together, this step also places ambiguous samples near the boundaries to clarify ambiguity and reveal the root causes of their occurrences and maintains the relative positions of the samples in the same cluster to preserve stability at the sample level. Several quantitative experiments and a use case are conducted to demonstrate the effectiveness and usefulness of our method in analyzing large-scale datasets, especially in fuzzy cluster analysis.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yuxing Zhou"},{"email":null,"name":"Changjian Chen"},{"email":null,"name":"Zhiyang Shen"},{"email":null,"name":"Jiangning Zhu"},{"email":null,"name":"Jiashu Chen"},{"email":null,"name":"Weikai Yang"},{"email":null,"name":"Shixia Liu"}],"award":"","doi":"10.1109/TVCG.2025.3566558","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"4a794224-d35b-4998-91b4-ec7982dbd26c","image_caption":"","keywords":["Layout","Power system stability","Thermal stability","Optimization","Data visualization","Visualization","Training","Stability criteria","Shape","Interviews"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-09-0860]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full23","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Of Trees and Treemaps","session_uid":"4a794224-d35b-4998-91b4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Of Trees and Treemaps"],"time_stamp":"2025-11-05T10:27:00.000Z","title":"Hierarchical Fuzzy-Cluster-Aware Grid Layout for Large-Scale Data","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/yiiG1AZHJ1E"},{"UID":"4becbe1b-2bde-4b12-adfd-e45a36678884","abstract":"Text labels are widely used to convey auxiliary information in visualization and graphic design. The substantial variability in the categories and structures of labeled objects leads to diverse label layouts. Recent single-model learning-based solutions in label placement struggle to capture fine-grained differences between these layouts, which in turn limits their performance. In addition, although human designers often consult previous works to gain design insights, existing label layouts typically serve merely as training data, limiting the extent to which embedded design knowledge can be exploited. To address these challenges, we propose a mixture of cluster-guided experts (MoCE) solution for label placement. In this design, multiple experts jointly refine layout features, with each expert responsible for a specific cluster of layouts. A cluster-based gating function assigns input samples to experts based on representation clustering. We implement this idea through the Label Placement Cluster-guided Experts (LPCE) model, in which a MoCE layer integrates multiple feed-forward networks (FFNs), with each expert composed of a pair of FFNs. Furthermore, we introduce a retrieval augmentation strategy into LPCE, which retrieves and encodes reference layouts for each input sample to enrich its representations. Extensive experiments demonstrate that LPCE achieves superior performance in label placement, both quantitatively and qualitatively, surpassing a range of state-of-the-art baselines. Our algorithm is available at https://github.com/PingshunZhang/LPCE.","accessible_pdf":"Accessible","authors":[{"affiliation":"Southwest University","email":"z2211973606@email.swu.edu.cn","name":"Pingshun Zhang"},{"affiliation":"Southwest University","email":"enyuche@gmail.com","name":"Enyu Che"},{"affiliation":"COLLEGE OF COMPUTER AND INFORMATION SCIENCE, SOUTHWEST UNIVERSITY SCHOOL OF SOFTWAREC","email":"out1147205215@outlook.com","name":"Yinan Chen"},{"affiliation":"Southwest University","email":"bihuang@cs.stonybrook.edu","name":"Bingyao Huang"},{"affiliation":"Stony Brook University","email":"hling@cs.stonybrook.edu","name":"Haibin Ling"},{"affiliation":"Southwest University","email":"qujingwei@swu.edu.cn","name":"Jingwei Qu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"4becbe1b-2bde-4b12-adfd-e45a36678884","image_caption":"","keywords":["Label placement","Mixture of experts","Retrieval augmentation"],"open_access_supplemental_link":"https://jingweiqu.github.io/project/LPCE","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1431","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full24","session_room":"Hall M2","session_room_id":"m2","session_title":"Ordering and Layout","session_uid":"4becbe1b-2bde-4b12-adfd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Ordering and Layout"],"time_stamp":"2025-11-05T13:00:00.000Z","title":"Mixture of Cluster-guided Experts for Retrieval-Augmented Label Placement","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/1kLUiSSxIuM"},{"UID":"4cf2c4a7-98c7-4d49-a5c2-410ecde134cb","abstract":"A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of California Berkeley","email":"chase_stokes@berkeley.edu","name":"Chase Stokes"},{"affiliation":"Georgia Institute of Technology","email":"klin368@gatech.edu","name":"Kylie Lin"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"4cf2c4a7-98c7-4d49-a5c2-410ecde134cb","image_caption":"","keywords":["Information visualizations","affordances","methodology","conclusions","large-language models."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.17024","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1507","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full28","session_room":"Hall E1","session_room_id":"e1","session_title":"The VIS in GenAI","session_uid":"4cf2c4a7-98c7-4d49-a5c2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The VIS in GenAI"],"time_stamp":"2025-11-05T13:24:00.000Z","title":"Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zLchh59XyM4"},{"UID":"4e824cf3-2aef-487f-b8a9-b0262fb2132d","abstract":"Data-driven articles have emerged as a powerful method for transforming complex datasets into accessible narratives. \nExisting approaches, such as interactive visualizations and scrollytelling, enhance engagement and simplify complexity by guiding users through data stories.\n\\rvis{However, these designs often prioritize narrative structure over reader-driven exploration, which may limit opportunities for autonomous interaction.}\nTo address this limitation, existing literature explored features that dynamically link textual content to corresponding elements in interactive graphs. \nIn this study, we investigate the impact of such tools on the reading experience, specifically the hedonic and pragmatic dimensions, in addition to loyalty intentions.\nWe implemented Vizualink, a text-graph linking feature, and examined its impact through an online study with 360 participants.\nOur results indicate that Vizualink enhances the reading experience, which in turn positively impacts readers' intention to return for more content, recommend the article, and consider paying for access.\nA thematic analysis of participant comments highlighted enhanced interactivity and criticism regarding potential hindrances to user experience.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Neuch\u00e2tel","email":"abdessalam.ouaazki@unine.ch","name":"Abdessalam Ouaazki"},{"affiliation":"University of Neuch\u00e2tel","email":"adrian.holzer@unine.ch","name":"Adrian Holzer"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"4e824cf3-2aef-487f-b8a9-b0262fb2132d","image_caption":"","keywords":["Vizualink","Text-Graph Integration","Data-Driven Articles","Interactive Systems"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1217","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short11","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Multi-modal and Spatial","session_uid":"4e824cf3-2aef-487f-b8a9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Multi-modal and Spatial"],"time_stamp":"2025-11-06T08:48:00.000Z","title":"Vizualink: Evaluating Usability and Loyalty Impacts of Text-Graph Integration in Data-Driven Articles","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"4e91945c-334a-4953-87ed-1564920f05dc","abstract":"Ownership relationships of early printed books from the 15th century reveal complex patterns of distribution and possession, offering valuable insights for historical research. This paper presents OwnershipTracker, a visual analytics application developed to explore and trace these relationships using data from the Material Evidence in Incunabula (MEI) database. OwnershipTracker integrates bibliographic records, copy-specific data, and book provenance and ownership details, enabling users to uncover intricate ownership sequences over time. The application combines several visualization techniques, including network graphs to map connections between owners, timelines for temporal analysis, chord diagrams to quantify transfer patterns, and a distinctive, collaboratively designed spiderweb-like diagram highlighting converging and dispersing ownership transfers through specific owners. Developed iteratively with input from historical book researchers, the application underwent multiple refinements to align with domain research requirements. A summative evaluation with domain experts showcased the tool\u2019s ability to address the defined requirements and tasks. The final version of OwnershipTracker is deployed and accessible at: https://booktracker.nms.kcl.ac.uk/ownership.","accessible_pdf":null,"authors":[{"affiliation":"King's College London","email":"yiwen.xing@kcl.ac.uk","name":"Yiwen Xing"},{"affiliation":"King's College London","email":"jimeilai222@gmail.com","name":"Meilai Ji"},{"affiliation":"University of Oxford","email":"c.dondi@cerl.org","name":"Cristina Dondi"},{"affiliation":"King's College London","email":"alfie.abdulrahman@gmail.com","name":"Alfie Abdul-Rahman"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"4e91945c-334a-4953-87ed-1564920f05dc","image_caption":"","keywords":["Design study","visualization application","human-centered design"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://kclpure.kcl.ac.uk/portal/en/publications/ownershiptracker-a-visual-analytics-approach-to-uncovering-histor","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1558","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full3","session_room":"Hall E1","session_room_id":"e1","session_title":"Analysts, Assemble!","session_uid":"4e91945c-334a-4953-87ed","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analysts, Assemble!"],"time_stamp":"2025-11-06T13:24:00.000Z","title":"OwnershipTracker: A Visual Analytics Approach to Uncovering Historical Book Ownership Patterns","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/fkfdha6pCeI"},{"UID":"513ae265-0ca2-4c62-b8ee-70676f0b815d","abstract":"Decision-making is a central yet under-defined goal in visualization research. While existing task models address decision processes, they often neglect the conditions framing a decision. To better support decision-making tasks, we propose a characterization scheme that describes decision problems through key properties of the data, users, and task context. This scheme helps visualization researchers specify decision-support claims more precisely and informs the design of appropriate visual encodings and interactions. We demonstrate the utility of our approach by applying it to characterize decision tasks targeted by existing design studies, highlighting opportunities for future research in decision-centric visualization.","accessible_pdf":null,"authors":[{"affiliation":"University of Rostock","email":"lena.cibulski@uni-rostock.de","name":"Lena Cibulski"},{"affiliation":"University of Rostock","email":"stefan.bruckner@gmail.com","name":"Stefan Bruckner"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"513ae265-0ca2-4c62-b8ee-70676f0b815d","image_caption":"","keywords":["Decision-centric visualization","task characterization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1189","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"513ae265-0ca2-4c62-b8ee","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T08:57:00.000Z","title":"Towards Understanding Decision Problems as a Goal of Visualization Design","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"51452519-94bc-439f-9dc5-3db3fac8080a","abstract":"The transfer function (TF) design is crucial for enhancing the visualization quality and understanding of volume data in volume rendering. Recent research has proposed various multidimensional TFs to utilize diverse attributes extracted from volume data for controlling individual voxel rendering. Although multidimensional TFs enhance the ability to segregate data, manipulating various attributes for the rendering is cumbersome. In contrast, low-dimensional TFs are more beneficial as they are easier to manage, but separating volume data during rendering is problematic. This paper proposes a novel approach, a two-level transfer function, for rendering volume data by reducing TF dimensions. The proposed technique involves extracting multidimensional TF attributes from volume data and applying t-Stochastic Neighbor Embedding (t-SNE) to the TF attributes for dimensionality reduction. The two-level transfer function combines the classical 2D TF and t-SNE TF in the conventional direct volume rendering pipeline. The proposed approach is evaluated by comparing segments in t-SNE TF and rendering images using various volume datasets. The results of this study demonstrate that the proposed approach can effectively allow us to manipulate multidimensional attributes easily while maintaining high visualization quality in volume rendering.","accessible_pdf":"No","authors":[{"email":null,"name":"Sangbong Yoo"},{"email":null,"name":"Seokyeon Kim"},{"email":null,"name":"Yun Jang"}],"award":"","doi":"10.1109/TVCG.2024.3484471","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"51452519-94bc-439f-9dc5-3db3fac8080a","image_caption":"","keywords":["Rendering (computer graphics)","Transfer functions","Histograms","Image color analysis","Self-organizing feature maps","Dimensionality reduction","Data visualization","Visualization","Image segmentation","Feature extraction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-01-0045 ]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full36","session_room":"Hall M1","session_room_id":"m1","session_title":"Volume","session_uid":"51452519-94bc-439f-9dc5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Volume"],"time_stamp":"2025-11-05T13:48:00.000Z","title":"Two-Level Transfer Functions Using t-SNE for Data Segmentation in Direct Volume Rendering","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-M293vb-AIg"},{"UID":"544233de-86be-4bc4-ab7e-2e5de7d366b2","abstract":"Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues.","accessible_pdf":"Accessible","authors":[{"affiliation":"Monash University","email":"kim.marriott@monash.edu","name":"Kim Marriott"},{"affiliation":"Monash University","email":"matthew.butler@monash.edu","name":"Matthew Butler"},{"affiliation":"Monash University","email":"leona.holloway@monash.edu","name":"Leona Holloway"},{"affiliation":"None","email":"wjolley@bigpond.com","name":"William Jolley"},{"affiliation":"Yonsei University","email":"b.lee@yonsei.ac.kr","name":"Bongshin Lee"},{"affiliation":"Vision Australia","email":"bruce.maguire@visionaustralia.org","name":"Bruce Maguire"},{"affiliation":"University of North Carolina-Chapel Hill","email":"danielle.szafir@gmail.com","name":"Danielle Szafir"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"544233de-86be-4bc4-ab7e-2e5de7d366b2","image_caption":"","keywords":["Tactile graphic","visual perception","haptic perception","accessible data representation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1468","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full27","session_room":"Hall M2","session_room_id":"m2","session_title":"The Color and the Shape","session_uid":"544233de-86be-4bc4-ab7e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The Color and the Shape"],"time_stamp":"2025-11-06T08:30:00.000Z","title":"From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gHCENa3C9X4"},{"UID":"54daa89a-c0e5-4d1e-9891-4ce10711d2f9","abstract":"This work investigates the current research on in-situ visualisations for running: visualisations about data that are referred to during the running activity. We analyse 47 papers from 33 Human-Computer Interaction and Visualisation venues and identify six dimensions of a design space of in-situ running visualisations. Our analysis of this design space highlights an emerging trend: a shift from on-body, peripersonal visualisations (i.e., in the space within direct reach, such as visualisations on a smartwatch or a mobile phone display) towards extrapersonal displays (i.e., in the space beyond immediate reach, such as visualisations in immersive augmented reality displays) that integrate data in the runner\u2019s surrounding environment. We explore this opportunity by conducting a series of workshops with 10 active runners in total, eliciting design concepts for running visualisations and interactions beyond conventional 2D displays. We find that runners show a strong interest for visualisation designs that favour more context-aware, interactive, and unobtrusive experiences that seamlessly integrate with their run. These findings inform a set of design considerations for future immersive running visualisations and highlight directions for further research.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Queensland","email":"leon.li.ang@foxmail.com","name":"Ang Li"},{"affiliation":"University of Victoria","email":"cperin@uvic.ca","name":"Charles Perin"},{"affiliation":"University of Queensland","email":"g.demartini@uq.edu.au","name":"Gianluca Demartini"},{"affiliation":"University of Queensland","email":"viller@acm.org","name":"Stephen Viller"},{"affiliation":"The University of Queensland","email":"j.knibbe@uq.edu.au","name":"Jarrod Knibbe"},{"affiliation":"The University of Queensland","email":"m.cordeil@uq.edu.au","name":"Maxime Cordeil"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"54daa89a-c0e5-4d1e-9891-4ce10711d2f9","image_caption":"","keywords":["Running","Jogging; Survey","Taxonomy; Human-Subjects Qualitative Studies; Personal Visual Analytics; Mobile; Augmented/Mixed/Extended Reality","Immersive"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1017","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full18","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive & Ubiquitous Analytics","session_uid":"54daa89a-c0e5-4d1e-9891","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive & Ubiquitous Analytics"],"time_stamp":"2025-11-06T13:00:00.000Z","title":"Running with Data: a Survey of the Current Research and a Design Exploration of Future Immersive Visualisations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/6hTTrGMsPK8"},{"UID":"55b60ade-1288-4e41-a5db-4bbff4d517eb","abstract":"Visualization as a discipline often grapples with generalization by reasoning about how study results on the efficacy of a tool in one context might apply to another context. This work offers an account of the logic of generalization in visualization research and argues that it struggles in particular with applications of visualization as a decision aid. We use decision theory to define the dimensions on which decision problems can vary, and we present an analysis of heterogeneity in scenarios where visualization supports decision-making. Our findings identify utility as a focal and under-examined concept in visualization research on decision-making, demonstrating how the visualization community's logic of generalization might benefit from using decision theory as a lens for understanding context variation.","accessible_pdf":null,"authors":[{"affiliation":"University of Chicago","email":"kalea@uchicago.edu","name":"Alex Kale"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"55b60ade-1288-4e41-a5db-4bbff4d517eb","image_caption":"","keywords":["Decision theory","visualization","epistemology."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1198","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short10","session_room":"Hall E","session_room_id":"e1_e2","session_title":"VGTC Awards & Best Short Papers","session_uid":"55b60ade-1288-4e41-a5db","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VGTC Awards & Best Short Papers"],"time_stamp":"2025-11-04T11:10:00.000Z","title":"Toward a Logic of Generalization about Visualization as a Decision Aid","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VdSWiJuX_x8"},{"UID":"55c2bdfa-142e-44ed-989c-76c9b32b3d66","abstract":"Communicating climate change remains challenging, as climate reports, though rich in data and visualizations, often feel too abstract or technical for the public. Although personalization can enhance communication, most tools still lack the narrative and visualization tailoring needed to connect with individual experiences. We present CLAImate, an AI-enabled prototype that personalizes conversation narratives and localizes visualizations based on users' climate knowledge and geographic location. We evaluated CLAImate through internal verification of factual correctness, a formative study with experts, and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70% FACTSCORE. Visualization experts appreciated its clarity and personalization, and seven out of ten UK participants reported better understanding and local relevance of climate risks with CLAImate. We also discuss design challenges in personalization, accuracy, and scalability, and outline future directions for integrating visualizations in personalized conversational interfaces.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Massachusetts Amherst","email":"mrashik@cs.umass.edu","name":"Mashrur Rashik"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"jean-daniel.fekete@inria.fr","name":"Jean-Daniel Fekete"},{"affiliation":"University of Massachusetts Amherst","email":"nmahyar@cs.umass.edu","name":"Narges Mahyar"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"55c2bdfa-142e-44ed-989c-76c9b32b3d66","image_caption":"","keywords":["Other Application Areas; Communication/Presentation","Storytelling; General Public; Software Prototype."],"open_access_supplemental_link":"https://osf.io/9vgf2/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.11677","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1139","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"55c2bdfa-142e-44ed-989c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T10:42:00.000Z","title":"CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"565a0b31-3df7-4d49-859a-2a96dda32a65","abstract":"The analysis of 3D symmetric second-order tensor fields often relies on topological features such as degenerate tensor lines, neutral surfaces, and their generalization to mode surfaces, which reveal important structural insights into the data. However, uncertainty in such fields is typically visualized using derived scalar attributes or tensor glyph representations, which often fail to capture the global behavior. Recent advances have introduced uncertain topological features for tensor field ensembles by focusing on degenerate tensor locations. Yet, mode surfaces, including neutral surfaces and arbitrary mode surfaces are essential to a comprehensive understanding of tensor field topology. In this work, we present a generalization of uncertain degenerate tensor features to uncertain mode surfaces of arbitrary mode values, encompassing uncertain degenerate tensor lines as a special case. Our approach supports both surface and line geometries, forming a unified framework for analyzing uncertain mode-based topological features in tensor field ensembles. We demonstrate the effectiveness of our method on several real-world simulation datasets from engineering and materials science.","accessible_pdf":"Accessible","authors":[{"affiliation":"RWTH Aachen University","email":"gerrits@vis.rwth-aachen.de","name":"Tim Gerrits"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"565a0b31-3df7-4d49-859a-2a96dda32a65","image_caption":"","keywords":["Second-Order Tensors","Symmetric Tensors","Tensor Topology","Tensor Mode","Uncertainty"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2506.23406","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1108","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"565a0b31-3df7-4d49-859a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T10:24:00.000Z","title":"Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"594e8ec2-1e1d-4c9a-8893-5134132f5232","abstract":"The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA\u2019s effectiveness from both quantitative and qualitative perspectives.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Xiwei Xuan"},{"email":null,"name":"Xiaoqi Wang"},{"email":null,"name":"Wenbin He"},{"email":null,"name":"Jorge Piazentin Ono"},{"email":null,"name":"Liang Gou"},{"email":null,"name":"Kwan-Liu Ma"},{"email":null,"name":"Liu Ren"}],"award":"","doi":"10.1109/TVCG.2025.3535896","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"594e8ec2-1e1d-4c9a-8893-5134132f5232","image_caption":"","keywords":["Data integrity","Data models","Frequency modulation","Pipelines","Visual analytics","Measurement","Computational modeling","Labeling","Analytical models","Human in the loop"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0626]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full31","session_room":"Hall E1","session_room_id":"e1","session_title":"VA for AI","session_uid":"594e8ec2-1e1d-4c9a-8893","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VA for AI"],"time_stamp":"2025-11-05T09:30:00.000Z","title":"VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LajPfGQe32k"},{"UID":"5a4eadeb-75eb-48ac-8a15-c21a5b376113","abstract":"We investigate methods for placing labels in AR environments that have visually cluttered scenes. As the number of items increases in a scene within the user' FOV, it is challenging to effectively place labels based on existing label placement guidelines. To address this issue, we implemented three label placement techniques for in-view objects for AR applications. We specifically target a scenario, where various items of different types are scattered within the user's field of view, and multiple items of the same type are situated close together. We evaluate three placement techniques for three target tasks. Our study shows that using a label to spatially group the same types of items is beneficial for identifying, comparing, and summarizing data.","accessible_pdf":null,"authors":[{"affiliation":"Rochester Institute of Technology","email":"jhpigm@rit.edu","name":"Ji Hwan Park"},{"affiliation":"The University of Oklahoma","email":"bradenroper@ou.edu","name":"Braden Roper"},{"affiliation":"University of Oklahoma","email":"amirhossein.arezoumand@ou.edu","name":"Amirhossein Arezoumand"},{"affiliation":"University of Oklahoma","email":"tien.g.tran@ou.edu","name":"Tien Tran"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5a4eadeb-75eb-48ac-8a15-c21a5b376113","image_caption":"","keywords":["Label placement","view management systems","augmented reality","situated visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.00198","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1125","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"5a4eadeb-75eb-48ac-8a15","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T15:12:00.000Z","title":"Exploring AR Label Placements in Visually Cluttered Scenarios","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"5a5e8c22-ecab-4548-b9f8-3b84454ee079","abstract":"The visualization and analysis of street and pedestrian networks are important to various domain experts, including urban planners, climate researchers, and health experts. This has led to the development of new techniques for street and pedestrian network visualization, expanding possibilities for effective data presentation and interpretation. Despite their increasing adoption, there is no established design framework to guide the creation of these visualizations while addressing the diverse requirements of various domains. When exploring a feature of interest, domain experts often need to transform, integrate, and visualize a combination of thematic data (e.g., demographic, socioeconomic, pollution) and physical data (e.g., zip codes, street networks), often spanning multiple spatial and temporal scales. This not only complicates the process of visual data exploration and system implementation for developers but also creates significant entry barriers for experts who lack a background in programming. With this in mind, in this paper, we reviewed 45 studies utilizing street-overlaid visualizations to understand how they are applied in practice. Through qualitative coding of these visualizations, we analyzed three key aspects of street and pedestrian network visualization usage: their analytical purposes, the visualization approaches employed, and the data sources used in their creation. Building on this design space, we introduce StreetWeave, a declarative grammar for designing custom visualizations of multivariate spatial network data across multiple resolutions. We demonstrate how StreetWeave can be used to create various street-overlaid visualizations, enabling effective exploration and analysis of spatial data. StreetWeave is available at https://urbantk.org/streetweave.","accessible_pdf":null,"authors":[{"affiliation":"University of Illinois at Chicago","email":"ssraba2@uic.edu","name":"Sanjana Srabanti"},{"affiliation":"University of Illinois at Chicago","email":"g.elisabeta.marai@gmail.com","name":"G. Elisabeta Marai"},{"affiliation":"University of Illinois Chicago","email":"fabiom@uic.edu","name":"Fabio Miranda"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5a5e8c22-ecab-4548-b9f8-3b84454ee079","image_caption":"","keywords":["Urban visual analytics","design space","street-overlaid visualization","visualization grammar"],"open_access_supplemental_link":"http://urbantk.org/streetweave","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.07496","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1934","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full29","session_room":"Hall M1","session_room_id":"m1","session_title":"Transportation, Buildings, and Urban Vis","session_uid":"5a5e8c22-ecab-4548-b9f8","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Transportation, Buildings, and Urban Vis"],"time_stamp":"2025-11-06T13:12:00.000Z","title":"StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/hoMn73KnXCg"},{"UID":"5b91b14a-3bf6-41db-9687-a9c0406af811","abstract":"Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability.","accessible_pdf":"Accessible","authors":[{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"ychen081@connect.hkust-gz.edu.cn","name":"Yiyu Chen"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"ywu012@connect.hkust-gz.edu.cn","name":"Yifan Wu"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"sshen190@connect.hkust-gz.edu.cn","name":"Shuyu Shen"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yxie740@connect.hkust-gz.edu.cn","name":"Yupeng Xie"},{"affiliation":"The Hong Kong University of Science and Technology","email":"lshenaj@connect.ust.hk","name":"Leixian Shen"},{"affiliation":"The Hong Kong University of Science and Technology \uff08Guangzhou\uff09","email":"xionghui@hkust-gz.edu.cn","name":"Hui Xiong"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yuyuluo@hkust-gz.edu.cn","name":"Yuyu Luo"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5b91b14a-3bf6-41db-9687-a9c0406af811","image_caption":"","keywords":["Chart Annotation","Grammar Language-agnostic"],"open_access_supplemental_link":"https://github.com/HKUSTDial/ChartMark","open_access_supplemental_question":"Thoroughly documented source code","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1349","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short11","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Multi-modal and Spatial","session_uid":"5b91b14a-3bf6-41db-9687","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Multi-modal and Spatial"],"time_stamp":"2025-11-06T08:57:00.000Z","title":"ChartMark: A Structured Grammar for Chart Annotation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"5cec6278-7356-4658-8125-0dc6b65f8b08","abstract":"Unstructured meshes present challenges in scientific data analysis due to irregular distribution and complex connectivity. Computing and storing connectivity information is a major bottleneck for visualization algorithms, affecting both time and memory performance. Recent task-parallel data structures address this by precomputing connectivity information at runtime while the analysis algorithm executes, effectively hiding computation costs and improving performance. However, existing approaches are CPU-bound, forcing the data structure and analysis algorithm to compete for the same computational resources, limiting potential speedups. To overcome this limitation, we introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU systems. Specifically, we offload the computation of mesh connectivity information to GPU threads, enabling CPU threads to focus on executing the visualization algorithm. Following this paradigm, we propose GPU-Aided Localized data structurE (GALE), the first open-source CUDA-based data structure designed for heterogeneous task parallelism. Experiments on two 20-core CPUs and an NVIDIA V100 GPU show that GALE achieves up to 2.7\u00d7 speedup over state-of-the-art localized data structures while maintaining memory efficiency.","accessible_pdf":null,"authors":[{"affiliation":"The Ohio State University","email":"liu.12722@osu.edu","name":"Guoxi Liu"},{"affiliation":"Clemson University","email":"tlranda@clemson.edu","name":"Thomas Randall"},{"affiliation":"Clemson University","email":"rge@clemson.edu","name":"Rong Ge"},{"affiliation":"Clemson University","email":"fiurici@clemson.edu","name":"Federico Iuricich"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5cec6278-7356-4658-8125-0dc6b65f8b08","image_caption":"","keywords":["Data structure","unstructured mesh","topological data analysis","parallel computation","GPU algorithm"],"open_access_supplemental_link":"https://osf.io/zxm4w","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.15230","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1512","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full2","session_room":"Hall E2","session_room_id":"e2","session_title":"Algorithms and Workflows","session_uid":"5cec6278-7356-4658-8125","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Algorithms and Workflows"],"time_stamp":"2025-11-06T14:00:00.000Z","title":"GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/wUoqSN_s3tw"},{"UID":"5cf45946-05e6-4d9c-876e-abeacdf5b8e1","abstract":"We investigate the perceived visual complexity (VC) in data visualizations using objective image-based metrics. We collected VC scores through a large-scale crowdsourcing experiment involving 349 participants and 1,800 visualization images. We then examined how these scores align with 12 image-based metrics spanning pixel-based and statistic-information-theoretic (clutter), color, shape, and our two new object-based metrics (meaningful-color-count (MeC) and text-to-ink ratio (TiR)). Our results show that both low-level edges and high-level elements affect perceived VC in visualization images; the number of corners and distinct colors are robust metrics across visualizations. Second, feature congestion, a statistical information-theoretic metric capturing color and texture patterns, is the strongest predictor of perceived complexity in visualizations rich in the same continuous color/texture stimuli; edge density effectively explains VC in node-link diagrams. Additionally, we observe a bell-curve effect for texts: increasing TiR initially reduces complexity, reaching an optimal point, beyond which further text increases VC. Our quantification model is also interpretable - enabling metric-based explanations - grounded in the VisComplexity2K dataset, bridging computational metrics with human perceptual responses. The preregistration is available at osf.io/5xe8a. osf.io/bdet6 has the dataset and analysis code.","accessible_pdf":null,"authors":[{"affiliation":"The Ohio State University","email":"chu.752@osu.edu","name":"Mengdi Chu"},{"affiliation":"The Ohio State University","email":"qiu.573@osu.edu","name":"Zefeng Qiu"},{"affiliation":"The Ohio State University","email":"ling.253@osu.edu","name":"Meng Ling"},{"affiliation":"The Ohio State University","email":"jiang.2126@osu.edu","name":"Shuning Jiang"},{"affiliation":"University of Nottingham","email":"robert.laramee@nottingham.ac.uk","name":"Robert Laramee"},{"affiliation":"University of Stuttgart","email":"michael.sedlmair@visus.uni-stuttgart.de","name":"Michael Sedlmair"},{"affiliation":"The Ohio State University","email":"chen.8028@osu.edu","name":"Jian Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5cf45946-05e6-4d9c-876e-abeacdf5b8e1","image_caption":"","keywords":["Perceived visual complexity","image-based metrics","scene-like","text-ink-ratio","meaningful-color-count"],"open_access_supplemental_link":"https://osf.io/bdet6/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/ypez4","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1919","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full11","session_room":"Hall E2","session_room_id":"e2","session_title":"Embeddings & Metrics","session_uid":"5cf45946-05e6-4d9c-876e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Embeddings & Metrics"],"time_stamp":"2025-11-05T13:24:00.000Z","title":"What Makes a Visualization Image Complex?","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gvbjZE1CgLQ"},{"UID":"5d4ddab6-e79a-4a05-8604-2d252e0aa269","abstract":"Guidance-enhanced approaches are used to support users in making sense of their data and overcoming challenging analytical scenarios. While recent literature underscores the value of guidance, a lack of clear explanations to motivate system interventions may still negatively impact guidance effectiveness. Hence, guidance-enhanced VA approaches require meticulous design, demanding contextual adjustments for developing appropriate explanations. Our article discusses the concept of explainable guidance and how it impacts the user\u2013system relationship\u2014specifically, a user's trust in guidance within the VA process. We subsequently propose a model that supports the design of explainability strategies for guidance in VA. The model builds upon flourishing literature in explainable AI, available guidelines for developing effective guidance in VA systems, and accrued knowledge on user\u2013system trust dynamics. Our model responds to challenges concerning guidance adoption and context-effectiveness by fostering trust through appropriately designed explanations. To demonstrate the model's value, we employ it in designing explanations within two existing VA scenarios. We also describe a design walk-through with a guidance expert to showcase how our model supports designers in clarifying the rationale behind system interventions and designing explainable guidance.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Maath Musleh"},{"email":null,"name":"Renata Georgia Raidou"},{"email":null,"name":"Davide Ceneda"}],"award":"","doi":"10.1109/TVCG.2025.3562929","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"5d4ddab6-e79a-4a05-8604-2d252e0aa269","image_caption":"","keywords":["Artificial intelligence","Buildings","Explainable AI","Decision making","Context modeling","Boosting","Analytical models","Adaptation models","Visual analytics","Usability"],"open_access_supplemental_link":null,"open_access_supplemental_question":"We thoroughly document how we conduct the literature survey and extract concepts from the papers.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0733 ]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full30","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Trust No One","session_uid":"5d4ddab6-e79a-4a05-8604","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Trust No One"],"time_stamp":"2025-11-07T09:18:00.000Z","title":"TrustME: A Context-Aware Explainability Model to Promote User Trust in Guidance","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mavB31i3NwM"},{"UID":"5f806c3f-27b1-47e9-81ce-9255b7532d0f","abstract":"Team-based combat scenarios are prevalent in various real-world applications like video gaming. Analyzing tactics in these scenarios is essential for gaining insights into game processes and improving combat behaviors. The decision-making data in team-based combat include character actions, movement trajectories, and event sequences. Existing studies face challenges in visualizing and analyzing combat tactics due to the complexity and the multifaceted characteristics of the decision-making data. To address these challenges, we introduce TactiVis, a visual analytics system designed for analyzing combat decision-making behavior. Using MOBA game as a representative case of team-based combat, TactiVis adopts a macro-to-micro tactics visual analytics framework consisting of three stages: match-level analysis, event-level understanding, and character-level comparison. In the TactiVis system, we introduce the v-storyline visualization, which encodes positions along the vertical axis to reveal tactical patterns. Case studies and a usability study demonstrate the utility and usability of TactiVis for helping analysts understand combat patterns and analyze tactics.","accessible_pdf":"Accessible","authors":[{"affiliation":"Beijing Institute of Technology","email":"3120215511@bit.edu.cn","name":"Hancheng Zhang"},{"affiliation":"Beijing Institute of Technology","email":"liguozhengsdu@gmail.com","name":"Guozheng Li"},{"affiliation":"Shenzhen University","email":"lumin.vis@gmail.com","name":"Min Lu"},{"affiliation":"Beijing Normal University","email":"jinchengli@bnu.edu.cn","name":"Jincheng Li"},{"affiliation":"Beijing Institute of Technology","email":"liuchi02@gmail.com","name":"Chi Harold Liu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"5f806c3f-27b1-47e9-81ce-9255b7532d0f","image_caption":"","keywords":["Team-based combat","storyline visualization","MOBA games","tactic analysis."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1897","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full16","session_room":"Hall M1","session_room_id":"m1","session_title":"Games, Sports, and Music","session_uid":"5f806c3f-27b1-47e9-81ce","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Games, Sports, and Music"],"time_stamp":"2025-11-06T10:39:00.000Z","title":"TactiVis: Towards Better Understanding of Team-based Combat Tactics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/w72IpVgxklQ"},{"UID":"5fedee73-4905-4fd7-9b29-fd2c5684ca48","abstract":"ParaView is one of the most prominent software tools for scientific visualization used by scientists around the world. Color is a primary conduit to visually map data to its representation and, thus, enable investigation and interpretation of the data. Colormap selection has a significant impact on the data revealed; its design and selection is a critical aspect of scientific data visualization. A common choice for a user is the program\u2019s default colormap, so careful consideration of this default is consequential. Although the current default colormap in ParaView, a succession of hues from cool blue to warm red, has served the community well, research shows that more nuanced colormap configurations increase discriminability while maintaining other critical metrics. These findings inspire us to revisit and update the default colors in ParaView. Here we present a new ParaView default colormap, the criteria and methods of development, and example visualizations and analytic metrics.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Francesca Samsel"},{"email":null,"name":"W. Alan Scott"},{"email":null,"name":"Kenneth Moreland"}],"award":"","doi":"10.1109/MCG.2024.3383137","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"5fedee73-4905-4fd7-9b29-fd2c5684ca48","image_caption":"","keywords":["Software tools","Data visualization","Image color analysis","Analytical models","Color"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3383137","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga2","session_room":"Hall E1","session_room_id":"e1","session_title":"Visual Analytics Methods, Tools, and Infrastructure","session_uid":"5fedee73-4905-4fd7-9b29","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visual Analytics Methods, Tools, and Infrastructure"],"time_stamp":"2025-11-06T11:15:00.000Z","title":"A New Default Colormap for ParaView","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JfetpDJuh5Q"},{"UID":"603070d4-693a-4aec-8b08-632a59a24c59","abstract":"Though powerful tools for analysis and communication, interactive visualizations often fail to support real-time interaction with large datasets with millions or more records. To highlight and filter data, users indicate values or intervals of interest. Such selections may span multiple components, combine in complex ways, and require optimizations to ensure low-latency updates. We describe Mosaic Selections, a model for representing, managing, and optimizing user selections, in which one or more filter predicates are added to queries that request data for visualizations and input widgets. By analyzing both queries and selection predicates, Mosaic Selections enable automatic optimizations, including pre-aggregating data to rapidly compute selection updates. We contribute a formal description of our selection model and optimization methods, and their implementation in the open-source Mosaic architecture. Benchmark results demonstrate orders-of-magnitude latency improvements for selection-based optimizations over unoptimized queries and existing optimizers for the Vega language. The Mosaic Selection model provides infrastructure for flexible, interoperable filtering across multiple visualizations, alongside automatic optimizations to scale to millions and even billions of records.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Washington","email":"jheer@uw.edu","name":"Jeffrey Heer"},{"affiliation":"Carnegie Mellon University","email":"domoritz@cmu.edu","name":"Dominik Moritz"},{"affiliation":"University of Washington","email":"rpechuk@uw.edu","name":"Ron Pechuk"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"603070d4-693a-4aec-8b08-632a59a24c59","image_caption":"","keywords":["scalable visualization","interactive selection","multiple coordinated views","brushing and linking","user interfaces"],"open_access_supplemental_link":"https://github.com/uwdata/mosaic-selection-benchmarks","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.19690","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1313","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full2","session_room":"Hall E2","session_room_id":"e2","session_title":"Algorithms and Workflows","session_uid":"603070d4-693a-4aec-8b08","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Algorithms and Workflows"],"time_stamp":"2025-11-06T13:00:00.000Z","title":"Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/wUoqSN_s3tw"},{"UID":"605afb22-7f43-443f-9183-77604f4577f3","abstract":"Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.","accessible_pdf":null,"authors":[{"affiliation":"University of Washington","email":"snyderl@cs.washington.edu","name":"Luke S. Snyder"},{"affiliation":"Microsoft Research","email":"clwang15uw@gmail.com","name":"Chenglong Wang"},{"affiliation":"Microsoft Research","email":"sdrucker@microsoft.com","name":"Steven Drucker"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"605afb22-7f43-443f-9183-77604f4577f3","image_caption":"","keywords":["Visualization","Large Language Models","Retargeting"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"http://arxiv.org/abs/2507.01436","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1128","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short12","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Viz for AI & AI for Viz","session_uid":"605afb22-7f43-443f-9183","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Viz for AI & AI for Viz"],"time_stamp":"2025-11-05T10:15:00.000Z","title":"Challenges & Opportunities with LLM-Assisted Visualization Retargeting","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"60b8dd96-21c8-4a3d-8e8b-b50374e1230e","abstract":"In this paper, we evaluated the use of three pre-attentive visual variables, namely, area, color hue, and flicker, to highlight elements on four charts: bar chart, line chart, linear progress chart, and radial progress chart. We exposed 48 participants---using short time lapses of 250 ms---to a set of stimuli showing one of the four charts with the three visual variables. Then we examined the accuracy of their answers in identifying the highlighted target element under sedentary and walking conditions. Our results show that all three visual variables can be perceived with a medium to high accuracy rate, though performance varies depending on the mobility condition, the visual variable, and even the chart type. Among the visual variables, color and flicker yield the best target identification accuracy rates, while the area proves to be the least effective. The findings from our first investigation contribute to the design of efficient smartwatch micro visualizations that support glanceable and in-motion data reading.","accessible_pdf":null,"authors":[{"affiliation":"University of Stuttgart","email":"fairouz.grioui@vis.uni-stuttgart.de","name":"Fairouz Grioui"},{"affiliation":"Universit\u00e4t Stuttgart","email":"st166324@stud.uni-stuttgart.de","name":"Yasmin Amzir"},{"affiliation":"University of Stuttgart","email":"nina.doerr@visus.uni-stuttgart.de","name":"Nina Doerr"},{"affiliation":"University of Stuttgart","email":"research@blascheck.eu","name":"Tanja Blascheck"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"60b8dd96-21c8-4a3d-8e8b-b50374e1230e","image_caption":"","keywords":["pre-attentive visual variables","micro visualization","mobile visualizations","smartwatch."],"open_access_supplemental_link":"https://osf.io/n348a/?view_only=139b008328dd44708945ed6fab310a1a","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1272","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"60b8dd96-21c8-4a3d-8e8b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T15:03:00.000Z","title":"Comparing Pre-attentive Visual Variables in a Target Identification Task for Glanceable Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"619269e9-f6be-4882-aec8-0ffe4cee1a96","abstract":"We contribute an in-depth analysis of the workflows and tensions arising from generative AI (genAI) use in biomedical visualization (BioMedVis). Although genAI affords facile production of aesthetic visuals for biological and medical content, the architecture of these tools fundamentally limits the accuracy and trustworthiness of the depicted information, from imaginary (or fanciful) molecules to alien anatomy. Through 17 interviews with a diverse group of practitioners and researchers, we qualitatively analyze the concerns and values driving genAI (dis)use for the visual representation of spatially oriented biomedical data. We find that BioMedVis experts, both in roles as developers and designers, use genAI tools at different stages of their daily workflows and hold attitudes ranging from enthusiastic adopters to skeptical avoiders of genAI. In contrasting the current use and perspectives on genAI observed in our study with predictions towards genAI in the visualization pipeline from prior work, we refocus the discussion of genAI\u2019s effects on projects in visualization in the here and now with its respective opportunities and pitfalls for future visualization research. At a time when public trust in science is in jeopardy, we are reminded to first do no harm, not just in biomedical visualization but in science communication more broadly. Our observations reaffirm the necessity of human intervention for empathetic design and assessment of accurate scientific visuals. Supplemental study materials are available at https://osf.io/genaixbiomedvis/.","accessible_pdf":null,"authors":[{"affiliation":"University of Bergen","email":"roxanne.ziman@uib.no","name":"Roxanne Ziman"},{"affiliation":"University of Toronto","email":"s.saharan@utoronto.ca","name":"Shehryar Saharan"},{"affiliation":"Harvard Medical School","email":"mcgill@digizyme.com","name":"Ga\u00ebl McGill"},{"affiliation":"University of Bergen","email":"laura.garrison@uib.no","name":"Laura Garrison"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"619269e9-f6be-4882-aec8-0ffe4cee1a96","image_caption":"","keywords":["biomedical visualization","science communication","generative AI","human-AI collaboration","creativity","qualitative methods"],"open_access_supplemental_link":"https://osf.io/mbw86/?view_only=e087ab5b90a6474abec7bfc42cd2b105","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.14494","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1548","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full28","session_room":"Hall E1","session_room_id":"e1","session_title":"The VIS in GenAI","session_uid":"619269e9-f6be-4882-aec8","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The VIS in GenAI"],"time_stamp":"2025-11-05T13:36:00.000Z","title":"It looks sexy but it's wrong.'' Tensions in creativity and accuracy using genAI for biomedical visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zLchh59XyM4"},{"UID":"625a86de-1529-4724-8869-f081c7776f88","abstract":"We present a new comprehensive theory for explaining, exploring, and using pattern as a visual variable in visualization. Although patterns have long been used for data encoding and continue to be valuable today, their conceptual foundations are precarious: the concepts and terminology used across the research literature and in practice are inconsistent, making it challenging to use patterns effectively and to conduct research to inform their use. To address this problem, we conduct a comprehensive cross-disciplinary literature review that clarifies ambiguities around the use of pattern and texture. As a result, we offer a new consistent treatment of pattern as a composite visual variable composed of structured groups of graphic primitives that can serve as marks for encoding data individually and collectively. This new and widely applicable formulation opens a sizable design space for the visual variable pattern, which we formalize as a new system comprising three sets of variables: the spatial arrangement of primitives, the appearance relationships among primitives, and the retinal visual variables that characterize individual primitives. We show how our pattern system relates to existing visualization theory and highlight opportunities for visualization design. We further explore patterns based on complex spatial arrangements, demonstrating explanatory power and connecting our conceptualization to broader theory on maps and cartography. An author version and additional materials are available on OSF: osf.io/z7ae2.","accessible_pdf":null,"authors":[{"affiliation":"University of Utah","email":"hetingying.hty@gmail.com","name":"Tingying He"},{"affiliation":"City, University of London","email":"j.dykes@city.ac.uk","name":"Jason Dykes"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"petra.isenberg@inria.fr","name":"Petra Isenberg"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"tobias.isenberg@gmail.com","name":"Tobias Isenberg"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"625a86de-1529-4724-8869-f081c7776f88","image_caption":"","keywords":["Pattern","texture","visual variables","retinal variables","data visualization","Jacques Bertin"],"open_access_supplemental_link":"https://osf.io/z7ae2/","open_access_supplemental_question":"We provide a well-documented appendix that includes detailed discussions and examples to support our theory. All images we created are correctly licensed under CC BY 4.0, and the paper has been uploaded to OSF and arXiv as well as will be shared on HAL; there is no code that relates to the paper, so a replicability stamp application was not an option.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.02639","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1614","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full33","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Encoding & Comprehension","session_uid":"625a86de-1529-4724-8869","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Encoding & Comprehension"],"time_stamp":"2025-11-05T08:54:00.000Z","title":"Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"63daa1e4-de6c-4f05-8528-64eaf5d0295c","abstract":"Interactive visualization editors empower users to author visualizations without writing code, but do not provide guidance on the art and craft of effective visual communication. In this article, we explore the potential of using an off-the-shelf large language models (LLMs) to provide actionable and customized feedback to visualization designers. Our implementation, Visualizationary, demonstrates how ChatGPT can be used for this purpose through two key components: a preamble of visualization design guidelines and a suite of perceptual filters that extract salient metrics from a visualization image. We present findings from a longitudinal user study involving 13 visualization designers\u20146 novices, 4 intermediates, and 3 experts\u2014who authored a new visualization from scratch over several days. Our results indicate that providing guidance in natural language via an LLM can aid even seasoned designers in refining their visualizations.","accessible_pdf":"No","authors":[{"email":null,"name":"Sungbok Shin"},{"email":null,"name":"Sanghyun Hong"},{"email":null,"name":"Niklas Elmqvist"}],"award":"","doi":"10.1109/TVCG.2025.3579700","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"63daa1e4-de6c-4f05-8528-64eaf5d0295c","image_caption":"","keywords":["Data visualization","Visualization","Computational modeling","Training","Measurement","Filters","Predictive models","Image color analysis","Translation","Large language models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2409.13109","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3579700","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full1","session_room":"Hall E1","session_room_id":"e1","session_title":"Agentic Visualization and Intelligent Systems","session_uid":"63daa1e4-de6c-4f05-8528","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Agentic Visualization and Intelligent Systems"],"time_stamp":"2025-11-05T15:33:00.000Z","title":"Visualizationary: Automating Design Feedback for Visualization Designers Using LLMs","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/2z0_8lHbM04"},{"UID":"64c1bc8e-6529-47d5-8f6e-c194f4766ec2","abstract":"Studies of visual semantics for information visualization aim to understand observers\u2019 expectations about the meaning of visual features (e.g., color, texture) because visualizations that align with those expectations are easier to interpret. Previous work on visual semantics focused primarily on color, with the implicit assumption that color semantics is unaffected by changes in the size of the visualization (given sufficient perceptual discriminability across sizes). Changing size from small scale (e.g., small figures in a paper) to large scale (e.g., large figures in a slide presentation) is straightforward for visualizations that have solid colored regions, but can be more complicated for visualizations with heterogeneous textures because there are multiple ways to scale textures\u2014zooming or repeating texture elements. Previous work suggested that original textures were more perceptually similar to repeat-scaled rather than zoom-scaled textures. Here, we found that texture semantics was preserved after both types of enlargement, suggesting that texture semantics is robust to scaling, at least for geometric textures in which elements are visible at all scales.","accessible_pdf":null,"authors":[{"affiliation":"University of Wisconsin - Madison","email":"zoeshoward@gmail.com","name":"Zoe S. Howard"},{"affiliation":"University of Wisconsin - Madison","email":"kschloss@wisc.edu","name":"Karen B. Schloss"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"64c1bc8e-6529-47d5-8f6e-c194f4766ec2","image_caption":"","keywords":["information visualization","perceptual semantics","visual reasoning","texture perception","visual communication"],"open_access_supplemental_link":"https://osf.io/s7ghe/?view_only=66b92b5dd3e74ee3a231e9b4c9e786a6","open_access_supplemental_question":"The experiment stimuli, data, and analysis code are posted on OSF","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/4uadx_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1227","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"64c1bc8e-6529-47d5-8f6e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T15:39:00.000Z","title":"Texture Semantics is Robust to Scaling","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"652040d5-96ca-48fd-a25f-f9bc2bb89dd1","abstract":"We contribute a design study on using visual analytics for AI-assisted music composition. The main result is the interface MAICO (Music AI Co-creativity), which allows composers and other music creators to interactively generate, explore, select, edit, and compare samples from generative music models. MAICO is based on the idea of visual parameter space analysis and supports the simultaneous analysis of hundreds of short samples of symbolic music from multiple models, displaying them in different metric- and similarity-based layouts. We developed and evaluated MAICO together with a professional composer who actively used it for five months to create, among other things, a composition for the Biennale Arte 2024 in Venice, which was recorded by the Munich Symphonic Orchestra. We discuss our design choices and lessons learned from this endeavor to support Human-AI co-creativity with visual analytics.","accessible_pdf":"No","authors":[{"email":null,"name":"Simeon Rau"},{"email":null,"name":"Frank Heyen"},{"email":null,"name":"Benedikt Brachtel"},{"email":null,"name":"Michael Sedlmair"}],"award":"","doi":"10.1109/TVCG.2025.3539779","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"652040d5-96ca-48fd-a25f-f9bc2bb89dd1","image_caption":"","keywords":["Music","Artificial intelligence","Analytical models","Data visualization","Data models","Visual analytics","Aerospace electronics","Training","Generative AI","Computational modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0704.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full16","session_room":"Hall M1","session_room_id":"m1","session_title":"Games, Sports, and Music","session_uid":"652040d5-96ca-48fd-a25f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Games, Sports, and Music"],"time_stamp":"2025-11-06T11:03:00.000Z","title":"MAICO: A Visualization Design Study on AI-Assisted Music Composition","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/w72IpVgxklQ"},{"UID":"65c676ce-bbd8-4a3f-a650-73e39c0dc99f","abstract":"With the prevalence of smart contracts, smart Ponzi schemes have become a common fraud on blockchain and have caused significant financial loss to cryptocurrency investors in the past few years. Despite the critical importance of detecting smart Ponzi schemes, a reliable and transparent identification approach adaptive to various smart Ponzi schemes is still missing. To fill the research gap, we first extract semantic-meaningful actions to represent the execution behaviors specified in smart contract bytecodes, which are derived from a literature review and in-depth interviews with domain experts. We then propose PonziLens+, a novel visual analytic approach that provides an intuitive and reliable analysis of Ponzi-scheme-related features within these execution behaviors. PonziLens+ has three visualization modules that intuitively reveal all potential behaviors of a smart contract, highlighting fraudulent features across three levels of detail. It can help smart contract investors and auditors achieve confident identification of any smart Ponzi schemes. We conducted two case studies and in-depth user interviews with 12 domain experts and common investors to evaluate PonziLens+. The results demonstrate the effectiveness and usability of PonziLens+ in achieving an effective identification of smart Ponzi schemes.","accessible_pdf":"No","authors":[{"email":null,"name":"Xiaolin Wen"},{"email":null,"name":"Tai D. Nguyen"},{"email":null,"name":"Shaolun Ruan"},{"email":null,"name":"Qiaomu Shen"},{"email":null,"name":"Jun Sun"},{"email":null,"name":"Feida Zhu"},{"email":null,"name":"Yong Wang"}],"award":"","doi":"10.1109/TVCG.2024.3516379","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"65c676ce-bbd8-4a3f-a650-73e39c0dc99f","image_caption":"","keywords":["Smart contracts","Blockchains","Codes","Fraud","Source coding","Data visualization","Flow graphs","Feature extraction","Reliability","Investment"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2412.18470","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-02-0089]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full3","session_room":"Hall E1","session_room_id":"e1","session_title":"Analysts, Assemble!","session_uid":"65c676ce-bbd8-4a3f-a650","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analysts, Assemble!"],"time_stamp":"2025-11-06T14:00:00.000Z","title":"PonziLens+: Visualizing Bytecode Actions for Smart Ponzi Scheme Identification","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/fkfdha6pCeI"},{"UID":"67904347-7281-4081-b903-1114c27844ee","abstract":"Balaton Borders translates ecological data from Lake Balaton into ceramic tableware that represents human impact on the landscape, from reedbed reduction to shoreline modification and land erosion. Designed for performative dining, the pieces turn shared meals into multisensory encounters where food and data ceramics spark collective reflection on ecological disruption.","accessible_pdf":null,"authors":[{"affiliation":"Independent Artist","email":"gyeviki.hajnal@gmail.com","name":"Hajnal Gyeviki"},{"affiliation":"Moholy-Nagy University of Art and Design","email":"minko.mihaly@mome.hu","name":"Mih\u00e1ly Mink\u00f3"},{"affiliation":"Moholy-Nagy University of Art and Design","email":"karyda@mome.hu","name":"Mary Karyda"},{"affiliation":"Moholy-Nagy University of Art and Design","email":"damla.cay@mome.hu","name":"Damla \u00c7ay"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"67904347-7281-4081-b903-1114c27844ee","image_caption":"","keywords":["data physicalization","data edibilization","commensality","ecological design","environmental data","ceramic design","performative dining"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1000","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap1","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Pictorial","session_uid":"67904347-7281-4081-b903","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Pictorial"],"time_stamp":"2025-11-06T10:39:00.000Z","title":"Balaton Borders: Data Ceramics for Ecological Reflection","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/NZ0Wwrqv6fA"},{"UID":"67fc7baa-0f81-46bb-b2ca-85e82da220be","abstract":"We present a novel visualization-driven illumination model for density plots, a new technique to enhance density plots by effectively revealing the detailed structures in high- and medium-density regions and outliers in low-density regions, while avoiding artifacts in the density field's colors. When visualizing large and dense discrete point samples, scatterplots and dot density maps often suffer from overplotting, and density plots are commonly employed to provide aggregated views while revealing underlying structures. Yet, in such density plots, existing illumination models may produce color distortion and hide details in low-density regions, making it challenging to look up density values, compare them, and find outliers. The key novelty in this work includes (i) a visualization-driven illumination model that inherently supports density-plot-specific analysis tasks and (ii) a new image composition technique to reduce the interference between the image shading and the color-encoded density values. To demonstrate the effectiveness of our technique, we conducted a quantitative study, an empirical evaluation of our technique in a controlled study, and two case studies, exploring twelve datasets with up to two million data point samples.","accessible_pdf":"No","authors":[{"email":null,"name":"Xin Chen"},{"email":null,"name":"Yunhai Wang"},{"email":null,"name":"Huaiwei Bao"},{"email":null,"name":"Kecheng Lu"},{"email":null,"name":"Jaemin Jo"},{"email":null,"name":"Chi-Wing Fu"},{"email":null,"name":"Jean-Daniel Fekete"}],"award":"","doi":"10.1109/TVCG.2024.3495695","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"67fc7baa-0f81-46bb-b2ca-85e82da220be","image_caption":"","keywords":["Image color analysis","Lighting","Data visualization","Visualization","Distortion","Kernel","Estimation","Computational modeling","Analytical models","Shape"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.17265","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0683]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full25","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception","session_uid":"67fc7baa-0f81-46bb-b2ca","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception"],"time_stamp":"2025-11-06T14:00:00.000Z","title":"Visualization-Driven Illumination for Density Plots","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/D_j0t7Fy0w0"},{"UID":"6a28982d-2f47-4640-b7cd-0fd259a03615","abstract":"In many disciplines, the findable, accessible, interoperable, and reusable (FAIR) principles are a gold standard for data management and handling. While these disciplines often require FAIR principles to be at least partially implemented in their scientific work to avoid desk rejects, the visualization community is still working on a proper way to deal with the FAIR principles. In this article, we aim to show the relation between the visualization community and the FAIR community. This will be accomplished by a showcase of other communities that are incorporating the FAIR principles successfully. Based on these investigations, we aim to provide suggestions on how to implement FAIR principles in the visualization community.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Daniel Wiegreffe"},{"email":null,"name":"Christoph Garth"},{"email":null,"name":"Guido Reina"},{"email":null,"name":"Christina Gillmann"}],"award":"","doi":"10.1109/MCG.2024.3495932","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"6a28982d-2f47-4640-b7cd-0fd259a03615","image_caption":"","keywords":["Data visualization","Data handling","Information management","Information retrieval","Usability"],"open_access_supplemental_link":null,"open_access_supplemental_question":"The work examines if VIS is implementing the FAIR principles.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3495932","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga1","session_room":"Room 0.96 + 0.97","session_room_id":"0_96_0_97","session_title":"Reflections and Looking Forward","session_uid":"6a28982d-2f47-4640-b7cd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Reflections and Looking Forward"],"time_stamp":"2025-11-07T08:42:00.000Z","title":"How FAIR is VIS?","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/29PgqT6NWUQ"},{"UID":"6a8c1148-d065-4251-9183-2955b9445817","abstract":"In contemporary information ecologies saturated with misinformation, disinformation, and a distrust of science itself, public data communication faces significant hurdles. Although visualization research has broadened criteria for effective design, governing\nparadigms privilege the accurate and efficient transmission of data. Drawing on theory from linguistic anthropology, we argue that such approaches\u2014focused on encoding and decoding propositional content\u2014cannot fully account for how people engage with visualizations and why particular visualizations might invite adversarial or receptive responses. In this paper, we present evidence that data visualizations communicate not only semantic, propositional meaning\u2014meaning about data\u2014but also social, indexical meaning\u2014meaning beyond data. From a series of ethnographically-informed interviews, we document how readers make rich and varied assessments of a visualization\u2019s \u201cvibes\u201d\u2014inferences about the social provenance of a visualization based on its design features. Furthermore, these social attributions have the power to influence reception, as readers\u2019 decisions about how to engage with a visualization concern not only content, or even aesthetic appeal, but also their sense of alignment or disalignment with the entities they imagine to be involved in its production and circulation. We argue these inferences hinge on a function of human sign systems that has thus far been little studied in data visualization: socio-indexicality, whereby the formal features (rather than the content) of communication evoke social contexts, identities, and characteristics. Demonstrating the presence and significance of this socio-indexical function in visualization, this paper offers both a conceptual foundation and practical intervention for troubleshooting breakdowns in public data communication.","accessible_pdf":"Accessible","authors":[{"affiliation":"Massachusetts Institute of Technology","email":"mjmorgen@mit.edu","name":"Michelle Morgenstern"},{"affiliation":"Massachusetts Institute of Technology","email":"amyraefoxphd@gmail.com","name":"Amy Fox"},{"affiliation":"Massachusetts Institute of Technology","email":"gmj@mit.edu","name":"Graham Jones"},{"affiliation":"MIT","email":"arvindsatya@mit.edu","name":"Arvind Satyanarayan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"6a8c1148-d065-4251-9183-2955b9445817","image_caption":"","keywords":["Semiotics","Socio-indexicality","Attitudes","Reception","Engagement","Visualization Psychology","Public Data Communication"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/ERC6P","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1005","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full30","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Trust No One","session_uid":"6a8c1148-d065-4251-9183","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Trust No One"],"time_stamp":"2025-11-07T08:30:00.000Z","title":"Visualization Vibes: The Socio-Indexical Function of Visualization Design","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mavB31i3NwM"},{"UID":"6c986f26-eabe-4109-b6ab-7ba4c489451d","abstract":"Maps are essential to news media as they provide a familiar way to convey spatial context and present engaging narratives. However, the design of journalistic maps may be challenging, as editorial teams need to balance multiple aspects, such as aesthetics, the audience\u2019s expected data literacy, tight publication deadlines, and the team\u2019s technical skills. Data journalists often come from multiple areas and lack a cartography, data visualization, and data science background, limiting their competence in creating maps. While previous studies have examined spatial visualizations in data stories, this research seeks to gain a deeper understanding of the map design process employed by news outlets. To achieve this, we strive to answer two specific research questions: what is the design space of journalistic maps? and how do editorial teams produce journalistic map articles? To answer the first one, we collected and analyzed a large corpus of 462 journalistic maps used in news articles from five major news outlets published over three months. As a result, we created a design space comprised of eight dimensions that involved both properties describing the articles\u2019 aspects and the visual/interactive features of maps. We approach the second research question via semi-structured interviews with four data journalists who create data-driven articles daily. Through these interviews, we identified the most common design rationales made by editorial teams and potential gaps in current practices. We also collected the practitioners\u2019 feedback on our design space to externally validate it. With these results, we aim to provide researchers and journalists with empirical data to design and study journalistic maps.","accessible_pdf":null,"authors":[{"affiliation":"Universidade Federal de Pernambuco","email":"agsn@cin.ufpe.br","name":"Arlindo Gomes"},{"affiliation":"Universidade Federal de Pernambuco","email":"emilly.brito@ufpe.br","name":"Emilly Brito"},{"affiliation":"Universidade Federal de Pernambuco","email":"gusto@cin.ufpe.br","name":"Luiz Morais"},{"affiliation":"Universidade Federal de Pernambuco","email":"nivan@cin.ufpe.br","name":"Nivan Ferreira"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"6c986f26-eabe-4109-b6ab-7ba4c489451d","image_caption":"","keywords":["design space","maps","news","visualization design","data visualization","data journalism"],"open_access_supplemental_link":"https://osf.io/bhn6e/?view_only=6a130b6323364a1db514a3df45b94647","open_access_supplemental_question":"We provided our data collection in PDF format, a dataset that other authors can use to conduct new experiments and apply our data to different objectives, such as exploring data literacy or trust in data visualization. Additionally, we provided our classification of those maps in spreadsheet format, allowing other researchers to perform new analyses and explore factors we did not discuss in our data. Finally, we provide a Codebook that introduces newcomers to our design space and can also be used as supporting material for classes and workshops. We believe that these practices offer many possibilities for researchers to explore ways we did not and expand this work with new ideas and findings.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1345","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full20","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Maps & Spatial Vis","session_uid":"6c986f26-eabe-4109-b6ab","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Maps & Spatial Vis"],"time_stamp":"2025-11-05T08:54:00.000Z","title":"How do Data Journalists Design Maps to Tell Stories?","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/nkeAR0PgfSw"},{"UID":"6d474cda-8152-40b9-ba2f-7a28f9bbe0bd","abstract":"Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.","accessible_pdf":null,"authors":[{"affiliation":"City, University of London","email":"maeve.hutchinson@city.ac.uk","name":"Maeve Hutchinson"},{"affiliation":"City, University of London","email":"radu.jianu@city.ac.uk","name":"Radu Jianu"},{"affiliation":"City, University of London","email":"a.slingsby@city.ac.uk","name":"Aidan Slingsby"},{"affiliation":"City, University of London","email":"j.d.wood@city.ac.uk","name":"Jo Wood"},{"affiliation":"City, University of London","email":"pranava.madhyastha@city.ac.uk","name":"Pranava Madhyastha"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"6d474cda-8152-40b9-ba2f-7a28f9bbe0bd","image_caption":"","keywords":["Design","Literate Visualization","Natural Language"],"open_access_supplemental_link":"https://github.com/maevehutch/DesignQAR; https://maevehutch.github.io/DesignQAR/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2506.16571","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1218","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"6d474cda-8152-40b9-ba2f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T09:06:00.000Z","title":"Capturing Visualization Design Rationale","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"6f935187-a32b-477d-90a0-1a7e959c6222","abstract":"Vision Language Models (VLMs) demonstrate promising chart comprehension capabilities. Yet, prior explorations of their visualization literacy have been limited to assessing their response correctness and fail to explore their internal reasoning. To address this gap, we adapted attention-guided class activation maps (AG-CAM) for VLMs to visualize the influence and importance of input features (image and text) on model responses.  Using this approach,  we conducted an examination of four open-source (ChartGemma, Janus 1B and 7B, and LLaVA) and two closed-source (GPT-4o, Gemini) models comparing their performance and, for the open-source models, their AG-CAM results. Overall, we found that ChartGemma, a 3B parameter VLM fine-tuned for chart question-answering (QA), outperformed other open-source models and exhibited performance on par with significantly larger closed-source VLMs. We also found that VLMs exhibit spatial reasoning by accurately localizing key chart features, and semantic reasoning by associating visual elements with corresponding data values and query tokens. Our approach is the first to demonstrate the use of AG-CAM on early fusion VLM architectures, which are widely used, and for chart QA. We also show preliminary evidence that these results can align with human reasoning. Our promising open-source VLMs results pave the way for transparent and reproducible research in AI visualization literacy.","accessible_pdf":null,"authors":[{"affiliation":"University of Waterloo","email":"a7dong@uwaterloo.ca","name":"Lianghan Dong"},{"affiliation":"University of Waterloo","email":"amcrisan@uwaterloo.ca","name":"Anamaria Crisan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"6f935187-a32b-477d-90a0-1a7e959c6222","image_caption":"","keywords":["Vision Language Models","Visualization Literacy","Explainability","Chart Question and Answering"],"open_access_supplemental_link":"https://osf.io/fp3rg/","open_access_supplemental_question":"Novel methods, open source code, live demo","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2504.05445","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1740","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full4","session_room":"Hall E2","session_room_id":"e2","session_title":"Analytics & Reasoning","session_uid":"6f935187-a32b-477d-90a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analytics & Reasoning"],"time_stamp":"2025-11-06T14:57:00.000Z","title":"Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/vzBq3yA2yUs"},{"UID":"707ca454-9b92-46e1-88a4-98f997fb6434","abstract":"Deep learning (DL) models have proven to be suitable for various applications, achieving state-of-the-art performance. Despite that, they experience notable performance drops when subjected to realistic transformations in input data. Analyzing model behavior under input transformations is essential to preemptively identify possible\nbreaking points and understand what image characteristics might cause them before their failure in the real world. We introduce SEG-RobustEye, a visual analytics (VA) design developed to assist in the evaluation of the robustness of segmentation models for endoscopy images under realistic input transformations. SEG-RobustEye is based on ProactiV [13], a VA framework designed for understanding the behaviour of image-to-image translation models. SEG-RobustEye is a tailored instance of the framework and an extension of ProactiV for medical images, in concrete endoscopic images. These require visual designs that enhance the relevant features for such medical applications, which are different from general natural scenes. SEG-RobustEye is designed to discover features that affect the model behaviour under specific transformations.\nSEG-RobustEye connects the provided perspectives by ProactiV, i.e., global and instance level, and extends to subgroup level patterns. Subgroup level patterns facilitate the discovery and generalizability of selected subgroups of instances. The value of our approach was verified against real-world cases in endoscopy imaging by DL developers as proof of concept of the potential of SEG-RobustEye and, by extension, of ProactiV.\n\n[13] V. Prasad, R. J. van Sloun, A. Vilanova, and N. Pezzotti. ProactiV: Studying deep learning model behavior under input transformations. IEEE Transactions on Visualization and Computer Graphics, 30(8):5651\u20135665, 2024","accessible_pdf":null,"authors":[{"affiliation":"Eindhoven University of Technolocy","email":"popa.ada@gmail.com","name":"Andreea Melania Popa"},{"affiliation":"Eindhoven University of Technology","email":"v.prasad@tue.nl","name":"Vidya Prasad"},{"affiliation":"Eindhoven University of Technolocy","email":"t.j.m.jaspers@tue.nl","name":"Tim J.M. Jaspers"},{"affiliation":"Eindhoven University of Technology","email":"fvdsommen@tue.nl","name":"Fons van der Sommen"},{"affiliation":"Eindhoven University of Technology","email":"a.vilanova@tue.nl","name":"Anna Vilanova"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"707ca454-9b92-46e1-88a4-98f997fb6434","image_caption":"","keywords":["Visual analytics","explainable AI","medical imaging","input transformations","robustness analysis","model behavior","deep learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1153","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"707ca454-9b92-46e1-88a4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T10:42:00.000Z","title":"SEG-RobustEye: Understanding medical image segmentation models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"70fe9706-bf4e-4d28-a8e4-fc5b25dca854","abstract":"We present the Critical Design Strategy (CDS)---a structured method designed to facilitate the examination of visualisation designs through reflection and critical thought. The CDS helps designers think critically and make informed improvements using heuristic evaluation. When developing a visual tool or pioneering a novel visualisation approach, identifying areas for enhancement can be challenging. Critical thinking is particularly crucial for visualisation designers and tool developers, especially those new to the field, such as studying visualisation in higher education. The CDS consists of three stages across six perspectives: Stage 1 captures the essence of the idea by assigning an indicative title and selecting five adjectives (from twenty options) to form initial impressions of the design. Stage 2 involves an in-depth critique using 30 heuristic questions spanning six key perspectives---user, environment, interface, components, design, and visual marks. Stage 3 focuses on synthesising insights, reflecting on design decisions, and determining the next steps forward. We introduce the CDS and explore its use across three visualisation modules in both undergraduate and postgraduate courses. Our longstanding experience with the CDS has allowed us to refine and develop it over time: from its initial creation through workshops in 2017/18 to improvements in wording and the development of two applications by 2020, followed by the expansion of support notes and refinement of heuristics through 2023; while using it in our teaching each year. This sustained use allows us to reflect on its practical application and offer guidance on how others can incorporate it into their own work.","accessible_pdf":"Accessible","authors":[{"affiliation":"Bangor University","email":"j.c.roberts@bangor.ac.uk","name":"Jonathan C. Roberts"},{"affiliation":"Basrah University","email":"hananalnjar@yahoo.com","name":"Hanan Alnjar"},{"affiliation":"Bangor University","email":"aron.e.owen@bangor.ac.uk","name":"Aron Owen"},{"affiliation":"Bangor University","email":"p.ritsos@bangor.ac.uk","name":"Panagiotis D. Ritsos"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"70fe9706-bf4e-4d28-a8e4-fc5b25dca854","image_caption":"","keywords":["Visualisation design","Design critique","Pedagogy","Visualisation theory","Information visualisation","Teaching visualisation"],"open_access_supplemental_link":"https://arxiv.org/abs/2508.05325","open_access_supplemental_question":"Novel heuristic evaluation method with detailed supplemental notes available via IEEE TVCG and the arXiv preprint (https://arxiv.org/abs/2508.05325). A supporting website hosts an online CDS tool, extended guidance, and direct links to all supplemental materials (https://cds-critical-design-strategy.github.io/index.html).","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.05325","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1737","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full21","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Models, Methods, and Typologies","session_uid":"70fe9706-bf4e-4d28-a8e4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Models, Methods, and Typologies"],"time_stamp":"2025-11-07T08:42:00.000Z","title":"Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/moykjX8QeZI"},{"UID":"72706d51-0934-4969-9f1d-a4b478186fc3","abstract":"Scientists often explore and analyze large-scale scientific simulation data by leveraging 2-D and 3-D visualizations. The data and tasks can be complex and therefore best supported using myriad display technologies, from mobile devices to large high-resolution display walls to virtual reality headsets. Using a simulation of neuron connections in the human brain provided for the 2023 IEEE Scientific Visualization Contest, we present our work leveraging various web technologies to create a multiplatform scientific visualization application. Users can spread visualization and interaction across multiple devices to support flexible user interfaces and both colocated and remote collaboration. Drawing inspiration from responsive web design principles, this work demonstrates that a single codebase can be adapted to develop scientific visualization applications that operate everywhere.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Thomas Marrinan"},{"email":null,"name":"Victor A. Mateevitsi"},{"email":null,"name":"Madeleine Moeller"},{"email":null,"name":"Alina Kanayinkal"},{"email":null,"name":"Michael E. Papka"}],"award":"","doi":"10.1109/MCG.2024.3444460","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"72706d51-0934-4969-9f1d-a4b478186fc3","image_caption":"","keywords":["Neurons","Data visualization","Collaboration","Three-dimensional displays","Calcium","Web design","Task analysis"],"open_access_supplemental_link":null,"open_access_supplemental_question":"substantial supplemental material","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2404.17619","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3444460","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full37","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Rescheduled Papers","session_uid":"72706d51-0934-4969-9f1d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Rescheduled Papers"],"time_stamp":"2025-11-06T13:36:00.000Z","title":"2023 IEEE Scientific Visualization Contest Winner: VisAnywhere: Developing Multiplatform Scientific Visualization Applications","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"72737da9-0932-41f6-99a2-803817cf8c15","abstract":"Interactive time-varying volume visualization is challenging due to its complex spatiotemporal features and sheer size of the dataset. Recent works transform the original discrete time-varying volumetric data into continuous Implicit Neural Representations (INR) to address the issues of compression, rendering, and super-resolution in both spatial and temporal domains. However, training the INR takes a long time to converge, especially when handling large-scale time-varying volumetric datasets. In this work, we proposed F-Hash, a novel feature-based multi-resolution Tesseract encoding architecture to greatly enhance the convergence speed compared with existing input encoding methods for modeling time-varying volumetric data. The proposed design incorporates multi-level collision-free hash functions that map dynamic 4D multi-resolution embedding grids without bucket waste, achieving high encoding capacity with compact encoding parameters. Our encoding method is agnostic to time-varying feature detection methods, making it a unified encoding solution for feature tracking and evolution visualization. Experiments show the F-Hash achieves state-of-the-art convergence speed in training various time-varying volumetric datasets for diverse features. We also proposed an adaptive ray marching algorithm to optimize the sample streaming for faster rendering of the time-varying neural representation.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Nebraska-Lincoln","email":"sunjianxin66@gmail.com","name":"Jianxin Sun"},{"affiliation":"Argonne National Laboratory","email":"dlenz@anl.gov","name":"David Lenz"},{"affiliation":"University of Nebraska-Lincoln","email":"hfyu@unl.edu","name":"Hongfeng Yu"},{"affiliation":"Argonne National Laboratory","email":"tpeterka@mcs.anl.gov","name":"Tom Peterka"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"72737da9-0932-41f6-99a2-803817cf8c15","image_caption":"","keywords":["Time-varying volume","volume visualization","input encoding","deep learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.03836","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1455","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full36","session_room":"Hall M1","session_room_id":"m1","session_title":"Volume","session_uid":"72737da9-0932-41f6-99a2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Volume"],"time_stamp":"2025-11-05T13:00:00.000Z","title":"F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-M293vb-AIg"},{"UID":"72c11016-caee-44a4-82cf-1939cf5d21d3","abstract":"We present a data-domain sampling regime for quantifying CNNs\u2019 graphic perception behaviors. This regime lets us evaluate CNNs\u2019 ratio estimation ability in bar charts from three perspectives: sensitivity to training-test distribution discrepancies, stability to limited samples, and relative expertise to human observers. After analyzing 16 million trials from 800 CNN models and 6,825 trials from 113 human participants, we arrived at a simple and actionable conclusion: CNNs can outperform humans and their biases simply depend on the training-test distance. We show evidence of this simple, elegant behavior of the machines when they interpret visualization images. osf.io/gfqc3 provides registration, the code for our sampling regime, and experimental results.","accessible_pdf":null,"authors":[{"affiliation":"The Ohio State University","email":"jiang.2126@osu.edu","name":"Shuning Jiang"},{"affiliation":"The Ohio State University","email":"weilunchao760414@gmail.com","name":"Wei-Lun Chao"},{"affiliation":"University of Massachusetts Boston","email":"daniel.haehn@umb.edu","name":"Daniel Haehn"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"},{"affiliation":"The Ohio State University","email":"chen.8028@osu.edu","name":"Jian Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"72c11016-caee-44a4-82cf-1939cf5d21d3","image_caption":"","keywords":["Quantification","convolutional neural network","sampling","graphical perception","evaluation"],"open_access_supplemental_link":"https://osf.io/gfqc3/","open_access_supplemental_question":"Our work features thoroughly documented source code and interactive Google Colab notebooks, enabling immediate, zero-setup replication of our analyses online.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://www.arxiv.org/abs/2507.03866","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2070","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full33","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Encoding & Comprehension","session_uid":"72c11016-caee-44a4-82cf","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Encoding & Comprehension"],"time_stamp":"2025-11-05T09:30:00.000Z","title":"A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"758bf6f9-59e3-4c64-a3fe-b0dd2ff08a38","abstract":"Dashboards have arguably been the most used visualizations during the COVID-19 pandemic. They were used to communicate its evolution to national governments for disaster mitigation, to the public domain to inform about its status, and to epidemiologists to comprehend and predict the evolution of the disease. Each design had to be tailored for different tasks and to varying audiences\u2014in many cases set up in a very short time due to the urgent need. In this article, we collect notable examples of dashboards and reflect on their use and design during the pandemic from a user-oriented perspective. We interview a group of researchers with varying visualization expertise who actively used dashboards during the pandemic as part of their daily workflow. We discuss our findings and compile a list of lessons learned to support future visualization researchers and dashboard designers.","accessible_pdf":"No","authors":[{"email":null,"name":"Alessio Arleo"},{"email":null,"name":"Rita Borgo"},{"email":null,"name":"J\u00f6rn Kohlhammer"},{"email":null,"name":"Roy A. Ruddle"},{"email":null,"name":"H. Scharlach"},{"email":null,"name":"Xiaoru Yuan"},{"email":null,"name":"Melanie Tory"},{"email":null,"name":"Daniel Keefe"}],"award":"","doi":"10.1109/MCG.2025.3538257","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"758bf6f9-59e3-4c64-a3fe-b0dd2ff08a38","image_caption":"","keywords":["COVID-19","Visualization","Pandemics","Prevention and mitigation","Government","Public healthcare","Interviews","Fake news","Diseases"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2503.15529","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2025.3538257","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga1","session_room":"Room 0.96 + 0.97","session_room_id":"0_96_0_97","session_title":"Reflections and Looking Forward","session_uid":"758bf6f9-59e3-4c64-a3fe","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Reflections and Looking Forward"],"time_stamp":"2025-11-07T09:06:00.000Z","title":"Reflections on the Use of Dashboards in the COVID-19 Pandemic","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/29PgqT6NWUQ"},{"UID":"76560e50-f8e7-446f-943e-2ba7ff188c86","abstract":"Overdraw is inevitable in large-scale scatterplots. Current scatterplot abstraction methods lose features in medium-to-low density regions. We propose a visual abstraction method designed to provide better feature preservation across arbitrary abstraction levels for large-scale scatterplots, particularly in medium-to-low density regions. The method consists of three closely interconnected steps: first, we partition the scatterplot into iso-density regions and equalize visual density; then, we allocate pixels for different classes within each region; finally, we reconstruct the data distribution based on pixels. User studies, quantitative and qualitative evaluations demonstrate that, compared to previous methods, our approach better preserves features and exhibits a special advantage when handling ultra-high dynamic range data distributions.","accessible_pdf":null,"authors":[{"affiliation":"Visual Computing Lab","email":"sygzh6@tju.edu.cn","name":"Ziheng Guo"},{"affiliation":"Tianjin University","email":"tianxiangwei@tju.edu.cn","name":"Tianxiang Wei"},{"affiliation":"Communication University of China","email":"lzyfun@cuc.edu.cn","name":"Zeyu Li"},{"affiliation":"Tianjin University","email":"lianghaozhang@tju.edu.cn","name":"Lianghao Zhang"},{"affiliation":"Tianjin University","email":"sisilee144144@gmail.com","name":"Sisi Li"},{"affiliation":"Tianjin University","email":"jwzhang@tju.edu.cn","name":"Jiawan Zhang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"76560e50-f8e7-446f-943e-2ba7ff188c86","image_caption":"","keywords":["Scatterplot Abstraction","Overlap-free","Overdraw","Arbitrary Abstraction Level"],"open_access_supplemental_link":"https://github.com/Guozihengwww/PixelatedScatter","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1593","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full9","session_room":"Hall M2","session_room_id":"m2","session_title":"Dastardly Dashboards","session_uid":"76560e50-f8e7-446f-943e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dastardly Dashboards"],"time_stamp":"2025-11-05T08:30:00.000Z","title":"PixelatedScatter: Arbitrary-level Visual Abstraction for Large-scale Multiclass Scatterplots","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/Ga7NSmKd54M"},{"UID":"77c7ae29-07ac-4a7b-90d1-5ca6794c6632","abstract":"Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.","accessible_pdf":null,"authors":[{"affiliation":"University of Notre Dame","email":"ktang2@nd.edu","name":"Kaiyuan Tang"},{"affiliation":"University of Notre Dame","email":"kai@nd.edu","name":"Kuangshi Ai"},{"affiliation":"The Hong Kong University of Science and Technology","email":"junhanvis@outlook.com","name":"Jun Han"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"77c7ae29-07ac-4a7b-90d1-5ca6794c6632","image_caption":"","keywords":["Novel view synthesis","style transfer","textured Gaussian splatting","vision-language model","volume visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.13586","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1274","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full32","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Volumes & 3D","session_uid":"77c7ae29-07ac-4a7b-90d1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Volumes & 3D"],"time_stamp":"2025-11-06T11:03:00.000Z","title":"TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"787f3aab-3371-4bf1-94c6-9141cd0db272","abstract":"Visualization design influences how people perceive data patterns, yet most research focuses on low-level analytic tasks, such as finding correlations. The extent to which these perceptual affordances translate to high-level decision-making in the real world remains underexplored. Through a case study of academic mentorship selection using bar charts and pie charts, we investigated whether chart types differentially influence how students evaluate faculty research profiles. Our crowdsourced experiment revealed only minimal differences in decision outcomes between chart types, suggesting that perceptual affordances established in controlled analytical tasks may not directly translate to high-level decision scenarios. These findings emphasize the importance of evaluating visualizations within real-world contexts and highlight the need to distinguish between perceptual and decision affordances when developing visualization guidelines.","accessible_pdf":"Accessible","authors":[{"affiliation":"Georgia Institute of Technology","email":"yixuanli@gatech.edu","name":"Yixuan Li"},{"affiliation":"University of Massachusetts Amherst","email":"emery@cs.umass.edu","name":"Emery Berger"},{"affiliation":"Yonsei University","email":"minsuk.kahng@gmail.com","name":"Minsuk Kahng"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"787f3aab-3371-4bf1-94c6-9141cd0db272","image_caption":"","keywords":["Visual Affordances","Decision-Making","Perception"],"open_access_supplemental_link":"https://osf.io/gfuwp/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2410.04686","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1118","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"787f3aab-3371-4bf1-94c6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T14:54:00.000Z","title":"From Perception to Decision: Assessing the Role of Chart Type Affordances in High-Level Decision Tasks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"78f54a18-4ae6-4d52-9b39-50df068a510c","abstract":"In today's data-rich environment, visualization literacy\u2014the ability to understand and communicate information through charts\u2014is increasingly important. However, constructing effective charts can be challenging due to the numerous design choices involved. Off-the-shelf systems and libraries produce charts with carefully selected defaults that users may not be aware of, making it hard to increase their visualization literacy with those systems. In addition, traditional ways of improving visualization literacy, such as textbooks and tutorials, can be burdensome as they require sifting through a plethora of resources. To address this challenge, we designed Iguanodon, an easy-to-use game application that complements the traditional methods of improving visualization construction literacy. In our game application, users interactively choose whether to apply design choices, which we assign to sub-tasks that must be optimized to create an effective chart. The application offers multiple game variations to help users learn how different design choices should be applied to construct effective charts. Furthermore, our approach easily adapts to different visualization design guidelines. We describe the application's design and present the results of a user study with 37 participants. Our findings indicate that our game-based approach supports users in improving their visualization literacy.","accessible_pdf":"No","authors":[{"email":null,"name":"Patrick Adelberger"},{"email":null,"name":"Oleg Lesota"},{"email":null,"name":"Klaus Eckelt"},{"email":null,"name":"Markus Schedl"},{"email":null,"name":"Marc Streit"}],"award":"","doi":"10.1109/TVCG.2024.3468948","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"78f54a18-4ae6-4d52-9b39-50df068a510c","image_caption":"","keywords":["Data visualization","Games","Visualization","Guidelines","Tutorials","Design methodology","Bars","Topology","Reviews","Recommender systems"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/xung4_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023-07-0405.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full35","session_room":"Room 0.11 + 0.12","session_room_id":"0_11_0_12","session_title":"Visualization Literacy","session_uid":"78f54a18-4ae6-4d52-9b39","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization Literacy"],"time_stamp":"2025-11-07T09:30:00.000Z","title":"Iguanodon: A Code-Breaking Game for Improving Visualization Construction Literacy","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/R3RWFseiZmo"},{"UID":"79767fe9-582a-4d94-8cd6-2af68466de1d","abstract":"As the volume of scientific literature continues to expand exponentially, traditional research tools struggle to keep pace\u2014often reinforcing disciplinary silos and limiting opportunities for discovery. The Knowledge Cosmos re-imagines research exploration through an interactive, 3D visualization platform that treats science not as a static repository, but as a navigable universe. By spatializing 17 million academic papers based on semantic similarity, the platform enables users to explore the structure of knowledge intuitively, uncover interdisciplinary connections, and identify underexplored intellectual gaps. Drawing on the principles of play, immersion, and serendipity, The Knowledge Cosmos democratizes the bird\u2019s eye view of research and encourages curiosity-driven inquiry across a wide range of users including students, educators, independent thinkers, and lifelong learners. This paper outlines the conceptual foundations, design, and technological infrastructure of the platform, shares insights from preliminary usability testing, and discusses future directions to scale its potential as a catalyst for interdisciplinary exploration and knowledge creation.","accessible_pdf":null,"authors":[{"affiliation":"Independent Academic","email":"amcgail2@gmail.com","name":"Alec McGail"},{"affiliation":"Harvard University","email":"rifaa_tajani@mde.harvard.edu","name":"Rifaa Tajani"},{"affiliation":"Independent Academic","email":"nikitasridhar14@gmail.com","name":"Nikita Sridhar"},{"affiliation":"University of Texas at Austin","email":"stephlijiabao@gmail.com","name":"Jiabao Li"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"79767fe9-582a-4d94-8cd6-2af68466de1d","image_caption":"","keywords":["Large-scale data visualization","Science and technology visualization","User-centered design and evaluation","Knowledge visualization and concept mapping","Interdisciplinary research","3D and spatial interaction","Visual encoding and design"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1124","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap2","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Papers","session_uid":"79767fe9-582a-4d94-8cd6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Papers"],"time_stamp":"2025-11-06T09:30:00.000Z","title":"The Knowledge Cosmos: An Immersive Platform for Interdisciplinary Research Discovery","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VhkjwheU9Bc"},{"UID":"7a68a26e-4b20-444f-a76a-eee7a08ab3be","abstract":"Understanding the behavior of large language models (LLMs) is crucial for ensuring their safe and reliable use. However, existing explainable AI (XAI) methods for LLMs primarily rely on word-level explanations, which are often computationally inefficient and misaligned with human reasoning processes. Moreover, these methods often treat explanation as a one-time output, overlooking its inherently interactive and iterative nature. In this paper, we present LLM Analyzer, an interactive visualization system that addresses these limitations by enabling intuitive and efficient exploration of LLM behaviors through counterfactual analysis. Our system features a novel algorithm that generates fluent and semantically meaningful counterfactuals via targeted removal and replacement operations at user-defined levels of granularity. These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization, supporting dynamic analysis of model behavior. A user study with LLM practitioners and interviews with experts demonstrate the system\u2019s usability and effectiveness, emphasizing the importance of involving humans in the explanation process as active participants rather than passive recipients.","accessible_pdf":"Accessible","authors":[{"affiliation":"ETH Z\u00fcrich","email":"furui.cheng@inf.ethz.ch","name":"Furui Cheng"},{"affiliation":"ETH Zurich","email":"vzouhar@inf.ethz.ch","name":"Vil\u00e9m Zouhar"},{"affiliation":"ETH Z\u00fcrich","email":"robin.chan@inf.ethz.ch","name":"Robin Chan"},{"affiliation":"University of Konstanz","email":"daniel.fuerst@uni-konstanz.de","name":"Daniel F\u00fcrst"},{"affiliation":"IBM Research AI","email":"hendrik@strobelt.com","name":"Hendrik Strobelt"},{"affiliation":"ETH Z\u00fcrich","email":"melassady@ai.ethz.ch","name":"Mennatallah El-Assady"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"7a68a26e-4b20-444f-a76a-eee7a08ab3be","image_caption":"","keywords":["Counterfactual","Explainable Artificial Intelligence","Large Language Model","Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":"The paper contributes an interactive visualization system for analyzing large language model behaviors together with an algorithm for meaning textual counterfactual generation.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2405.00708","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1701","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full34","session_room":"Hall M2","session_room_id":"m2","session_title":"Vis & Language","session_uid":"7a68a26e-4b20-444f-a76a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Vis & Language"],"time_stamp":"2025-11-06T10:39:00.000Z","title":"Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JQEgOnSg18o"},{"UID":"7b762da7-ec92-461b-ab0a-95e7e278234d","abstract":"Aortic dissection is a life-threatening cardiovascular disease characterized by blood entering the media layer of the aortic vessel wall. This creates a second flow channel, known as the false lumen, which weakens the aortic wall and can potentially lead to fatal aortic rupture. Current risk stratification of aortic dissections is primarily based on morphological features of the aorta. However, hemodynamics also play a significant role in disease progression, though their investigation and visualization remain challenging. Common flow visualizations often experience visual clutter, especially when dealing with the intricate morphologies of aortic dissections. In this work, we implement and evaluate different approaches to visualizing the flow in aortic dissections effectively. We employ three techniques, namely streaklines with depth-dependent halos, transparent streaklines, and smoke surfaces. The latter is a technique based on streak surfaces, enhanced with opacity modulations, to produce a smoke-like appearance that improves visual clarity. We adapt the original opacity modulation of smoke surfaces to visualize flow even within the complex geometries of aortic dissections, thereby enhancing visual fidelity. To effectively capture dissection hemodynamics, we developed customized seeding structures that adapt to the shape of the surrounding lumen. Our evaluation, conducted via an online questionnaire, included medical professionals, fluid simulation experts, and visualization specialists. By analyzing results across these groups, we highlight differences in preference and interpretability, offering insight into domain-specific needs. No single visualization technique emerged as the best overall. Smoke surfaces provide the best overall clarity and visual realism. However, participants found streaklines with halos to be the best for quantifying flow, dispite them introducing significant visual clutter. Transparent streaklines serve as a middle ground, offering improved clarity over halos while maintaining some level of detail. Across all participant groups, smoke surfaces were rated as the most visually appealing and lifelike, with medical professionals highlighting their resemblance to contrast-agent injections used in clinical practice.","accessible_pdf":null,"authors":[{"affiliation":"Otto-von-Guericke-University","email":"aar.schr@gmail.com","name":"Aaron Schroeder"},{"affiliation":"Otto-von-Guericke University","email":"kai.ostendorf@st.ovgu.de","name":"Kai Ostendorf"},{"affiliation":"Stanford University School of Medicine","email":"baeumler@stanford.edu","name":"Kathrin Baeumler"},{"affiliation":"Stanford University","email":"mastro@stanford.edu","name":"Domenico Mastrodicasa"},{"affiliation":"Stanford University School of Medicine","email":"d.fleischmann@stanford.edu","name":"Dominik Fleischmann"},{"affiliation":"University of Magdeburg","email":"bernhard@isg.cs.uni-magdeburg.de","name":"Bernhard Preim"},{"affiliation":"University of Magdeburg","email":"theisel@ovgu.de","name":"Holger Theisel"},{"affiliation":"Stanford University School of Medicine","email":"gmistelb@stanford.edu","name":"Gabriel Mistelbauer"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"7b762da7-ec92-461b-ab0a-95e7e278234d","image_caption":"","keywords":["Medical Visualization","Flow Visualization","Hemodynamics","Aortic Dissection."],"open_access_supplemental_link":"https://github.com/aaschr/vis_2025_smoke_surfaces_for_AD","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1426","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full8","session_room":"Hall M1","session_room_id":"m1","session_title":"Biomedical Visualization","session_uid":"7b762da7-ec92-461b-ab0a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Biomedical Visualization"],"time_stamp":"2025-11-06T15:21:00.000Z","title":"Understanding Aortic Dissection Hemodynamics: Evaluating Adapted Smoke Surfaces Against Streakline-Based Techniques","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rgq3gdIQGxk"},{"UID":"7baf358e-7ab0-44c2-b0c3-dbab651a9268","abstract":"The fields of astronomy and astrophysics are evolving and incorporating technologies to effectively view and explore 3D data visualizations, including Augmented Reality (AR). An analysis of the feasibility of using AR in journal publications for 3D visualizations took place two years ago, in 2023, when Adams et al. evaluated whether the perceived workload between AR and non-AR technologies was comparable. Given that the use of AR for astronomy journal publications was, and still is, in its infancy, the original study had to utilize data intended for K-12 education that had similar interactions and data types as a proxy for real-world data that could be visualized in future astronomy publications. In this paper, we present the results of a conceptual replication study of Adams et al.\u2019s work to validate whether their findings hold with real astronomy stimuli. We found in our replication that many of the trends in the original study hold true, but that the workload experienced by participants was significantly higher under multiple conditions when using real-world data. Additionally, we found that the tradeoff between engagement and workload was as prevalent in the replication as it was in the original study. Our results provide a new framing for researchers to understand the tradeoffs of immersive visualization technologies and the increased workload of pairing these tools with complex, scientific stimuli. All Supplemental Material in our study is available at https://osf.io/j8urq/.","accessible_pdf":"Accessible","authors":[{"affiliation":"Northeastern University","email":"18mcreamer@gmail.com","name":"Mackenzie Michael Creamer"},{"affiliation":"Harvard-Smithsonian Center for Astrophysics","email":"jonathan.carifio@cfa.harvard.edu","name":"Jonathan Carifio"},{"affiliation":"Harvard-Smithsonian Center for Astrophysics","email":"agoodman@cfa.harvard.edu","name":"Alyssa Goodman"},{"affiliation":"Northeastern University","email":"m.borkin@neu.edu","name":"Michelle Borkin"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"7baf358e-7ab0-44c2-b0c3-dbab651a9268","image_caption":"","keywords":["Augmented Reality","Human Subjects-Empirical Study","Astronomy Visualization","Replication Study","AR-Enhanced Publications"],"open_access_supplemental_link":"https://osf.io/j8urq/files/osfstorage","open_access_supplemental_question":"Our study is a direct replication, accompanied by extensive supplemental materials that enhance transparency. These include a detailed comparison of methodological differences with the original study, fully documented source code for all analyses, and an additional experiment quantifying the impact of software variance.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/v6sfg_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1080","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"7baf358e-7ab0-44c2-b0c3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T14:45:00.000Z","title":"Validation through Replication of Augmented Reality as a Visualization Technique for Scholarly Publications in Astronomy","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"7c5cabc5-b76c-4eba-be3a-c6fcfd508888","abstract":"We contribute an autoethnographic reflection on the complexity of defining and measuring visualization literacy (i.e., the ability to interpret and construct visualizations) to expose our tacit thoughts that often exist in-between polished works and remain unreported in individual research papers. Our work is inspired by the growing number of empirical studies in visualization research that rely on visualization literacy as a basis for developing effective data representations or educational interventions. Researchers have already made various efforts to assess this construct, yet it is often hard to pinpoint either what we want to measure or what we are effectively measuring. In this autoethnography, we gather insights from 14 internal interviews with researchers who are users or designers of visualization literacy tests. We aim to identify what makes visualization literacy assessment a wicked problem. We further reflect on the fluidity of visualization literacy and discuss how this property may lead to misalignment between what the construct is and how measurements of it are used or designed. We also examine potential threats to measurement validity from conceptual, operational, and methodological perspectives. Based on our experiences and reflections, we propose several calls to action aimed at tackling the wicked problem of visualization literacy measurement, such as by broadening test scopes and modalities, improving test ecological validity, making it easier to use tests, seeking interdisciplinary collaboration, and drawing from continued dialogue on visualization literacy to expect and be more comfortable with its fluidity.","accessible_pdf":"Accessible","authors":[{"affiliation":"Northwestern University","email":"wanqian.ge@northwestern.edu","name":"Lily W. Ge"},{"affiliation":"Universit\u00e9 Paris-Saclay, Inria, Institut Polytechnique de Paris, CNRS, LISN, i3","email":"acabouat@gmail.com","name":"Anne-Flore Cabouat"},{"affiliation":"Worcester Polytechnic Institute","email":"kbonilla@wpi.edu","name":"Karen Bonilla"},{"affiliation":"Northwestern University","email":"yuancui2025@u.northwestern.edu","name":"Yuan Cui"},{"affiliation":"Worcester Polytechnic Institute","email":"yding5@wpi.edu","name":"Yiren Ding"},{"affiliation":"Worcester Polytechnic Institute","email":"ntrakotondravony@wpi.edu","name":"No\u00eblle Rakotondravony"},{"affiliation":"Northeastern University","email":"18mcreamer@gmail.com","name":"Mackenzie Creamer"},{"affiliation":"UC Santa Cruz","email":"jtotto@ucsc.edu","name":"Jasmine Otto"},{"affiliation":"Northwestern University","email":"maryam.hedayati@u.northwestern.edu","name":"Maryam Hedayati"},{"affiliation":"IBM Research","email":"bumchul.kwon@us.ibm.com","name":"Bum Chul Kwon"},{"affiliation":"Universit\u00e0 degli Studi di Brescia","email":"angela.locoro@unibs.it","name":"Angela Locoro"},{"affiliation":"Worcester Polytechnic Institute","email":"ltharrison@wpi.edu","name":"Lane Harrison"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"petra.isenberg@inria.fr","name":"Petra Isenberg"},{"affiliation":"Northeastern University","email":"m.correll@northeastern.edu","name":"Michael Correll"},{"affiliation":"Northwestern University","email":"matthew.kay@u.northwestern.edu","name":"Matthew Kay"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"7c5cabc5-b76c-4eba-be3a-c6fcfd508888","image_caption":"","keywords":["Visualization literacy","autoethnography","measurement","validity"],"open_access_supplemental_link":"https://osf.io/xwr4c/","open_access_supplemental_question":"We provided detailed supplemental materials for our autoethnography, including the interview protocol with the semi-structured questions and the full Miro board that we used during analysis.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/dfr4p_v2","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1406","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full35","session_room":"Room 0.11 + 0.12","session_room_id":"0_11_0_12","session_title":"Visualization Literacy","session_uid":"7c5cabc5-b76c-4eba-be3a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization Literacy"],"time_stamp":"2025-11-07T08:30:00.000Z","title":"An Autoethnography on Visualization Literacy: A Wicked Measurement Problem","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/R3RWFseiZmo"},{"UID":"7dce17af-ccb4-4f41-ab8a-f66a16030913","abstract":"We present \u201cDrawing in the Flow\u201d, a mixed-reality 3D user interface for authoring illustrative, multivariate 3D flow visualizations by sketching on, or better stated, in, 3D data. The approach interprets hand-drawn 3D strokes relative to an underlying data \u201ccanvas\u201d and applies animated \u201cink-data settling\u201d to ensure the strokes accurately reflect vector field data (i.e., 3D streamline paths). Color and other visual properties are interpreted relative to scalar data variables with \u201clazy data binding\u201d to help users prioritize creative visual design tasks. Results include example 3D illustrations of multiple flow fields. The work is significant because of the ability to make authoring accurate 3D mixed reality data visualizations accessible to stakeholders without programming or scripting experience and because it demonstrates a novel approach to balancing the tradeoff between accuracy and expression in 3D data visualization.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"sands224@umn.edu","name":"Walter Sands"},{"affiliation":"University of Minnesota","email":"dorr0024@umn.edu","name":"Sean Dorr"},{"affiliation":"University of Minnesota","email":"tran0563@umn.edu","name":"Kiet Tran"},{"affiliation":"University of Minnesota","email":"dfk@umn.edu","name":"Daniel Keefe"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"7dce17af-ccb4-4f41-ab8a-f66a16030913","image_caption":"","keywords":["Extended Reality","Sketching","Flow Visualization."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1202","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full37","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Rescheduled Papers","session_uid":"7dce17af-ccb4-4f41-ab8a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Rescheduled Papers"],"time_stamp":"2025-11-06T13:24:00.000Z","title":"Drawing in the Flow: A Data-Aware Mixed-Reality Sketching Interface for Illustrative 3D Flow Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"7ecd3f2d-f7b0-4b86-aa66-248cc17c0720","abstract":"Chart corpora, which comprise data visualizations and their semantic labels, are crucial for advancing visualization research. However, the labels in most existing corpora are high-level (e.g., chart types), hindering their utility for broader applications in the era of AI. In this paper, we contribute VISANATOMY, a corpus containing 942 real-world SVG charts produced by over 50 tools, encompassing 40 chart types and featuring structural and stylistic design variations. Each chart is augmented with multi-level fine-grained labels on its semantic components, including each graphical element\u2019s type, role, and position, hierarchical groupings of elements, group layouts, and visual encodings. In total, VISANATOMY provides labels for more than 383k graphical elements. We demonstrate the richness of the semantic labels by comparing VISANATOMY with existing corpora. We illustrate its usefulness through four applications: semantic role inference for SVG elements, chart semantic decomposition, chart type classification, and content navigation for accessibility. Finally, we discuss research opportunities to further improve VISANATOMY.","accessible_pdf":null,"authors":[{"affiliation":"University of Maryland","email":"cchen24@terpmail.umd.edu","name":"Chen Chen"},{"affiliation":"University of Maryland","email":"hbako@virginia.edu","name":"Hannah Bako"},{"affiliation":"University of Maryland","email":"peihong@umd.edu","name":"Peihong Yu"},{"affiliation":"University of Maryland","email":"hookerj100@gmail.com","name":"John Hooker"},{"affiliation":"University of Maryland","email":"jjoyal@terpmail.umd.edu","name":"Jeffrey Joyal"},{"affiliation":"University of Maryland","email":"wang.c.simon@gmail.com","name":"Simon Wang"},{"affiliation":"University of Maryland","email":"skim1270@terpmail.umd.edu","name":"Samuel Kim"},{"affiliation":"University of Maryland","email":"aprilwushuang@gmail.com","name":"Jessica Wu"},{"affiliation":"University of Maryland","email":"ading1@terpmail.umd.edu","name":"Aoxue Ding"},{"affiliation":"University of Maryland","email":"lsandeep@terpmail.umd.edu","name":"Lara Sandeep"},{"affiliation":"University of Maryland, College Park","email":"alexychen2002@gmail.com","name":"Alex Chen"},{"affiliation":"University of Maryland","email":"csinha@terpmail.umd.edu","name":"Chayanika Sinha"},{"affiliation":"University of Maryland","email":"leozcliu@umd.edu","name":"Zhicheng Liu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"7ecd3f2d-f7b0-4b86-aa66-248cc17c0720","image_caption":"","keywords":["Chart","SVG","data visualization","corpus","dataset","multilevel fine-grained semantic labels"],"open_access_supplemental_link":"https://osf.io/962xc/?view_only=adbb315fd8794f6dac6b9625d385900f","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2410.12268","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1411","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full5","session_room":"Hall M2","session_room_id":"m2","session_title":"Authoring and Design","session_uid":"7ecd3f2d-f7b0-4b86-aa66","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Authoring and Design"],"time_stamp":"2025-11-05T15:09:00.000Z","title":"VisAnatomy: An SVG Chart Corpus with Fine-Grained Semantic Labels","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/S3LGLxyblpM"},{"UID":"7eeae5e6-01ae-4570-90f6-069956401413","abstract":"Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, or computational linguistics. Previous works approach this problem from topic- and entity-based perspectives, but the capability of the underlying NLP model limits their effectiveness. Recent advances in prompting with LLMs present opportunities to enhance such approaches with higher accuracy and customizability. However, poorly designed prompts and visualizations could mislead users into falsely interpreting the visualizations and hinder the system's trustworthiness. In this paper, we address this issue by taking into account the user analysis tasks and visualization goals in the prompt-based data extraction stage, thereby extending the concept of Model Alignment. We present HINTs, a VA system for supporting sensemaking on large collections of documents, combining previous entity-based and topic-based approaches. The visualization pipeline of HINTs consists of three stages. First, entities and topics are extracted from the corpus with prompts. Then, the result is modeled as a hypergraph and hierarchically clustered. Finally, an enhanced space-filling curve layout is applied to visualize the hypergraph for interactive exploration. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate the sensemaking of interested documents. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are still necessary. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.","accessible_pdf":"No","authors":[{"email":null,"name":"Sam Yu-Te Lee"},{"email":null,"name":"Kwan-Liu Ma"}],"award":"","doi":"10.1109/TVCG.2024.3459961","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"7eeae5e6-01ae-4570-90f6-069956401413","image_caption":"","keywords":["Data visualization","Data mining","Intelligent agents","Visual analytics","Tag clouds","Pipelines","Fake news"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2403.02752","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3459961","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full1","session_room":"Hall E1","session_room_id":"e1","session_title":"Agentic Visualization and Intelligent Systems","session_uid":"7eeae5e6-01ae-4570-90f6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Agentic Visualization and Intelligent Systems"],"time_stamp":"2025-11-05T15:09:00.000Z","title":"HINTs: Sensemaking on large collections of documents with Hypergraph visualization and Intelligent agents","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/2z0_8lHbM04"},{"UID":"80f2d2e8-877c-469a-8ce2-bb3e5bea1f93","abstract":"In this work, we challenge the dominant use of logarithmic scales to communicate values spanning multiple orders of magnitude\u2014Orders of Magnitude Values (OMVs)\u2014to the general public. Focusing on bar charts, we incorporate cognitive insights into visualization design to better align with how humans perceive OMVs. Studies in cognitive psychology suggest that, for large numerical ranges such as millions and billions, people do not think logarithmically. Instead, they perceive numbers in a piecewise linear manner, grouping values into scale words (e.g., millions) and applying linear reasoning within each group. We build upon a recently introduced piecewise linear scale, EplusM, and validate its use in bar charts, which we refer to as EplusM bar charts. We also introduce two novel variants of the EplusM bar chart informed by findings in numerical perception: Bricks, which builds on the concepts of round numbers and subitizing, and Multi-Magnitude, which leverages categorical perception of large numbers. In a crowdsourced experiment, we evaluate four bar chart designs: 1) Log, 2) EplusM, 3) Bricks, and 4) Multi-Magnitude, across value retrieval and quantitative comparison tasks. Our results show that EplusM bar charts are significantly preferred over logarithmic designs, increase user confidence, and reduce perceived mental demand, while maintaining task performance. These findings suggest that EplusM bar charts can serve as effective alternatives to logarithmic ones when visualizing OMVs for general audiences.","accessible_pdf":"Accessible","authors":[{"affiliation":"Berger-Levrault","email":"kbatziakoudi@gmail.com","name":"Katerina Batziakoudi"},{"affiliation":"Berger-Levrault","email":"reystef@gmail.com","name":"St\u00e9phanie Rey"},{"affiliation":"Universit\u00e9 Paris-Saclay, CNRS, Inria, LISN","email":"jean-daniel.fekete@inria.fr","name":"Jean-Daniel Fekete"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"80f2d2e8-877c-469a-8ce2-bb3e5bea1f93","image_caption":"","keywords":["orders of magnitude","comparisons","exponent","mantissa","logarithmic scale","bar charts"],"open_access_supplemental_link":"https://osf.io/ hybvp/?view_only=5cd17943e9ba46deb66a8f7f4eeeb4da","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://hal.science/hal-05171203","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1497","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full25","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception","session_uid":"80f2d2e8-877c-469a-8ce2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception"],"time_stamp":"2025-11-06T13:12:00.000Z","title":"Beyond Log Scales: Toward Cognitively Informed Bar Charts for Orders of Magnitude Values","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/D_j0t7Fy0w0"},{"UID":"81f868a3-7d8d-405b-9b46-b8838a4fa991","abstract":"This article presents a visual analytics framework, idMotif, to support domain experts in identifying motifs in protein sequences. A motif is a short sequence of amino acids usually associated with distinct functions of a protein, and identifying similar motifs in protein sequences helps us to predict certain types of disease or infection. idMotif can be used to explore, analyze, and visualize such motifs in protein sequences. We introduce a deep-learning-based method for grouping protein sequences and allow users to discover motif candidates of protein groups based on local explanations of the decision of a deep-learning model. idMotif provides several interactive linked views for between and within protein cluster/group and sequence analysis. Through a case study and experts\u2019 feedback, we demonstrate how the framework helps domain experts analyze protein sequences and motif identification.","accessible_pdf":"No","authors":[{"email":null,"name":"Ji Hwan Park"},{"email":null,"name":"Vikash Prasad"},{"email":null,"name":"Sydney Newsom"},{"email":null,"name":"Fares Najar"},{"email":null,"name":"Rakhi Rajan"}],"award":"","doi":"10.1109/MCG.2023.3345742","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"81f868a3-7d8d-405b-9b46-b8838a4fa991","image_caption":"","keywords":["Proteins","Predictive models","Biological system modeling","Protein sequence","Amino acids","Transformers","Computational modeling","Visual analytics","Interactive systems"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2402.05953","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2023.3345742","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga2","session_room":"Hall E1","session_room_id":"e1","session_title":"Visual Analytics Methods, Tools, and Infrastructure","session_uid":"81f868a3-7d8d-405b-9b46","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visual Analytics Methods, Tools, and Infrastructure"],"time_stamp":"2025-11-06T10:39:00.000Z","title":"IdMotif: An Interactive Motif Identification in Protein Sequences","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JfetpDJuh5Q"},{"UID":"82b3ce83-b1a6-4d36-af15-47e7a8d42033","abstract":"We present VOICE, a novel approach to science communication that connects large language models\u2019 conversational capabilities with interactive exploratory visualization. VOICE introduces several innovative technical contributions that drive our conversational visualization framework. Based on the collected design requirements, we introduce a two-layer agent architecture that can perform task assignment, instruction extraction, and coherent content generation. We employ fine-tuning and prompt engineering techniques to tailor agents\u2019 performance to their specific roles and accurately respond to user queries. Our interactive text-to-visualization method generates a flythrough sequence matching the content explanation. In addition, natural language interaction provides capabilities to navigate and manipulate 3D models in real-time. The VOICE framework can receive arbitrary voice commands from the user and respond verbally, tightly coupled with a corresponding visual representation, with low latency and high accuracy. We demonstrate the effectiveness of our approach by implementing a proof-of-concept prototype and applying it to the molecular visualization domain: analyzing three 3D molecular models with multiscale and multi-instance attributes. Finally, we conduct a comprehensive evaluation of the system, including quantitative and qualitative analyses on our collected dataset, along with a detailed public user study and expert interviews. The results confirm that our framework and prototype effectively meet the design requirements and cater to the needs of diverse target users.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Donggang Jia"},{"email":null,"name":"Alexandra Irger"},{"email":null,"name":"Lonni Besan\u00e7on"},{"email":null,"name":"Ond\u0159ej Strnad"},{"email":null,"name":"Deng Luo"},{"email":null,"name":"Johanna Bj\u00f6rklund"},{"email":null,"name":"Alexandre Kouyoumdjian"},{"email":null,"name":"Anders Ynnerman"},{"email":null,"name":"Ivan Viola"}],"award":"","doi":"10.1109/TVCG.2025.3579956","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"82b3ce83-b1a6-4d36-af15-47e7a8d42033","image_caption":"","keywords":["Visualization","Data visualization","Oral communication","Biology","Biological system modeling","Three-dimensional displays","Solid modeling","Real-time systems","Prototypes","Interviews"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2304.04083","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3579956","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full1","session_room":"Hall E1","session_room_id":"e1","session_title":"Agentic Visualization and Intelligent Systems","session_uid":"82b3ce83-b1a6-4d36-af15","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Agentic Visualization and Intelligent Systems"],"time_stamp":"2025-11-05T15:45:00.000Z","title":"Voice: Visual oracle for interaction, conversation, and explanation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/2z0_8lHbM04"},{"UID":"8330bc4e-c9eb-41e0-8fa0-8043f0518241","abstract":"Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Notre Dame","email":"dan3@nd.edu","name":"Delin An"},{"affiliation":"University of Notre Dame","email":"pdu@nd.edu","name":"Pan Du"},{"affiliation":"Cornell University","email":"jw2837@cornell.edu","name":"Jian-Xun Wang"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8330bc4e-c9eb-41e0-8fa0-8043f0518241","image_caption":"","keywords":["Conditional diffusion model","volume-guided surface generation","multi-branch vessel modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"http://arxiv.org/abs/2507.13404","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1206","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full32","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Volumes & 3D","session_uid":"8330bc4e-c9eb-41e0-8fa0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Volumes & 3D"],"time_stamp":"2025-11-06T10:51:00.000Z","title":"AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"8347663a-7e88-43e2-a557-e7b2179fd4e5","abstract":"Visualizing multiple time series presents fundamental tradeoffs between scalability and visual clarity. Time series capture the behavior of many large-scale real-world processes, from stock market trends to urban activities. Users often gain insights by visualizing them as line charts, juxtaposing or superposing multiple time series to compare them and identify trends and patterns. However, existing representations struggle with scalability: when covering long time spans, leading to visual clutter from too many small multiples or overlapping lines. We propose TiVy, a new algorithm that summarizes time series using sequential patterns. It transforms the series into a set of symbolic sequences based on subsequence visual similarity using Dynamic Time Warping (DTW), then constructs a disjoint grouping of similar subsequences based on the frequent sequential patterns. The grouping result, a visual summary of time series, provides uncluttered superposition with fewer small multiples. Unlike common clustering techniques, TiVy extracts similar subsequences (of varying lengths) aligned in time. We also present an interactive time series visualization that renders large-scale time series in real-time. Our experimental evaluation shows that our algorithm (1) extracts clear and accurate patterns when visualizing  time series data, (2) achieves a significant speed-up (1000X) compared to a straightforward DTW clustering. We also demonstrate the efficiency of our approach to explore hidden structures in massive time series data in two usage scenarios.","accessible_pdf":null,"authors":[{"affiliation":"Adobe Research","email":"ychan@adobe.com","name":"Gromit Yeuk-Yin Chan"},{"affiliation":"University of Sao Paulo","email":"gnonato@icmc.usp.br","name":"Luis Gustavo Nonato"},{"affiliation":"Paris Descartes University","email":"themis@mi.parisdescartes.fr","name":"Themis Palpanas"},{"affiliation":"New York University","email":"csilva@nyu.edu","name":"Claudio Silva"},{"affiliation":"New York University","email":"juliana.freire@nyu.edu","name":"Juliana Freire"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8347663a-7e88-43e2-a557-e7b2179fd4e5","image_caption":"","keywords":["Time Series Visualization","Sub-sequence Clustering"],"open_access_supplemental_link":"https://github.com/GromitC/TiVy","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2507.18972","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1930","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full33","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Encoding & Comprehension","session_uid":"8347663a-7e88-43e2-a557","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Encoding & Comprehension"],"time_stamp":"2025-11-05T09:18:00.000Z","title":"TiVy: Time Series Visual Summary for Scalable Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"849ca631-398a-4087-9f66-a0837fc3af86","abstract":"Data-driven decision making has become a popular practice in science, industry, and public policy. Yet data alone, as an imperfect and partial representation of reality, is often insufficient to make good analysis decisions. Knowledge about the context of a dataset, its strengths and weaknesses, and its applicability for certain tasks is essential. Analysts are often not only familiar with the data itself, but also have data hunches about their analysis subject. In this work, we present an interview study with analysts from a wide range of domains and with varied expertise and experience, inquiring about the role of contextual knowledge. We provide insights into how data is insufficient in analysts' workflows and how they incorporate other sources of knowledge into their analysis. We analyzed how knowledge of data shaped their analysis outcome. Based on the results, we suggest design opportunities to better and more robustly consider both knowledge and data in analysis processes.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Utah","email":"haihan.lin@utah.edu","name":"Haihan Lin"},{"affiliation":"University of Utah","email":"mlisnic@wpi.edu","name":"Maxim Lisnic"},{"affiliation":"Link\u00f6ping University","email":"derya.akbaba@liu.se","name":"Derya Akbaba"},{"affiliation":"Link\u00f6ping University","email":"miriah.meyer@liu.se","name":"Miriah Meyer"},{"affiliation":"University of Utah","email":"alexander.lex@gmail.com","name":"Alexander Lex"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"849ca631-398a-4087-9f66-a0837fc3af86","image_caption":"","keywords":["Human-Subjects Qualitative Studies"],"open_access_supplemental_link":"https://osf.io/f89jp/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/dn32z_v2","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1785","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full4","session_room":"Hall E2","session_room_id":"e2","session_title":"Analytics & Reasoning","session_uid":"849ca631-398a-4087-9f66","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analytics & Reasoning"],"time_stamp":"2025-11-06T15:09:00.000Z","title":"Here's what you need to know about my data: Exploring Expert Knowledge's Role in Data Analysis","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/vzBq3yA2yUs"},{"UID":"84ab0ade-9529-4942-b5e1-ca953e56c880","abstract":"In this paper, we present a novel compression framework, TFZ, that preserves the topology of 2D symmetric and asymmetric second-order tensor fields defined on flat triangular meshes. A tensor field assigns a tensor\u2014a multi-dimensional array of numbers \u2014 to each point in space. Tensor fields, such as the stress and strain tensors, and the Riemann curvature tensor, are essential to both science and engineering. The topology of tensor fields captures the core structure of data, and is useful in various disciplines, such as graphics (for manipulating shapes and textures) and neuroscience (for analyzing brain structures from diffusion MRI). Lossy data compression may distort the topology of tensor fields, thus hindering downstream analysis and visualization tasks. TFZ ensures that certain topological features are preserved during lossy compression. Specifically, TFZ preserves degenerate points essential to the topology of symmetric tensor fields and retains eigenvector and eigenvalue graphs that represent the topology of asymmetric tensor fields. TFZ scans through each cell, preserving the local topology of each cell, and thereby ensuring certain global topological guarantees. We showcase the effectiveness of our framework in enhancing the lossy scientific data compressors SZ3 and SPERR.","accessible_pdf":null,"authors":[{"affiliation":"University of Utah","email":"u1472279@utah.edu","name":"Nathaniel Gorski"},{"affiliation":"University of Kentucky","email":"xliang@uky.edu","name":"Xin Liang"},{"affiliation":"The Ohio State University","email":"guo.2154@osu.edu","name":"Hanqi Guo"},{"affiliation":"University of Utah","email":"wang.bei@gmail.com","name":"Bei Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"84ab0ade-9529-4942-b5e1-ca953e56c880","image_caption":"","keywords":["Lossy compression","tensor fields","topology preservation","topological data analysis","topology in visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1929","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full14","session_room":"Hall M1","session_room_id":"m1","session_title":"Flow and Topology","session_uid":"84ab0ade-9529-4942-b5e1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Flow and Topology"],"time_stamp":"2025-11-05T15:21:00.000Z","title":"TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ar3ziIYBxhc"},{"UID":"84d64ebc-a5cc-4fd5-855f-ead6dc4ddfc0","abstract":"Communicating the complexity of oceanic phenomena\u2014such as hypoxia and acidification\u2014poses a persistent challenge for marine science. Despite advances in sensing technologies and computational models, conventional formats like static visualizations and text-based reports often fall short in conveying the dynamics of ocean changes. To address this gap, we present OceanVive, an immersive and interactive visualization system that transforms complex ocean datasets into navigable spatial narratives. OceanVive incorporates an exploratory panel on a table-sized tablet for managing immersive content on a large screen and integrates adaptive visual encodings, contextual storytelling, and intuitive navigation pathways to support effective communication. We validate the system through expert interviews, demonstrating its potential to enhance science communication and promote deeper public understanding.","accessible_pdf":"Accessible","authors":[{"affiliation":"ShanghaiTech University","email":"ouyy@shanghaitech.edu.cn","name":"Yang Ouyang"},{"affiliation":"ShanghaiTech University","email":"wuych3@shanghaitech.edu.cn","name":"Yuchen Wu"},{"affiliation":"ShanghaiTech University","email":"wangxy7@shanghaitech.edu.cn","name":"Xiyuan Wang"},{"affiliation":"ShanghaiTech University","email":"xielx@shanghaitech.edu.cn","name":"Laixin Xie"},{"affiliation":"The Hong Kong University of Science and Technology","email":"chengwc@ust.hk","name":"Weicong Cheng"},{"affiliation":"The Hong Kong University of Science and Technology","email":"magan@ust.hk","name":"Jianping Gan"},{"affiliation":"ShanghaiTech University","email":"liquan@shanghaitech.edu.cn","name":"Quan Li"},{"affiliation":"Hong Kong University of Science and Technology","email":"mxj@cse.ust.hk","name":"Xiaojuan Ma"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"84d64ebc-a5cc-4fd5-855f-ead6dc4ddfc0","image_caption":"","keywords":["Immersive visualization","Communication"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1076","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"84d64ebc-a5cc-4fd5-855f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T14:54:00.000Z","title":"OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"84ef8d69-bdba-43e8-a078-ae651a3de164","abstract":"Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and a user study demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.","accessible_pdf":null,"authors":[{"affiliation":"East China Normal University","email":"51275902056@stu.ecnu.edu.cn","name":"Nan Xiang"},{"affiliation":"East China Normal University","email":"51215901019@stu.ecnu.edu.cn","name":"Tianyi Liang"},{"affiliation":"East China Normal University","email":"hwhuang@stu.ecnu.edu.cn","name":"Haiwen Huang"},{"affiliation":"East China Normal University","email":"52265901032@stu.ecnu.edu.cn","name":"Shiqi Jiang"},{"affiliation":"School of Computer Science and Technology","email":"2690210766@qq.com","name":"Hao Huang"},{"affiliation":"East China Normal University","email":"yifeihuang17@gmail.com","name":"Yifei Huang"},{"affiliation":"East China Normal University","email":"lychen@sei.ecnu.edu.cn","name":"Liangyu Chen"},{"affiliation":"School of Computer Science and Technology","email":"cbwang@cs.ecnu.edu.cn","name":"Changbo Wang"},{"affiliation":"East China Normal University","email":"chli@cs.ecnu.edu.cn","name":"Chenhui Li"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"84ef8d69-bdba-43e8-a078-ae651a3de164","image_caption":"","keywords":["Prompt engineering","text-to-3D generation","shape exploration","visualization design","visual perception"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.00428","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1770","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full28","session_room":"Hall E1","session_room_id":"e1","session_title":"The VIS in GenAI","session_uid":"84ef8d69-bdba-43e8-a078","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The VIS in GenAI"],"time_stamp":"2025-11-05T13:48:00.000Z","title":"Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zLchh59XyM4"},{"UID":"851f1b42-91da-4dcc-a08e-c6d8e564d251","abstract":"Interpreting data visualizations is an essential skill in today\u2019s education, yet students often struggle with understanding unfamiliar formats. This study investigates how four learning materials \u2013 textbook, comic, video, and game \u2013 affect middle- and high school students\u2019 ability to interpret line charts, area charts, stacked area charts, and stream graphs. We conducted a comparative classroom study with 68 students, using pre- and post-tests, worksheet activities, and group discussions to assess learning outcomes and understanding. Our results show statistically significant improvement in students\u2019 understanding of stacked area charts and streamgraphs, while no significant differences between the learning materials were found. This suggests that more factors than initially anticipated \u2013 such as engagement, motivation and active learning strategies \u2013 influence the learning outcome. The analysis of the worksheets revealed that while students could infer surface-level insights from charts, over 70% struggled to identify underlying patterns or relationships. Additionally, a common challenge across all learning materials was reading fatigue, which often led students to skim content, disengage, or misinterpret key information. These findings highlight the need for educational tools and approaches that foster deeper understanding of unfamiliar visualizations, reduce cognitive load, and encourage active engagement.","accessible_pdf":"Accessible","authors":[{"affiliation":"St. P\u00f6lten University of Applied Sciences","email":"magdalena.boucher@fhstp.ac.at","name":"Magdalena Boucher"},{"affiliation":"Masaryk University","email":"makej@mail.muni.cz","name":"Magdalena Kejstova"},{"affiliation":"St. Poelten University of Applied Sciences","email":"christina.stoiber@fhstp.ac.at","name":"Christina Stoiber"},{"affiliation":"Austrian Computer Society","email":"martin.kandlhofer@ocg.at","name":"Martin Kandlhofer"},{"affiliation":"Austrian Computer Society","email":"leniproduction@gmail.com","name":"Alena Boucher"},{"affiliation":"Masaryk University","email":"simone.kriglstein@univie.ac.at","name":"Simone Kriglstein"},{"affiliation":"Erich Fried Realgymnasium","email":"shelley.buchinger@brg9.at","name":"Shelley Buchinger"},{"affiliation":"St. Poelten University of Applied Sciences","email":"wolfgang.aigner@fhstp.ac.at","name":"Wolfgang Aigner"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"851f1b42-91da-4dcc-a08e-c6d8e564d251","image_caption":"","keywords":["visualization education","schools","learning materials","visualization literacy"],"open_access_supplemental_link":"https://phaidra.fhstp.ac.at/detail/o:7304","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://phaidra.fhstp.ac.at/detail/o:7302","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1863","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full35","session_room":"Room 0.11 + 0.12","session_room_id":"0_11_0_12","session_title":"Visualization Literacy","session_uid":"851f1b42-91da-4dcc-a08e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization Literacy"],"time_stamp":"2025-11-07T08:54:00.000Z","title":"Enhancing Data Visualization Literacy: A Comparative Study of Learning Materials in Schools","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/R3RWFseiZmo"},{"UID":"8573acd0-2f1d-4f77-91b5-273ee5567696","abstract":"When analyzing heterogeneous data comprising numerical and categorical attributes, it is common to treat the different data types separately or transform the categorical attributes to numerical ones. The transformation has the advantage of facilitating an integrated multi-variate analysis of all attributes. We propose a novel technique for transforming categorical data into interpretable numerical feature vectors using Large Language Models (LLMs). The LLMs are used to identify the categorical attributes\u2019 main characteristics and assign numerical values to these characteristics, thus generating a multi-dimensional feature vector. The transformation can be computed fully automatically, but due to the interpretability of the characteristics, it can also be adjusted intuitively by an end user. We provide a respective interactive tool that aims to validate and possibly improve the AI-generated outputs. Having transformed a categorical attribute, we propose novel methods for ordering and color-coding the categories based on the similarities of the feature vectors.","accessible_pdf":"No","authors":[{"email":null,"name":"Karim Huesmann"},{"email":null,"name":"Lars Linsen"}],"award":"","doi":"10.1109/TVCG.2024.3460652","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"8573acd0-2f1d-4f77-91b5-273ee5567696","image_caption":"","keywords":["Vectors","Data visualization","Frequency measurement","Encoding","Image color analysis","Automobiles","Large language models","Data models","Semantics","Computational modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024.3460652 ]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full4","session_room":"Hall E2","session_room_id":"e2","session_title":"Analytics & Reasoning","session_uid":"8573acd0-2f1d-4f77-91b5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analytics & Reasoning"],"time_stamp":"2025-11-06T15:45:00.000Z","title":"Large Language Models for Transforming Categorical Data to Interpretable Feature Vectors","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/vzBq3yA2yUs"},{"UID":"86b81feb-984c-4b76-bda1-202a9c48c25c","abstract":"The emergence of distinct machine learning explanation methods has leveraged a number of new issues to be investigated. The disagreement problem is one such issue, as there may be scenarios where the output of different explanation methods disagree with each other. Although understanding how often, when, and where explanation methods agree or disagree is important to increase confidence in the explanations, few works have been dedicated to investigating such a problem. In this work, we proposed Visagreement, a visualization tool designed to assist practitioners in investigating the disagreement problem. Visagreement builds upon metrics to quantitatively compare and evaluate explanations, enabling visual resources to uncover where and why methods mostly agree or disagree. The tool is tailored for tabular data with binary classification and focuses on local feature importance methods. In the provided use cases, Visagreement turned out to be effective in revealing, among other phenomena, how disagreements relate to the quality of the explanations and machine learning model accuracy, thus assisting users in deciding where and when to trust explanations. To assess the effectiveness and practical utility of Visagreement, we conducted an evaluation involving four experts. These experts assessed the tool's Effectiveness, Usability, and Impact on Decision-Making. The experts confirm the Visagreement tool's effectiveness and user-friendliness, making it a valuable asset for analyzing and exploring (dis)agreements.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Priscylla Silva"},{"email":null,"name":"Vitoria Guardieiro"},{"email":null,"name":"Brian Barr"},{"email":null,"name":"Claudio Silva"},{"email":null,"name":"Luis Gustavo Nonato"}],"award":"","doi":"10.1109/TVCG.2025.3558074","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"86b81feb-984c-4b76-bda1-202a9c48c25c","image_caption":"","keywords":["Measurement","Machine learning","Data visualization","Accuracy","Extraterrestrial measurements","Analytical models","Data models","Visual analytics","Training","Space exploration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://www.techrxiv.org/doi/full/10.36227/techrxiv.173386520.03447635/v2","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-12-1092.","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full12","session_room":"Hall E1","session_room_id":"e1","session_title":"Explanation, Exploration, and Model Configuration","session_uid":"86b81feb-984c-4b76-bda1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explanation, Exploration, and Model Configuration"],"time_stamp":"2025-11-06T09:30:00.000Z","title":"Visagreement: Visualizing and Exploring Explanations (Dis)Agreement","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/h8CsAxh4LWg"},{"UID":"86dbd3cf-0467-4e0e-8612-b70910cdefbc","abstract":"Augmented reality (AR) has increasingly been used to communicate environmental impacts, offering greater engagement than conventional displays. However, its effect on message credibility\u2014how much people believe in the content of the communication\u2014remains unclear. In a preregistered study, we compared the perceived credibility of environmental information presented via visualizations on an AR headset or a desktop display. We created display-specific visual encodings (3D concrete for AR, 2D bar charts for desktop) and added two control conditions to cross display and encoding. We found no difference in message credibility between AR and desktop, though concrete AR was rated most engaging. Supplementary material is available at https://osf.io/n4p5c/.","accessible_pdf":null,"authors":[{"affiliation":"Universit\u00e9 de Bordeaux","email":"aymeric.ferron@inria.fr","name":"Aymeric Ferron"},{"affiliation":"Inria, CNRS, LISN","email":"ambre.assor@inria.fr","name":"Ambre Assor"},{"affiliation":"Inria, CNRS, Univ. Bordeaux","email":"pierre.dragice@gmail.com","name":"Pierre Dragicevic"},{"affiliation":"CNRS, Inria, Univ. Bordeaux, LaBRI","email":"yvonne.jansen@cnrs.fr","name":"Yvonne Jansen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"86dbd3cf-0467-4e0e-8612-b70910cdefbc","image_caption":"","keywords":["Augmented Reality","Data Visualization","Credibility","Comparative Study","Sustainable HCI"],"open_access_supplemental_link":"https://osf.io/n4p5c/files/osfstorage","open_access_supplemental_question":"We conducted a pre-registered study: https://osf.io/3djhs\nAll data and analyses scripts are available on OSF.\nThe code is publicly available on our Gitlab: https://gitlab.inria.fr/aferron/ar-credibility\nWe provide extra documentation on our OSF repository, including videos, questionnaires and the applications we tested: https://osf.io/n4p5c/","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://hal.science/hal-05200516/document","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1257","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full18","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive & Ubiquitous Analytics","session_uid":"86dbd3cf-0467-4e0e-8612","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive & Ubiquitous Analytics"],"time_stamp":"2025-11-06T13:12:00.000Z","title":"Investigating the Effects of Augmented Reality on Message Credibility When Visualizing Environmental Impacts","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/6hTTrGMsPK8"},{"UID":"87a90010-17cf-4a03-a81f-38b56bb28d64","abstract":"Data slice finding is an emerging technique for validating machine learning (ML) models by identifying and analyzing subgroups in a dataset that exhibit poor performance, often characterized by distinct feature sets or descriptive metadata. However, in the context of validating vision models involving unstructured image data, this approach faces significant challenges, including the laborious and costly requirement for additional metadata and the complex task of interpreting the root causes of underperformance. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for metadata-free data slice finding. Our system identifies interpretable data slices that involve common model behaviors and visualizes these patterns through an Attribution Mosaic design. Our interactive interface provides straightforward guidance for users to detect, interpret, and annotate predominant model issues, such as spurious correlations (model biases) and mislabeled data, with minimal effort. Additionally, it employs a cutting-edge model regularization technique to mitigate the detected issues and enhance the model\u2019s performance. The efficacy of AttributionScanner is demonstrated through use cases involving two benchmark datasets, with qualitative and quantitative evaluations showcasing its substantial effectiveness in vision model validation, ultimately leading to more reliable and accurate models.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Xiwei Xuan"},{"email":null,"name":"Jorge Piazentin Ono"},{"email":null,"name":"Liang Gou"},{"email":null,"name":"Kwan-Liu Ma"},{"email":null,"name":"Liu Ren"}],"award":"","doi":"10.1109/TVCG.2025.3546644","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"87a90010-17cf-4a03-a81f-38b56bb28d64","image_caption":"","keywords":["Computational modeling","Analytical models","Data visualization","Correlation","Accuracy","Visual analytics","Annotations","Image color analysis","Vectors","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2401.06462v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-05-0337]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full12","session_room":"Hall E1","session_room_id":"e1","session_title":"Explanation, Exploration, and Model Configuration","session_uid":"87a90010-17cf-4a03-a81f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explanation, Exploration, and Model Configuration"],"time_stamp":"2025-11-06T09:18:00.000Z","title":"AttributionScanner: A Visual Analytics System for Model Validation with Metadata-Free Slice Finding","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/h8CsAxh4LWg"},{"UID":"880a666a-57b0-4876-bf2a-c16f6b787ff7","abstract":"Visual Analytics (VA) researchers frequently collaborate closely with domain experts to derive requirements and select appropriate solutions to fulfill these requirements. Despite strides made in exploring requirement and solution spaces, challenges persist due to the absence of guidance in the initial consideration space and the lack of shared problem-solving knowledge, often resulting in suboptimal solutions. To address these issues, we conducted an empirical study of VA research, with a focus on mapping the relations between requirement and solution spaces. Analyzing 220 VA papers, we formulate refined topologies for data, requirements, and solutions. We propose conceptualizing the connections between requirements, data, and solutions through knowledge graphs and utilizing solution paths to encapsulate fundamental problem-solving knowledge in visual analytics research. Through the integration of solution paths into a graph and analyzing their interconnections, we identified a subset of problem-driven design patterns that demonstrated the efficacy of our approach. By externalizing problem-solving knowledge and formulating problem-driven design patterns, our aim is to streamline the exploration of consideration space, facilitating the inclusion of \u201cgood\u201d solutions, and establish a benchmark for shared design decisions among researchers and readers.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yuchen Wu"},{"email":null,"name":"Shenghan Gao"},{"email":null,"name":"Shizhen Zhang"},{"email":null,"name":"Xiaofeng Dou"},{"email":null,"name":"Xingbo Wang"},{"email":null,"name":"Quan Li"}],"award":"","doi":"10.1109/TVCG.2025.3538768","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"880a666a-57b0-4876-bf2a-c16f6b787ff7","image_caption":"","keywords":["Problem-solving","Data visualization","Data models","Guidelines","Decision making","Terminology","Encoding","Computational modeling","Visual analytics","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":"https://upddp.github.io/#\uff1a This website presents our study on problem-driven design patterns in visual analytics. Drawing inspiration from the idea of formalizing best practices to solve common problems when designing systems in software engineering domain, we aim to aggregate scattered problem-solving knowledge in VA research for both researchers and readers to consider.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0628.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full4","session_room":"Hall E2","session_room_id":"e2","session_title":"Analytics & Reasoning","session_uid":"880a666a-57b0-4876-bf2a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analytics & Reasoning"],"time_stamp":"2025-11-06T15:33:00.000Z","title":"From Requirement to Solution: Unveiling Problem-Driven Design Patterns in Visual Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/vzBq3yA2yUs"},{"UID":"881a464d-c01d-453c-8f14-8b59acdde104","abstract":"While powerful and well-established, tools like ParaView present a steep learning curve that can discourage many potential users. This work introduces ParaView-MCP, an autonomous agent that integrates modern multimodal large language models (MLLMs) with ParaView to not only lower the barrier to entry but also augment ParaView with intelligent decision support. By leveraging the state-of-the-art reasoning, command execution, and vision capabilities of MLLMs, ParaView-MCP enables users to interact with ParaView through natural language and visual inputs. Specifically, our system adopted the Model Context Protocol (MCP), a standardized interface for model-application communication, which facilitates direct interaction between MLLMs and ParaView's Python API, allowing seamless information exchange between the user, the language model, and the visualization tool itself. Furthermore, by implementing a visual feedback mechanism that allows the agent to observe the viewport, we unlock a range of new capabilities, including recreating visualizations from examples, closed-loop visualization parameter updates based on user-defined goals, and even cross-application collaboration involving multiple tools.","accessible_pdf":null,"authors":[{"affiliation":"Lawrence Livermore National Laboratory","email":"shusen.liu.hust@gmail.com","name":"Shusen Liu"},{"affiliation":"Lawrence Livermore National Laboratory","email":"miao1@llnl.gov","name":"Haichao Miao"},{"affiliation":"Lawrence Livermore National Laboratory","email":"bremer5@llnl.gov","name":"Peer-Timo Bremer"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"881a464d-c01d-453c-8f14-8b59acdde104","image_caption":"","keywords":["Agent","Tool Use","Model Context Protocol"],"open_access_supplemental_link":"https://github.com/LLNL/paraview_mcp","open_access_supplemental_question":"Well documented source code","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2505.07064","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1100","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"881a464d-c01d-453c-8f14","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T10:15:00.000Z","title":"Paraview-MCP: An Autonomous Visualization Agent with Direct Tool Use","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"88a99aaa-b89c-494c-b810-2113f43914ab","abstract":"The dissemination of visualizations is primarily in the form of raster images, which often results in the loss of critical information such as source code, interactive features, and metadata. While previous methods have proposed embedding metadata into images to facilitate Visualization Image Data Retrieval (VIDR), most existing methods lack practicability since they are fragile to common image tampering during online distribution such as cropping and editing. To address this issue, we propose VisGuard, a tamper-resistant VIDR framework that reliably embeds metadata link into visualization images. The embedded data link remains recoverable even after substantial tampering upon images. We propose several techniques to enhance robustness, including repetitive data tiling, invertible information broadcasting, and an anchor-based scheme for crop localization. VisGuard enables various applications, including interactive chart reconstruction, tampering detection, and copyright protection. We conduct comprehensive experiments on VisGuard's superior performance in data retrieval accuracy, embedding capacity, and security against tampering and steganalysis, demonstrating VisGuard's competence in facilitating and safeguarding visualization dissemination and information conveyance.","accessible_pdf":null,"authors":[{"affiliation":"East China Normal University","email":"huayuan221@gmail.com","name":"Huayuan Ye"},{"affiliation":"East China Normal University","email":"jtchen@stu.ecnu.edu.cn","name":"Juntong Chen"},{"affiliation":"East China Normal University","email":"10195102459@stu.ecnu.edu.cn","name":"Shenzhuo Zhang"},{"affiliation":"Tsinghua University","email":"yp-zhang22@mails.tsinghua.edu.cn","name":"Yipeng Zhang"},{"affiliation":"School of Computer Science and Technology","email":"cbwang@cs.ecnu.edu.cn","name":"Changbo Wang"},{"affiliation":"East China Normal University","email":"chli@cs.ecnu.edu.cn","name":"Chenhui Li"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"88a99aaa-b89c-494c-b810-2113f43914ab","image_caption":"","keywords":["Visualization image data retrieval","image steganography","tampering resistance","tampering detection."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2507.14459","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1083","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full37","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Rescheduled Papers","session_uid":"88a99aaa-b89c-494c-b810","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Rescheduled Papers"],"time_stamp":"2025-11-06T13:00:00.000Z","title":"VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"894782c9-d5b8-4e59-959b-5f71ea2ff688","abstract":"Implicit Neural Representations (INRs) are widely used to encode data as continuous functions, enabling the visualization of large scale multivariate scientific simulation data with reduced memory usage. However, existing INR-based methods face three main limitations: (1) inflexible representation of complex structures, (2) primarily focusing on single-variable data, and (3) dependence on structured grids. Thus, their performance degrades when applied to complex real-world datasets. To address these limitations, we propose a novel neural network-based framework, MC-INR, which handles multivariate data on unstructured grids. It combines meta-learning and clustering to enable flexible encoding of complex structures. To further improve performance, we introduce a residual-based dynamic re-clustering mechanism that adaptively partitions clusters based on local error. We also propose a branched layer to leverage multivariate data through independent branches simultaneously. Experimental results demonstrate that MC-INR outperforms existing methods on scientific data encoding tasks.","accessible_pdf":"Accessible","authors":[{"affiliation":"Korea University","email":"ehsson@korea.ac.kr","name":"Hyunsoo Son"},{"affiliation":"Korea University","email":"wjdgus0967@korea.ac.kr","name":"Jeonghyun Noh"},{"affiliation":"Korea University","email":"orangeblush@korea.ac.kr","name":"Suemin Jeon"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"},{"affiliation":"Korea University","email":"wkjeong@korea.ac.kr","name":"Won-Ki Jeong"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"894782c9-d5b8-4e59-959b-5f71ea2ff688","image_caption":"","keywords":["Implicit neural representation","meta-learning","clustering","multivariate data encoding"],"open_access_supplemental_link":null,"open_access_supplemental_question":"superior performance on the unstructured grids datasets","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1115","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"894782c9-d5b8-4e59-959b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T14:03:00.000Z","title":"MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data  using Meta-Learning and Clustered Implicit Neural Representations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"89bf8d86-33c7-4bf0-b790-98bf9272ee2e","abstract":"Hagrid is a state-of-the-art space-filling-curve-based method for gridifying scatterplots. However, it exhibits limitations in preserving the global structures of scatterplots with areas of varying density due to the incompatibility of adapting the granularity level of the underlying space-filling curve to regions with different densities. To compensate for this shortcoming, we introduce SiGrid that combines Hagrid with the Sector-Based Regularization (SBR) technique. SiGrid applies SBR to generate a scatterplot with a more uniform and generally lower density as an intermediate step. This intermediate scatterplot can then be fed to Hagrid for improved results. We quantitatively evaluate SiGrid by comparing it to Hagrid over a set of 502 scatterplots of different sizes, ranging from 50 to 10000 points per dataset, using relevant quality metrics. While generally slower, the results demonstrate that SiGrid outperforms Hagrid regarding the quality metrics of rank-wise neighborhood preservation (trustworthiness), ordering preservation, and pairwise distance preservation (cross-correlation).","accessible_pdf":null,"authors":[{"affiliation":"University of Stuttgart","email":"rene.cutura@visus.uni-stuttgart.de","name":"Rene Cutura"},{"affiliation":"University of M\u00fcnster","email":"hennes.rave@uni-muenster.de","name":"Hennes Rave"},{"affiliation":"University of Stuttgart","email":"quynh.ngo@visus.uni-stuttgart.de","name":"Quynh Ngo"},{"affiliation":"University of M\u00fcnster","email":"molchano@uni-muenster.de","name":"Vladimir Molchanov"},{"affiliation":"University of M\u00fcnster","email":"linsen@uni-muenster.de","name":"Lars Linsen"},{"affiliation":"University of Stuttgart","email":"weiskopf@visus.uni-stuttgart.de","name":"Daniel Weiskopf"},{"affiliation":"University of Stuttgart","email":"michael.sedlmair@visus.uni-stuttgart.de","name":"Michael Sedlmair"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"89bf8d86-33c7-4bf0-b790-98bf9272ee2e","image_caption":"","keywords":["Scatterplot","Space-filling curve","Grid layout","Neighborhood preservation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1261","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"89bf8d86-33c7-4bf0-b790","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T11:09:00.000Z","title":"SiGrid: Gridifying Scatterplots with Sector-Based Regularization and Hagrid","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"8a0db17e-c752-4f4d-a713-082d59c2d1f7","abstract":"This paper investigates why recent generative AI models outperform humans in data visualization knowledge tasks. Through systematic comparative analysis of responses to visualization questions, we find that differences exist between two ChatGPT models and human outputs over rhetorical structure, knowledge breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more advanced model, displays a hybrid of characteristics from both humans and ChatGPT-3.5. The two models were generally favored over human responses, while their strengths in coverage and breadth, and emphasis on technical and task-oriented visualization feedback collectively shaped higher overall quality. Based on our findings, we draw implications for advancing user experiences based on the potential of LLMs and human perception over their capabilities, with relevance to broader applications of AI.","accessible_pdf":"Accessible","authors":[{"affiliation":"Boston College","email":"yongsu.ahn@pitt.edu","name":"Yongsu Ahn"},{"affiliation":"Boston College","email":"nam.wook.kim@bc.edu","name":"Nam Wook Kim"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8a0db17e-c752-4f4d-a713-082d59c2d1f7","image_caption":"","keywords":["Generative AI","LLM","Visualization","Question and answering","ChatGPT"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1360","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short12","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Viz for AI & AI for Viz","session_uid":"8a0db17e-c752-4f4d-a713","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Viz for AI & AI for Viz"],"time_stamp":"2025-11-05T11:00:00.000Z","title":"Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"8a14c17e-0a05-4437-9f34-b5b570a00445","abstract":"Volume visualization plays a significant role in revealing important intrinsic patterns of 3D scientific datasets. However, these datasets are often large, making it challenging for interactive visualization systems to deliver a seamless user experience because of high input latency that arises from I/O bottlenecks and limited fast memory resources with high miss rates. To address this issue, we have proposed a deep learning-based prefetching method called RmdnCache, which optimizes the data flow across the memory hierarchy to reduce the input latency of large-scale volume visualization. Our approach accurately prefetches the content of the next view to fast memory using learning-based prediction while rendering the current view. The proposed deep learning architecture consists of two networks, RNN and MDN in respective spaces, which work together to predict both the location and likelihood distribution of the next view for defining an optimal prefetching range. Our method outperforms existing state-of-the-art prefetching algorithms in reducing overall input latency for visualizing real-world large-scale volumetric datasets.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Jianxin Sun"},{"email":null,"name":"Xinyan Xie"},{"email":null,"name":"Hongfeng Yu"}],"award":"","doi":"10.1109/TVCG.2024.3410091","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"8a14c17e-0a05-4437-9f34-b5b570a00445","image_caption":"","keywords":["Prefetching","Data visualization","Rendering (computer graphics)","Three-dimensional displays","Training","Data models","Deep learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://par.nsf.gov/servlets/purl/10539350","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG-2023-09-0524]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full36","session_room":"Hall M1","session_room_id":"m1","session_title":"Volume","session_uid":"8a14c17e-0a05-4437-9f34","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Volume"],"time_stamp":"2025-11-05T13:36:00.000Z","title":"RmdnCache: Dual-Space Prefetching Neural Network for Large-Scale Volume Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-M293vb-AIg"},{"UID":"8aa31c36-374d-4155-b9c3-fd20d8deeec2","abstract":"Understanding how people perceive visualizations is crucial for designing effective visual data representations; however, many heuristic design guidelines are derived from specific tasks or visualization types, without considering the constraints or conditions under which those guidelines hold. In this work, we aimed to assess existing design heuristics for categorical visualization using well-established psychological knowledge. Specifically, we examine the impact of the subitizing phenomenon in cognitive psychology\u2014people\u2019s ability to automatically recognize a small set of objects instantly without counting\u2014in data visualizations. We conducted three experiments with multi-class scatterplots\u2014between 2 and 15 classes with varying design choices\u2014across three different tasks\u2014class estimation, correlation comparison, and clustering judgments\u2014to understand how performance changes as the number of classes (and therefore set size) increases. Our results indicate if the category number is smaller than six, people tend to perform well at all tasks, providing empirical evidence of subitizing in visualization. When category numbers increased, performance fell, with the magnitude of the performance change depending on task and encoding. Our study bridges the gap between heuristic guidelines and empirical evidence by applying well-established psychological theories, suggesting future opportunities for using psychological theories and constructs to characterize visualization perception.","accessible_pdf":null,"authors":[{"affiliation":"University of North Carolina-Chapel Hill","email":"zeyuwang@cs.unc.edu","name":"Arran Zeyu Wang"},{"affiliation":"University of Oklahoma","email":"quadri@ou.edu","name":"Ghulam Jilani Quadri"},{"affiliation":"The University of North Carolina at Chapel Hill","email":"gisellez@ad.unc.edu","name":"Mengyuan Zhu"},{"affiliation":"University of North Carolina-Chapel Hill","email":"chint@cs.unc.edu","name":"Chin Tseng"},{"affiliation":"University of North Carolina-Chapel Hill","email":"danielle.szafir@cs.unc.edu","name":"Danielle Szafir"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8aa31c36-374d-4155-b9c3-fd20d8deeec2","image_caption":"","keywords":["Visualization Perception","Psychology","Subitizing","Fechner\u2019s Law","Dual-System Theory","Categorical Data","Color","Shape"],"open_access_supplemental_link":"https://osf.io/y3z2b/?view_only=7f6569187b344fadbd11cc09a6e63d24","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1566","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full25","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception","session_uid":"8aa31c36-374d-4155-b9c3","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception"],"time_stamp":"2025-11-06T13:24:00.000Z","title":"Characterizing Visualization Perception with Psychological Phenomena: Uncovering the Role of Subitizing in Data Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/D_j0t7Fy0w0"},{"UID":"8ac3fc70-e73e-4ea1-8eb2-0ce94992c6bc","abstract":"Feature level sets (FLS) have shown significant potential in the analysis of multi-field data by using traits defined in attribute space to specify features in the domain. In this work, we address key challenges in the practical use of FLS: trait design and feature selection for rendering. To simplify trait design, we propose a Cartesian decomposition of traits into simpler components, making the process more intuitive and computationally efficient. Additionally, we utilize dictionary learning results to automatically suggest point traits. To enhance feature selection, we introduce trait-induced merge trees (TIMTs), a generalization of merge trees for feature level sets, aimed at topologically analyzing tensor fields or general multi-variate data. The leaves in the TIMT represent areas in the input data that are closest to the defined trait, thereby most closely resembling the defined feature. This merge tree provides a hierarchy of features, enabling the querying of the most relevant and persistent features. Our method includes various query techniques for the tree, allowing the highlighting of different aspects. We demonstrate the cross-application capabilities of this approach through five case studies from different domains.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Danhua Lei"},{"email":null,"name":"Jochen Jankowai"},{"email":null,"name":"Petar Hristov"},{"email":null,"name":"Hamish Carr"},{"email":null,"name":"Leif Denby"},{"email":null,"name":"Talha Bin Masood"},{"email":null,"name":"Ingrid Hotz"}],"award":"","doi":"10.1109/tvcg.2025.3525974","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"8ac3fc70-e73e-4ea1-8eb2-0ce94992c6bc","image_caption":"","keywords":["Level set","Rendering (computer graphics)","Feature extraction","Vegetation","Isosurfaces","Optical fiber testing","Geometry","Vectors","Transfer functions","Surface treatment"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2501.06238","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0615]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full23","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Of Trees and Treemaps","session_uid":"8ac3fc70-e73e-4ea1-8eb2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Of Trees and Treemaps"],"time_stamp":"2025-11-05T10:39:00.000Z","title":"Multi-field Visualization: Trait design and trait-induced merge trees","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/yiiG1AZHJ1E"},{"UID":"8b2cd3c8-c4e3-483a-a519-bbb3683dc274","abstract":"Embedding projections are popular for visualizing large datasets and models. However, people often encounter friction when using embedding visualization tools: (1) barriers to adoption, e.g., tedious data wrangling and loading, scalability limits, no integration of results into existing workflows, and (2) limitations in possible analyses, without integration with external tools to additionally show coordinated views of metadata. In this paper, we present Embedding Atlas, a scalable, interactive visualization tool designed to make interacting with large embeddings as easy as possible. Embedding Atlas uses modern web technologies and advanced algorithms\u2014including density-based clustering, and automated labeling\u2014to provide a fast and rich data analysis experience at scale. We evaluate Embedding Atlas with a competitive analysis against other popular embedding tools, showing that Embedding Atlas's feature set specifically helps reduce friction, and report a benchmark on its real-time rendering performance with millions of points. Embedding Atlas is available as open source to support future work in embedding-based analysis.","accessible_pdf":"Accessible","authors":[{"affiliation":"Apple","email":"donghao@apple.com","name":"Donghao Ren"},{"affiliation":"Apple","email":"fred.hohman@gmail.com","name":"Fred Hohman"},{"affiliation":"Apple Inc.","email":"halden.lin@gmail.com","name":"Halden Lin"},{"affiliation":"Apple","email":"domoritz@cmu.edu","name":"Dominik Moritz"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"8b2cd3c8-c4e3-483a-a519-bbb3683dc274","image_caption":"","keywords":["Embedding visualization","visual analytics"],"open_access_supplemental_link":"https://apple.github.io/embedding-atlas/","open_access_supplemental_question":"Our paper is a visualization system with multiple implemented algorithms for transforming data, all of which are open-sourced and able to be demoed in a web-browser without any installation.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://www.arxiv.org/abs/2505.06386","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1147","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"8b2cd3c8-c4e3-483a-a519","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:36:00.000Z","title":"Embedding Atlas: Low-Friction, Interactive Embedding Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"8b559c18-b722-4983-a0e2-cd7365817d7b","abstract":"Visualization for explainable and trustworthy machine learning remains one of the most important and heavily researched fields within information visualization and visual analytics with various application domains, such as medicine, finance, and bioinformatics. After our 2020 state-of-the-art report comprising 200 techniques, we have persistently collected peer-reviewed articles describing visualization techniques, categorized them based on the previously established categorization schema consisting of 119 categories, and provided the resulting collection of 542 techniques in an online survey browser. In this survey article, we present the updated findings of new analyses of this dataset as of fall 2023 and discuss trends, insights, and eight open challenges for using visualizations in machine learning. Our results corroborate the rapidly growing trend of visualization techniques for increasing trust in machine learning models in the past three years, with visualization found to help improve popular model explainability methods and check new deep learning architectures, for instance.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Angelos Chatzimparmpas"},{"email":null,"name":"Kostiantyn Kucher"},{"email":null,"name":"Andreas Kerren"}],"award":"","doi":"10.1109/MCG.2024.3360881","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"8b559c18-b722-4983-a0e2-cd7365817d7b","image_caption":"","keywords":["Surveys","Data visualization","Browsers","Market research","Industries","Explainable AI","Trusted computing","Machine learning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2403.12005","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3360881","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga1","session_room":"Room 0.96 + 0.97","session_room_id":"0_96_0_97","session_title":"Reflections and Looking Forward","session_uid":"8b559c18-b722-4983-a0e2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Reflections and Looking Forward"],"time_stamp":"2025-11-07T08:30:00.000Z","title":"Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/29PgqT6NWUQ"},{"UID":"8d730321-05df-42cd-bb4d-013ae3589399","abstract":"Visual Analytics (VA) has become a paramount discipline in supporting data analysis in many scientific domains, empowering the human user with automatic capabilities while keeping the lead in the analysis. At the same time, designing an effective VA solution is not a simple task, requiring its adaptation to the problem at hand and the intended user of the system. In this scenario, the User-Centered Design (UCD) methodology provides the framework to incorporate user needs into the design of a VA solution. On the other hand, its implementation mainly relies on qualitative feedback, with the designer missing tools supporting her in quantitatively reporting the user feedback and using it to hypothesize and test the successive changes to the VA solution. To overcome this limitation, we propose TraVIS, a Visual Analytics solution allowing the loading of a web-based VA system, collecting user traces, and analyzing them with respect to the system at hand. In this process, the designer can leverage the collected traces and relate them to the tasks the VA solution supports and how those can be achieved. Using TraVIS, the designer can identify ineffective interaction paths, analyze the user traces support to task completion, hypothesize corrections to the design, and evaluate the effect of changes. We evaluated TraVIS through experimentation with 11 VA systems from literature, a use case, and user evaluation with five experts. Results show the benefits that TraVIS provides in terms of identifying design problems and efficient support for UCD.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Matteo Filosa"},{"email":null,"name":"Alexandra Plexousaki"},{"email":null,"name":"Matteo Di Stadio"},{"email":null,"name":"Francesco Bovi"},{"email":null,"name":"Dario Benvenuti"},{"email":null,"name":"Tiziana Catarci"},{"email":null,"name":"Marco Angelini"}],"award":"","doi":"10.1109/TVCG.2025.3546863","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"8d730321-05df-42cd-bb4d-013ae3589399","image_caption":"","keywords":["Visual analytics","Data visualization","User centered design","Optimization","Taxonomy","Process mining","Iterative methods","Hands","Focusing","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3546863","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full19","session_room":"Hall E1","session_room_id":"e1","session_title":"Interaction, Provenance, and Collaboration","session_uid":"8d730321-05df-42cd-bb4d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Interaction, Provenance, and Collaboration"],"time_stamp":"2025-11-06T15:33:00.000Z","title":"TraVIS: A User Trace Analyzer to Support User-Centered Design of Visual Analytics Solutions","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/253qu_2o0K0"},{"UID":"9188996c-0f8f-420b-95eb-1cf466e3fa0d","abstract":"Text is an integral but understudied component of visualization design. Although recent studies have examined how text elements (e.g., titles and annotations) influence comprehension, preferences, and predictions, many questions remain about textual design and use in practice. This paper introduces a framework for understanding text functions in information visualizations, building on and filling gaps in prior classifications and taxonomies. Through an analysis of 120 real-world visualizations and 804 text elements, we identified ten distinct text functions, ranging from identifying data mappings to presenting valenced subtext. We further identify patterns in text usage and conduct a factor analysis, revealing four overarching text-informed design strategies: Attribution and Variables, Annotation-Centric Design, Visual Embellishments, and Narrative Framing. In addition to these factors, we explore features of title rhetoric and text multifunctionality, while also uncovering previously unexamined text functions, such as text replacing visual elements. Our findings highlight the flexibility of text, demonstrating how different text elements in a given design can combine to communicate, synthesize, and frame visual information. This framework adds important nuance and detail to existing frameworks that analyze the diverse roles of text in visualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of California Berkeley","email":"chase_stokes@berkeley.edu","name":"Chase Stokes"},{"affiliation":"Northeastern University","email":"aarunku5@asu.edu","name":"Anjana Arunkumar"},{"affiliation":"UC Berkeley","email":"hearst@berkeley.edu","name":"Marti Hearst"},{"affiliation":"Northeastern University","email":"l.padilla@northeastern.edu","name":"Lace Padilla"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9188996c-0f8f-420b-95eb-1cf466e3fa0d","image_caption":"","keywords":["Visualization","text","language","text function","factor analysis","design patterns."],"open_access_supplemental_link":"https://osf.io/swqfc/","open_access_supplemental_question":"We provide all analysis code necessary for the rigorous validation we completed on our factor anlysis results.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.12334","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1611","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full15","session_room":"Hall E2","session_room_id":"e2","session_title":"From Data to Meaning","session_uid":"9188996c-0f8f-420b-95eb","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["From Data to Meaning"],"time_stamp":"2025-11-06T11:03:00.000Z","title":"An Analysis of Text Functions in Information Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/v-iY-PKyXHs"},{"UID":"918b3f1e-6e9b-43f0-bb8b-5bad6b3956ef","abstract":"Smartwatches increasingly serve as daily data analysis tools, yet their visualization designs face unique challenges: small screens, diverse dial shapes (round/square), and mobile usage contexts (sitting/walking). These factors may interact in unexpected ways, potentially compromising data readability during real-world use. Through a controlled experiment (N=32) comparing bar vs. radial visualizations across dial shapes and motion states, we found: (1) walking significantly reduces estimation accuracy, (2) bar charts consistently outperform radial variants by better supporting linear perception, and (3) radial bar graphs on square dials create particularly poor affordances than other combination. Our work provides the empirical evidence for how smartwatch dial factors, usage contexts and visualization design jointly impact data interpretation, offering concrete design guidelines for smartwatch visualization. Supplemental material is available at https://osf.io/k7fa3/.","accessible_pdf":null,"authors":[{"affiliation":"Xi\u2019an Jiaotong-Liverpool University","email":"zhouxuan.xia21@student.xjtlu.edu.cn","name":"Zhouxuan Xia"},{"affiliation":"Xi\u2019an Jiaotong-Liverpool University","email":"fengyuan.liao22@student.xjtlu.edu.cn","name":"Fengyuan Liao"},{"affiliation":"Xi\u2019an Jiaotong-Liverpool University","email":"jinyuan.du18@student.xjtlu.edu.cn","name":"Jinyuan Du"},{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"yu.liu02@xjtlu.edu.cn","name":"Yu Liu"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"918b3f1e-6e9b-43f0-bb8b-5bad6b3956ef","image_caption":"","keywords":["Smartwatch Visualization","Mobile Visualization","Human-Computer Interaction"],"open_access_supplemental_link":"https://osf.io/k7fa3/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1152","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"918b3f1e-6e9b-43f0-bb8b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T15:12:00.000Z","title":"Visualizing on the Wrist: Impact of Motion, Dial Shape and Visualization Type on Smartwatch","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"9259ca04-ae61-4513-b066-711f3a75cbc1","abstract":"Tracking the flow of carbon (C) through the Earth\u2019s terrestrial biosphere remains a major challenge to understanding how ecosystems respond to environmental change [12]. To build an understanding of this flow, researchers have recently developed models that include thousands of variables whose intricate inter-dependencies can make them difficult to interpret. To open this scientific black box, we partnered with C cycle scientists at the NASA Jet Propulsion Lab who developed the Carbon Data-Model (CARDAMOM) framework [5]. This paper presents a design study of CLOVE, a C science visual analytics application that encodes model dimensions in an intuitive way by using visual metaphors that correspond to the natural world. We describe how CLOVE encapsulates C states and fluxes in complex models, and explains how C cycle scientists can use it to diagnose their model outputs.","accessible_pdf":"Accessible","authors":[{"affiliation":"Havard University","email":"linhphaaam@gmail.com","name":"Linh Pham"},{"affiliation":"Harvard University","email":"inbox.kevin.hu@gmail.com","name":"Kevin Hu"},{"affiliation":"ArtCenter College of Design","email":"junaline4@gmail.com","name":"Minyoung Joo"},{"affiliation":"University of Wisconsin - Madison","email":"ntwhite@wisc.edu","name":"Nathan White"},{"affiliation":"Jet Propulsion Lab","email":"alexis.a.bloom@jpl.nasa.gov","name":"Anthony Bloom"},{"affiliation":"Jet Propulsion Lab","email":"krys.t.blackwood@jpl.nasa.gov","name":"Krys Blackwood"},{"affiliation":"California Institute of Technology","email":"santiago@caltech.edu","name":"Santiago Lombeyda"},{"affiliation":"California Institute of Technology (Caltech)","email":"hmushkin@caltech.edu","name":"Hillary Mushkin"},{"affiliation":"California Institute of Technology","email":"sd@scottdavidoff.com","name":"Scott Davidoff"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9259ca04-ae61-4513-b066-711f3a75cbc1","image_caption":"","keywords":["earth system","bayesian carbon cycle","design study"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1112","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"9259ca04-ae61-4513-b066","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T11:00:00.000Z","title":"Interactive Visual Analytics of Carbon Cycle Science","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"930062ee-90b2-4c48-9896-626047d01771","abstract":"Visualization design is often described as a process of solving a well-defined problem by navigating a design space. While existing visualization design models have provided valuable structure and guidance, they tend to foreground technical problem-solving and underemphasize the interpretive, judgment-based aspects of design. In contrast, research in other design disciplines has emphasized the importance of framing---how designers define and redefine what the problem is---and the co-evolution of problem and solution spaces through reflective practice. These dimensions remain underexplored in visualization research, particularly from the perspective of expert practitioners. This paper investigates how visualization designers frame problems and navigate the interplay between problem understanding and solution development. We conducted a mixed-methods study with 11 expert design practitioners using design challenges, diary entries, and semi-structured interviews. Through reflexive thematic analysis, we identified key strategies that participants used to frame design problems, reframe them in response to evolving constraints or insights, and construct bridges between problem and solution spaces. These included the use of metaphors, heuristics, sketching, primary generators, and reflective evaluation of failed or incomplete ideas. Our findings contribute an empirically grounded account of visualization design as a reflective, co-evolutionary practice. We show that framing is not a preliminary step, but a continuous activity embedded in the act of designing. Participants frequently shifted their understanding of the problem based on solution attempts, feedback from tools, and ethical or narrative concerns. These insights extend current visualization design models and highlight the need for frameworks that better account for framing and interpretive judgment. We conclude with implications for visualization research, education, and practice. In particular, we discuss how design education can better support framing and co-evolutionary thinking, and how visualization research can benefit from greater attention to the cognitive strategies and reflective processes that underpin expert design.","accessible_pdf":null,"authors":[{"affiliation":"Purdue University","email":"parsonsp@purdue.edu","name":"Paul Parsons"},{"affiliation":"Purdue University","email":"shukla37@purdue.edu","name":"Prakash Chandra Shukla"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"930062ee-90b2-4c48-9896-626047d01771","image_caption":"","keywords":["Data visualization design","data visualization practice","problem framing","problem-solution co-evolution","design cognition"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1971","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full7","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Best Full Papers","session_uid":"930062ee-90b2-4c48-9896","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Best Full Papers"],"time_stamp":"2025-11-04T13:58:00.000Z","title":"Beyond Problem Solving: Framing and Problem\u2013Solution Co-Evolution in Data Visualization Design","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/3EozS40EBjc"},{"UID":"94381081-f2ce-4b6b-873e-14124be28e0b","abstract":"Graphical overlays that layer visual elements onto charts, are effective to convey insights and context in financial narrative visualizations. However, automating graphical overlays is challenging due to complex narrative structures and limited understanding of effective overlays. To address the challenge, we first summarize the commonly used graphical overlays and narrative structures, and the proper correspondence between them in financial narrative visualizations, elected by a survey of 1752 layered charts with corresponding narratives. We then design FinFlier, a two-stage innovative system leveraging a knowledge-grounding large language model to automate graphical overlays for financial visualizations. The text-data binding module enhances the connection between financial vocabulary and tabular data through advanced prompt engineering, and the graphics overlaying module generates effective overlays with narrative sequencing. We demonstrate the feasibility and expressiveness of FinFlier through a gallery of graphical overlays covering diverse financial narrative visualizations. Performance evaluations and user studies further confirm system\u2019s effectiveness and the quality of generated layered charts.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Jianing Hao"},{"email":null,"name":"Manling Yang"},{"email":null,"name":"Qing Shi"},{"email":null,"name":"Yuzhe Jiang"},{"email":null,"name":"Guang Zhang"},{"email":null,"name":"Wei Zeng"}],"award":"","doi":"10.1109/TVCG.2024.3514138","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"94381081-f2ce-4b6b-873e-14124be28e0b","image_caption":"","keywords":["Data visualization","Visualization","Economic indicators","Vocabulary","Surveys","Finance","Authoring systems","Terminology","Prompt engineering","Market research"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.06821","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023-12-0857]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full1","session_room":"Hall E1","session_room_id":"e1","session_title":"Agentic Visualization and Intelligent Systems","session_uid":"94381081-f2ce-4b6b-873e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Agentic Visualization and Intelligent Systems"],"time_stamp":"2025-11-05T14:57:00.000Z","title":"FinFlier: Automating Graphical Overlays for Financial Visualizations With Knowledge-Grounding Large Language Model","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/2z0_8lHbM04"},{"UID":"943a0aa8-b141-48cd-90a1-9a899d531ffe","abstract":"We describe a multidisciplinary collaboration to iteratively design an interactive exhibit for a public science center on paleoclimate, the study of past climates. We created a data physicalisation of mountains and ice sheets that can be tangibly manipulated by visitors to interact with a wind simulation visualisation that demonstrates how the climate of North America differed dramatically between now and the peak of the last ice age. We detail the system for interaction and visualisation plus design choices to appeal to an audience that ranges from children to scientists and responds to site requirements.","accessible_pdf":null,"authors":[{"affiliation":"University of Colorado Boulder","email":"dahu8258@colorado.edu","name":"David Hunter"},{"affiliation":"Mechanical Engineering","email":"pablo.botin@colorado.edu","name":"Pablo Botin"},{"affiliation":"UCAR","email":"emilysb@ucar.edu","name":"Emily Snode-Brenneman"},{"affiliation":"UCAR","email":"asteverm@ucar.edu","name":"Amy Stevermer"},{"affiliation":"UCAR|NCAR","email":"hatheway@ucar.edu","name":"Becca Hatheway"},{"affiliation":"NOAA","email":"dillon.amaya@noaa.gov","name":"Dillon Amaya"},{"affiliation":"Science Communicator","email":"eddie@eddiegoldstein.com","name":"Eddie Goldstein"},{"affiliation":"University of Colorado Boulder","email":"wayne.seltzer@colorado.edu","name":"Wayne Seltzer"},{"affiliation":"University of Colorado Boulder","email":"mdgross@colorado.edu","name":"Mark Gross"},{"affiliation":"CIRES","email":"kristopher.karnauskas@colorado.edu","name":"Kris Karnauskas"},{"affiliation":"Cornell University","email":"daniel.leithinger@cornell.edu","name":"Daniel Leithinger"},{"affiliation":"University of Colorado Boulder","email":"ellen.do@colorado.edu","name":"Ellen Yi-Luen Do"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"943a0aa8-b141-48cd-90a1-9a899d531ffe","image_caption":"","keywords":["visualization; physicalization; tangible interaction; exhibit design;"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1111","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap1","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Pictorial","session_uid":"943a0aa8-b141-48cd-90a1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Pictorial"],"time_stamp":"2025-11-06T10:15:00.000Z","title":"Winds Through Time: Interactive Data Visualization and Physicalization for Paleoclimate Communication","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/NZ0Wwrqv6fA"},{"UID":"973882c9-093a-4edf-b7b0-9157aabaa93b","abstract":"Machine learning algorithms may be chosen for their effectiveness at predictions, but they lose impact when human decision makers do not understand the predictions well enough to trust them. Though machine learning algorithms that produce interpretable models are available, many factors influence the selection for a particular application, and many tools exist for building understanding post-hoc. One way to develop insight is to probe through a concept humans use in making decisions -- an examination of alternate scenarios that might change the outcome called counterfactuals. In this short paper, we build on work using counterfactuals for comprehending machine learning models by proposing a technique wherein users explore counterfactuals as paths through a tree of possible data attribute changes. We extend the technique to groups of data points and consequently to groups of counterfactuals. We provide a prototype implementation and and evaluation with four users from different fields of expertise who were able to apply CFTree to their own domain data and discover interesting attribute relationships.","accessible_pdf":null,"authors":[{"affiliation":"DePaul University","email":"caroline.cao39@gmail.com","name":"Fang Cao"},{"affiliation":"DePaul University","email":"ebrown80@cdm.depaul.edu","name":"Eli Brown"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"973882c9-093a-4edf-b7b0-9157aabaa93b","image_caption":"","keywords":["counterfactuals","human-centered AI","interpretable machine learning","visual analytics"],"open_access_supplemental_link":"https://osf.io/j2wgx/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1355","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"973882c9-093a-4edf-b7b0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:45:00.000Z","title":"CFTree: Exploring Paths Through Counterfactuals","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"981234e2-bdbe-42ef-863d-00d6ea28561d","abstract":"This application paper investigates the stability of hydrogen bonds (H-bonds), as characterized by the Quantum Theory of Atoms in Molecules (QTAIM). First, we contribute a database of 4544 electron densities associated to four isomers of water hexamers (the so-called Ring, Book, Cage and Prism), generated by distorting their equilibrium geometry under various structural perturbations, modeling the natural dynamic behavior of molecular systems. Second, we present a new stability measure, called bond occurrence rate, associating each bond path present at equilibrium with its rate of occurrence within the input ensemble. We also provide an algorithm, called BondMatcher, for its automatic computation, based on a tailored, geometry-aware partial isomorphism estimation between the extremum graphs of the considered electron densities. Our new stability measure allows for the automatic identification of densities lacking H-bond paths, enabling further visual inspections. Specifically, the topological analysis enabled by our framework corroborates experimental observations and provides refined geometrical criteria for characterizing the disappearance of H-bond paths. Our electron density database and our C++ implementation are available at this address: https://github.com/thom-dani/BondMatcher.","accessible_pdf":null,"authors":[{"affiliation":"CNRS (LIP6)","email":"thomas.daniel@lip6.fr","name":"Thomas Daniel"},{"affiliation":"Center of New Technologies","email":"malgorzata.olejniczak@cent.uw.edu.pl","name":"Malgorzata Olejniczak"},{"affiliation":"CNRS","email":"julien.tierny@sorbonne-universite.fr","name":"Julien Tierny"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"981234e2-bdbe-42ef-863d-00d6ea28561d","image_caption":"","keywords":["Quantum chemistry","topological data analysis","discrete Morse theory","ensemble data."],"open_access_supplemental_link":null,"open_access_supplemental_question":"Replicable experiments (data, code and documentation are provided).","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2504.03205","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1440","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full2","session_room":"Hall E2","session_room_id":"e2","session_title":"Algorithms and Workflows","session_uid":"981234e2-bdbe-42ef-863d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Algorithms and Workflows"],"time_stamp":"2025-11-06T13:12:00.000Z","title":"BondMatcher: H-Bond Stability Analysis in Molecular Systems","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/wUoqSN_s3tw"},{"UID":"9942014b-e0f8-4cae-9b2a-81167d226c7b","abstract":"Multimodal vision-language models (VLMs) continue to achieve ever-improving scores on chart understanding benchmarks. Yet, we find that this progress does not fully capture the breadth of visual reasoning capabilities essential for interpreting charts. We introduce ENCQA, a novel benchmark informed by the visualization literature, designed to provide systematic coverage of visual encodings and analytic tasks that are crucial for chart understanding. ENCQA provides 2,076 synthetic question-answer pairs, enabling balanced coverage of six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks (find extrema, retrieve value, find anomaly, filter values, compute derived value exact, compute derived value relative, correlate values, and correlate values relative). Our evaluation of 9 state-of-the-art VLMs reveals that performance varies significantly across encodings within the same task, as well as across tasks. Contrary to expectations, we observe that performance does not improve with model size for many task-encoding pairs. Our results suggest that advancing chart understanding requires targeted strategies addressing specific visual reasoning gaps, rather than solely scaling up model or dataset size.","accessible_pdf":null,"authors":[{"affiliation":"Stanford University","email":"kushinm11@gmail.com","name":"Kushin Mukherjee"},{"affiliation":"Apple","email":"donghao@apple.com","name":"Donghao Ren"},{"affiliation":"Apple","email":"domoritz@cmu.edu","name":"Dominik Moritz"},{"affiliation":"Apple","email":"yassogba@gmail.com","name":"Yannick Assogba"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9942014b-e0f8-4cae-9b2a-81167d226c7b","image_caption":"","keywords":["Visual encodings","visualization understanding tasks","machine chart understanding","vision-language models","model benchmarking"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2508.04650v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1590","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full12","session_room":"Hall E1","session_room_id":"e1","session_title":"Explanation, Exploration, and Model Configuration","session_uid":"9942014b-e0f8-4cae-9b2a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explanation, Exploration, and Model Configuration"],"time_stamp":"2025-11-06T09:06:00.000Z","title":"EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/h8CsAxh4LWg"},{"UID":"99d47ba0-22e8-4d68-9484-346e6fc722b8","abstract":"Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.","accessible_pdf":null,"authors":[{"affiliation":"UC Davis","email":"david.bauer009@gmail.com","name":"David Bauer"},{"affiliation":"NVIDIA","email":"qiwu@nvidia.com","name":"Qi Wu"},{"affiliation":"University of Groningen","email":"h.gadirov@rug.nl","name":"Hamid Gadirov"},{"affiliation":"University of California at Davis","email":"ma@cs.ucdavis.edu","name":"Kwan-Liu Ma"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"99d47ba0-22e8-4d68-9484-346e6fc722b8","image_caption":"","keywords":["Radiance caching","path tracing","volume rendering","gaussian splatting"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.19718","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1100","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full32","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Volumes & 3D","session_uid":"99d47ba0-22e8-4d68-9484","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Volumes & 3D"],"time_stamp":"2025-11-06T10:27:00.000Z","title":"GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"9af6703f-fc27-4be6-a995-f84e8091b517","abstract":"Implicit neural representations (INRs) have emerged as a transformative paradigm for time-varying volumetric data compression and representation, owing to their ability to model high-dimensional signals effectively. INRs represent scalar fields based on sampled coordinates, typically using either a single network for the entire field or multiple networks across different spatial domains. However, these approaches often face challenges in modeling complex patterns and introducing boundary artifacts. To address these limitations, we propose\u00a0MoE-INR, an INR architecture based on a mixture-of-experts (MoE) framework. MoE-INR automates irregular subdivisions of spatiotemporal fields and dynamically assigns them to different expert networks. The architecture comprises three key components: a\u00a0policy network,\u00a0a shared encoder, and multiple expert decoders. The policy network subdivides the field and determines which expert decoder is responsible for a given input coordinate. The shared encoder extracts hidden representations from the input coordinates, and the expert decoders transform these high-dimensional features into scalar values. This design results in a unified framework accommodating diverse INR types, including conventional, grid-based, and ensemble. We evaluate the effectiveness of MoE-INR on multiple time-varying datasets with varying characteristics. Experimental results demonstrate that MoE-INR significantly outperforms existing non-MoE and MoE-based INRs and traditional lossy compression methods across quantitative and qualitative metrics under various compression ratios.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology","email":"junhanvis@outlook.com","name":"Jun Han"},{"affiliation":"University of Notre Dame","email":"ktang2@nd.edu","name":"Kaiyuan Tang"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9af6703f-fc27-4be6-a995-f84e8091b517","image_caption":"","keywords":["Time-varying data compression","implicit neural representation","volume visualization","mixture-of-experts"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1023","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full13","session_room":"Hall E2","session_room_id":"e2","session_title":"Fields, Fields, Fields","session_uid":"9af6703f-fc27-4be6-a995","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Fields, Fields, Fields"],"time_stamp":"2025-11-05T10:15:00.000Z","title":"MoE-INR: Implicit Neural Representation with Mixture-of-Experts for Time-Varying Volumetric Data Compression","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mHta4w5FJQg"},{"UID":"9d0c979c-02fc-47d5-9f8f-50fc27cf9c35","abstract":"We conduct a deconstructive reading of a qualitative interview study with 17 visual data journalists from newsrooms across the globe. We borrow a deconstruction approach from literary critique to explore the instability of meaning in language and reveal implicit beliefs in words and ideas. Through our analysis we surface two sets of opposing implicit beliefs in visual data journalism: objectivity/subjectivity and humanism/mechanism. We contextualize these beliefs through a genealogical analysis, which brings deconstruction theory into practice by providing a historic backdrop for these opposing perspectives. Our analysis shows that these beliefs held within visual data journalism are not self-enclosed but rather a product of external societal forces and paradigm shifts over time. Through this work, we demonstrate how thinking with critical theories such as deconstruction and genealogy can reframe success in visual data storytelling and diversify visualization research outcomes. These efforts push the ways in which we as researchers produce domain knowledge to examine the sociotechnical issues of today's values towards datafication and data visualization. All supplemental materials for this work are available at osf.io/5fr48.","accessible_pdf":null,"authors":[{"affiliation":"University of Bergen","email":"ke.zhang@uib.no","name":"Ke Er Amy Zhang"},{"affiliation":"University of Toronto","email":"j.jenkinson@utoronto.ca","name":"Jodie Jenkinson"},{"affiliation":"University of Bergen","email":"laura.garrison@uib.no","name":"Laura Garrison"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9d0c979c-02fc-47d5-9f8f-50fc27cf9c35","image_caption":"","keywords":["Visualization","visual data journalism","epistemology","critical theory","poststructuralism","genealogy","deconstruction"],"open_access_supplemental_link":"https://osf.io/5fr48/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.12377","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1666","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full26","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Stories and the Journalists Who Tell Them","session_uid":"9d0c979c-02fc-47d5-9f8f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Stories and the Journalists Who Tell Them"],"time_stamp":"2025-11-07T08:54:00.000Z","title":"Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/j1bJGzUi0jM"},{"UID":"9d6c7874-e4fa-449b-88aa-4b3937241e7d","abstract":"Constructing cell developmental trajectories is a critical task in single-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of potential cellular progression paths. However, current automated methods are limited to establishing cell developmental trajectories within individual samples, necessitating biologists to manually link cells across samples to construct complete cross-sample evolutionary trajectories that consider cellular spatial dynamics. This process demands substantial human effort due to the complex spatial correspondence between each pair of samples.\nTo address this challenge, we first proposed a GNN-based model to predict cross-sample cell developmental trajectories. We then developed TrajLens, a visual analytics system that supports biologists in exploring and refining the cell developmental trajectories based on predicted links. Specifically, we designed the visualization that integrates features on cell distribution and developmental direction across multiple samples, providing an overview of the spatial evolutionary patterns of cell populations along trajectories. Additionally, we included contour maps superimposed on the original cell distribution data, enabling biologists to explore them intuitively. To demonstrate our system's performance, we conducted quantitative evaluations of our model with two case studies and expert interviews to validate its usefulness and effectiveness.","accessible_pdf":"Accessible","authors":[{"affiliation":"Sichuan University","email":"wangqipengscu@stu.scu.edu.cn","name":"Qipeng Wang"},{"affiliation":"Singapore Management University","email":"haywardryan@foxmail.com","name":"Shaolun Ruan"},{"affiliation":"The Hong Kong University of Science and Technology","email":"rshengac@connect.ust.hk","name":"Rui Sheng"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"},{"affiliation":"Sichuan University","email":"zhumin@scu.edu.cn","name":"Min Zhu"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9d6c7874-e4fa-449b-88aa-4b3937241e7d","image_caption":"","keywords":["Visual Analytics","Single-cell RNA Sequencing","Cell Developmental Trajectories"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/ETFJ2","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"http://arxiv.org/abs/2507.15620","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1247","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full8","session_room":"Hall M1","session_room_id":"m1","session_title":"Biomedical Visualization","session_uid":"9d6c7874-e4fa-449b-88aa","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Biomedical Visualization"],"time_stamp":"2025-11-06T14:57:00.000Z","title":"TrajLens: Visual Analysis for Constructing Cell Developmental Trajectories in Cross-Sample Exploration","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rgq3gdIQGxk"},{"UID":"9e05775a-056c-42f7-a3c2-e81281c5045a","abstract":"We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities\u2014such as threshold-based filtering, slice extraction, and statistical analysis\u2014through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system\u2019s adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., \u201cvisualize the skull\u201d or \u201chighlight tissue boundaries\u201d). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.","accessible_pdf":null,"authors":[{"affiliation":"Los Alamos National Laboratory","email":"ayanju04@gmail.com","name":"Ayan Biswas"},{"affiliation":"Los Alamos National Laboratory","email":"tlturton@lanl.gov","name":"Terece Turton"},{"affiliation":"Los Alamos National Laboratory","email":"ranasinghe@lanl.gov","name":"Nishath Ranasinghe"},{"affiliation":"Los Alamos National Laboratory","email":"smjones@lanl.gov","name":"Shawn Jones"},{"affiliation":"Los Alamos National Laboratory","email":"love@lanl.gov","name":"Bradley Love"},{"affiliation":"Los Alamos National Laboratory","email":"wjones@lanl.gov","name":"William Jones"},{"affiliation":"Los Alamos National Laboratory","email":"hagberg@lanl.gov","name":"Aric Hagberg"},{"affiliation":"The Ohio State University","email":"shen.94@osu.edu","name":"Han-Wei Shen"},{"affiliation":"Los Alamos National Laboratory","email":"ndebard@lanl.gov","name":"Nathan Debardeleben"},{"affiliation":"Los Alamos National Laboratory","email":"earl@lanl.gov","name":"Earl Lawrence"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9e05775a-056c-42f7-a3c2-e81281c5045a","image_caption":"","keywords":["Scientific data","Large language models","Agentic workflows","Natural language","Feature-based visualization."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.21124","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1626","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full2","session_room":"Hall E2","session_room_id":"e2","session_title":"Algorithms and Workflows","session_uid":"9e05775a-056c-42f7-a3c2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Algorithms and Workflows"],"time_stamp":"2025-11-06T13:48:00.000Z","title":"VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/wUoqSN_s3tw"},{"UID":"9e3dfe2c-bbe4-4431-ae8b-1ee64a399c8b","abstract":"Sonification offers a non-visual way to understand data, with pitch-based encodings being the most common. Yet, how well people perceive slope and acceleration\u2014key features of data trends\u2014remains poorly understood. Drawing on people's natural abilities to perceive tempo, we introduce a novel sampling method for pitch-based sonification to enhance the perception of slope and acceleration in univariate functions. While traditional sonification methods often sample data at uniform x-spacing, yielding notes played at a fixed tempo with variable pitch intervals (Variable Pitch Interval), our approach samples at uniform y-spacing, producing notes with consistent pitch intervals but variable tempo (Variable Tempo). We conducted psychoacoustic experiments to understand slope and acceleration perception across three sampling methods: Variable Pitch Interval, Variable Tempo, and a Continuous (no sampling) baseline. In slope comparison tasks, Variable Tempo was more accurate than the other methods when modulated by the magnitude ratio between slopes. For acceleration perception, just-noticeable differences under Variable Tempo were over 13 times finer than with other methods. Participants also commonly reported higher confidence, lower mental effort, and a stronger preference for Variable Tempo compared to other methods. This work contributes models of slope and acceleration perception across pitch-based sonification techniques, introduces Variable Tempo as a novel and preferred sampling method, and provides promising initial evidence that leveraging timing can lead to more sensitive, accurate, and precise interpretation of derivative-based data features.","accessible_pdf":"Accessible","authors":[{"affiliation":"Stanford University","email":"danfan17@stanford.edu","name":"Danyang Fan"},{"affiliation":"Stanford University","email":"walksmit@stanford.edu","name":"Walker Smith"},{"affiliation":"Stanford University","email":"takako@ccrma.stanford.edu","name":"Takako Fujioka"},{"affiliation":"Stanford University","email":"cc@ccrma.stanford.edu","name":"Chris Chafe"},{"affiliation":"University of Michigan","email":"sileo@umich.edu","name":"Sile O'Modhrain"},{"affiliation":"Stanford University","email":"othello5@stanford.edu","name":"Diana Deutsch"},{"affiliation":"Stanford University","email":"sfollmer@stanford.edu","name":"Sean Follmer"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9e3dfe2c-bbe4-4431-ae8b-1ee64a399c8b","image_caption":"","keywords":["Visualization","Sonification","Empirical Studies","Auditory Perception"],"open_access_supplemental_link":"https://osf.io/a4cth/?view_only=784965f9e9f64a3ea720cf53ff241481","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://shape.stanford.edu/research/TempoSonification/Fan25PerceivingSlopeAndAccelerationVariableTempoSamplingSonification.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1627","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full37","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Rescheduled Papers","session_uid":"9e3dfe2c-bbe4-4431-ae8b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Rescheduled Papers"],"time_stamp":"2025-11-06T13:48:00.000Z","title":"Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"9e9a15b7-6ccb-4419-b391-e82e3bd8b907","abstract":"Data-driven news articles combine narrative storytelling with data visualizations to inform and influence public opinion on pressing societal issues. These articles often employ persuasive strategies, which are rhetorical techniques in narrative framing, visual rhetoric, or data presentation, to influence audience interpretation and opinion formation regarding information communication. While previous research has examined whether and when data visualizations persuade, the strategic choices made by persuaders remain largely unexplored. Addressing this gap, our work presents a taxonomy of persuasive strategies grounded in psychological theories and expert insights, categorizing 15 strategies across five dimensions: Credibility, Guided Interpretation, Reference-based Framing, Emotional Appeal, and Participation Invitation. To facilitate large-scale analysis, we curated a dataset of 936 data-driven news articles annotated with both persuasive strategies and their perceived effects. Leveraging this corpus, we developed a multimodal, multi-task learning model that jointly predicts the presence of persuasive strategies and their persuasive effects by incorporating both embedded (text and visualization) and explicit (visual narrative and psycholinguistic) features. Our evaluation demonstrates that our model outperforms state-of-the-art baselines in identifying persuasive strategies and measuring their effects.","accessible_pdf":null,"authors":[{"affiliation":"Tongji University","email":"2411927@tongji.edu.cn","name":"Zikai Li"},{"affiliation":"Tongji University","email":"chuyizheng02@gmail.com","name":"Chuyi Zheng"},{"affiliation":"Tongji University","email":"ankerlee168@gmail.com","name":"Ziang Li"},{"affiliation":"College of Design and Innovation, Tongji University","email":"shiyang1230@gmail.com","name":"Yang Shi"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"9e9a15b7-6ccb-4419-b391-e82e3bd8b907","image_caption":"","keywords":["Data-driven storytelling","persuasive strategies","taxonomy","computational modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1168","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full26","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Stories and the Journalists Who Tell Them","session_uid":"9e9a15b7-6ccb-4419-b391","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Stories and the Journalists Who Tell Them"],"time_stamp":"2025-11-07T08:42:00.000Z","title":"Data Speaks, But Who Gives It a Voice? Understanding Persuasive Strategies in Data-Driven News Articles","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/j1bJGzUi0jM"},{"UID":"9f054a2c-3dee-41b8-97e4-d8fe354cbd73","abstract":"Visual Parameter Space Analysis (VPSA) enables domain scientists to explore input-output relationships of computational models. Existing VPSA applications often feature multi-view visualizations designed by visualization experts for a specific scenario, making it hard for domain scientists to adapt them to their problems without professional help. We present RSVP, the Rapid Suggestive Visualization Prototyping system encoding VPSA knowledge to enable domain scientists to prototype custom visualization dashboards tailored to their specific needs. The system implements a task-oriented, multi-view visualization recommendation strategy over a visualization design space optimized for VPSA to guide users in meeting their analytical demands. We derived the VPSA knowledge implemented in the system by conducting an extensive meta design study over the body of work on VPSA. We show how this process can be used to perform a data and task abstraction, extract a common visualization design space, and derive a task-oriented VisRec strategy. User studies indicate that the system is user-friendly and can uncover novel insights.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Manfred Klaffenboeck"},{"email":null,"name":"Michael Gleicher"},{"email":null,"name":"Johannes Sorger"},{"email":null,"name":"Michael Wimmer"},{"email":null,"name":"Torsten Moeller"}],"award":"","doi":"10.1109/TVCG.2024.3431930","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"9f054a2c-3dee-41b8-97e4-d8fe354cbd73","image_caption":"","keywords":["Data visualization","Visualization","Computational modeling","Analytical models","Image segmentation","Image color analysis","Data models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2409.07105","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023-04-0221.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full10","session_room":"Hall E2","session_room_id":"e2","session_title":"Dimensionality Reduction and Parameter Space Analysis","session_uid":"9f054a2c-3dee-41b8-97e4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dimensionality Reduction and Parameter Space Analysis"],"time_stamp":"2025-11-05T15:45:00.000Z","title":"RSVP for VPSA : A Meta Design Study on Rapid Suggestive Visualization Prototyping for Visual Parameter Space Analysis","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/lJXNA9vtCPA"},{"UID":"a005eefe-c93e-4d8d-9e90-468a635c8887","abstract":"Advancements in accessibility technologies such as low-cost swell form printers or refreshable tactile displays promise to allow blind or low-vision (BLV) people to analyze data by transforming visual representations directly to tactile representations.\nHowever, it is possible that design guidelines derived from experiments on the visual perception system may not be suited for the tactile perception system. We investigate the potential mismatch between familiar visual encodings and tactile perception in an exploratory study into the strategies employed by BLV people to measure common graphical primitives converted to tactile representations. First, we replicate the Cleveland and McGill study on graphical perception using swell form printing with eleven BLV subjects. Then, we present results from a group interview in which we describe the strategies used by our subjects to read four common chart types. While our results suggest that familiar encodings based on visual perception studies can be useful in tactile graphics, our subjects also expressed a desire to use encodings designed explicitly for BLV people.  Based on this study, we identify gaps between the perceptual expectations of common charts and the perceptual tools available in tactile perception. Then, we present a set of guidelines for the design of tactile graphics that accounts for these gaps. Supplemental material is available at https://osf.io/3nsfp/?view_only=7b7b8dcbae1d4c9a8bb4325053d13d9f","accessible_pdf":"Accessible","authors":[{"affiliation":"Brandeis University","email":"areenkh@brandeis.edu","name":"Areen Khalaila"},{"affiliation":"Worcester Polytechnic Institute","email":"ltharrison@wpi.edu","name":"Lane Harrison"},{"affiliation":"Boston College","email":"nam.wook.kim@bc.edu","name":"Nam Wook Kim"},{"affiliation":"Brandeis University","email":"dylancashman@brandeis.edu","name":"Dylan Cashman"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a005eefe-c93e-4d8d-9e90-468a635c8887","image_caption":"","keywords":["Accessibility","Tactile Graphics","Graphical Perception"],"open_access_supplemental_link":"https://osf.io/3nsfp/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1508","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full7","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Best Full Papers","session_uid":"a005eefe-c93e-4d8d-9e90","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Best Full Papers"],"time_stamp":"2025-11-04T13:22:00.000Z","title":"They Aren't Built For Me: An Exploratory Study of Strategies for Measurement of Graphical Primitives in Tactile Graphics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/3EozS40EBjc"},{"UID":"a049409b-b4a3-48ec-9f4e-83714ad50ba0","abstract":"The transportation network is an important element in an urban system that supports daily activities, enabling people to travel from one place to another. One of the key challenges is the network complexity, which is composed of many node pairs distributed over the area. This spatial characteristic results in the high dimensional network problem in understanding the \u2018cause\u2019 of problems such as traffic congestion. Recent studies have proposed visual analytics systems aimed at understanding these underlying causes. Despite these efforts, the analysis of such causes is limited to identified patterns. However, given the intricate distribution of roads and their mutual influence, new patterns continuously emerge across all roads within urban transportation. At this stage, a well-defined visual analytics system can be a good solution for transportation practitioners. In this paper, we propose a system, CATOM (Causal Topology Map), for the cause-effect analysis of traffic patterns based on Granger causality for extracting causal topology maps. CATOM discovers causal relationships between roads through the Granger causality test and quantifies these relationships through the causal density. During the design process, the system was developed to fully utilize spatial information with visualization techniques to overcome the previous problems in the literature. We also evaluate the usability of our approach by conducting a SUS(System Usability Scale) test and traffic cause analysis with the real-world data from two study sites in collaboration with domain experts.","accessible_pdf":"No","authors":[{"email":null,"name":"Chanyoung Jung"},{"email":null,"name":"Soobin Yim"},{"email":null,"name":"Giwoong Park"},{"email":null,"name":"Simon Oh"},{"email":null,"name":"Yun Jang"}],"award":"","doi":"10.1109/TVCG.2024.3489676","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"a049409b-b4a3-48ec-9f4e-83714ad50ba0","image_caption":"","keywords":["Cause effect analysis","Time series analysis","Visual analytics","Roads","Spatiotemporal phenomena","Traffic congestion","Topology","Usability","Traffic control","Market research"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-01-0049]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full29","session_room":"Hall M1","session_room_id":"m1","session_title":"Transportation, Buildings, and Urban Vis","session_uid":"a049409b-b4a3-48ec-9f4e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Transportation, Buildings, and Urban Vis"],"time_stamp":"2025-11-06T13:36:00.000Z","title":"CATOM : Causal Topology Map for Spatiotemporal Traffic Analysis with Granger Causality in Urban Areas","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/hoMn73KnXCg"},{"UID":"a23fb437-ab4c-4969-a413-eca93f2567c6","abstract":"Despite decision-making being a vital goal of data visualization, little work has been done to differentiate decision-making tasks within the field. While visualization task taxonomies and typologies exist, they often focus on more granular analytical tasks that are too low-level to describe large complex decisions, which can make it difficult to reason about and design decision-support tools. In this paper, we contribute a typology of decision-making tasks that were iteratively refined from a list of design goals distilled from a literature review. Our typology is concise and consists of only three tasks: CHOOSE, ACTIVATE, and CREATE. Although decision types originating in other disciplines exist, we provide definitions for these tasks that are suitable for the visualization community. Our proposed typology offers two benefits. First, the ability to compose and hierarchically organize the tasks enables flexible and clear descriptions of decisions with varying levels of complexities. Second, the typology encourages productive discourse between visualization designers and domain experts by abstracting the intricacies of data, thereby promoting clarity and rigorous analysis of decision-making processes. We demonstrate the benefits of our typology through four case studies, and present an evaluation of the typology from semi-structured interviews with experienced members of the visualization community who have contributed to developing or publishing decision support systems for domain experts. Our interviewees used our typology to delineate the decision-making processes supported by their systems, demonstrating its descriptive capacity and effectiveness. Finally, we present preliminary findings on the usefulness of our typology for visualization design.","accessible_pdf":"No","authors":[{"email":null,"name":"Camelia D. Brumar"},{"email":null,"name":"Sam Molnar"},{"email":null,"name":"Gabriel Appleby"},{"email":null,"name":"Kristi Potter"},{"email":null,"name":"Remco Chang"}],"award":"","doi":"10.1109/TVCG.2025.3572842","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"a23fb437-ab4c-4969-a413-eca93f2567c6","image_caption":"","keywords":["Decision making","Data visualization","Systematic literature review","Interviews","Automobiles","Taxonomy","MCDM","Focusing","Decision support systems","Visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2404.08812","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-09-0815]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full21","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Models, Methods, and Typologies","session_uid":"a23fb437-ab4c-4969-a413","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Models, Methods, and Typologies"],"time_stamp":"2025-11-07T09:06:00.000Z","title":"A Typology of Decision-Making Tasks for Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/moykjX8QeZI"},{"UID":"a4b21717-3ef3-4fdf-b25a-30b41b9f0ef9","abstract":"Many toolkit developers seek to streamline the visualization programming process through structured support such as prescribed templates and example galleries. However, few projects examine how users organize their own visualization programs and how their coding choices may deviate from the intents of toolkit developers, impacting visualization prototyping and design. Further, is it possible to infer users\u2019 reasoning indirectly through their code, even when users copy code from other sources? We explore this question through a qualitative analysis of 715 D3 programs on Observable. We identify three levels of program organization based on how users decompose their code into smaller blocks: Program-, Chart-, and Component-Level code decomposition, with a strong preference for Component-Level reasoning. In a series of interviews, we corroborate that these levels reflect how Observable users reason about visualization programs. We compare common user-made components with those theorized in the Grammar of Graphics to assess overlap in user and toolkit developer reasoning. We find that, while the Grammar of Graphics covers basic visualizations well, it falls short in describing complex visualization types, especially those with animation, interaction, and parameterization components. Our findings highlight how user practices differ from formal grammars and reinforce ongoing efforts to rethink visualization toolkit support, including augmenting learning tools and AI assistants to better reflect real-world coding strategies.","accessible_pdf":"Accessible","authors":[{"affiliation":"Carnegie Mellon University","email":"lin.family.folders@gmail.com","name":"Melissa Lin"},{"affiliation":"University of Washington","email":"heerpate@cs.washington.edu","name":"Heer Patel"},{"affiliation":"University of Washington","email":"mlamkin@cs.washington.edu","name":"Medina Lamkin"},{"affiliation":"University of Maryland","email":"hbako@virginia.edu","name":"Hannah Bako"},{"affiliation":"University of Washington","email":"leibatt@cs.washington.edu","name":"Leilani Battle"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a4b21717-3ef3-4fdf-b25a-30b41b9f0ef9","image_caption":"","keywords":["Visualization toolkits","Code reuse."],"open_access_supplemental_link":"https://osf.io/sudb8/?view_only=cc72bdc685804e478852a96297328eb8","open_access_supplemental_question":"We provided substantial supplemental of our transparent analysis practices and data. This allows for future reproducibility of this work.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2405.14341","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1052","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"a4b21717-3ef3-4fdf-b25a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T08:48:00.000Z","title":"How Do Observable Users Decompose D3 Code? A Qualitative Study","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"a5574ea7-437e-4283-b890-3cb980ca2e5f","abstract":"Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray coronary angiography (CA) remains a significant clinical problem. Existing CA reconstruction methods often require extensive user interaction or large training datasets. Recently, Neural Radiance Field (NeRF) has successfully reconstructed high-fidelity scenes in natural and medical contexts without these requirements. However, challenges such as sparse-views, intra-scan motion, and complex vessel morphology hinder its direct application to CA data. We introduce NeRF-CA, a first step toward a fully automatic 4D CA reconstruction that achieves reconstructions from sparse coronary angiograms. To the best of our knowledge, we are the first to address the challenges of sparse-views and cardiac motion by decoupling the scene into the moving coronary artery and the static background, effectively translating the problem of motion into a strength. NeRF-CA serves as a first stepping stone for solving the 4D CA reconstruction problem, achieving adequate 4D reconstructions from as few as four angiograms, as required by clinical practice, while significantly outperforming state-of-the-art sparse-view X-ray NeRF. We validate our approach quantitatively and qualitatively using representative 4D phantom datasets and ablation studies.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Kirsten W.H. Maas"},{"email":null,"name":"Danny Ruijters"},{"email":null,"name":"Anna Vilanova"},{"email":null,"name":"Nicola Pezzotti"}],"award":"","doi":"10.1109/TVCG.2025.3579162","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"a5574ea7-437e-4283-b890-3cb980ca2e5f","image_caption":"","keywords":["Neural radiance field","Image reconstruction","Angiography","Three-dimensional displays","Arteries","X-ray imaging","Dynamics","Imaging","Training data","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2408.16355","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0780]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full13","session_room":"Hall E2","session_room_id":"e2","session_title":"Fields, Fields, Fields","session_uid":"a5574ea7-437e-4283-b890","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Fields, Fields, Fields"],"time_stamp":"2025-11-05T10:51:00.000Z","title":"NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with Extremely Sparse-views","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mHta4w5FJQg"},{"UID":"a5a6c51f-9a92-459b-bbf9-0192d4c7f09f","abstract":"Compact visualizations are often used to detect expected and unexpected data characteristics at a glance. The literature uses the terms fingerprints, thumbnails, or sketches for these bite-sized visualizations. To date, these visualizations have not been described in context and their discussion is fragmented across publications. In this paper, we propose the term Visual Fingerprints for these visualizations, highlight the task of visual detection for which they are designed, outline their visual properties, and discuss their commonalities and differences with related concepts. In doing so, we aim to bundle the existing research on these visualizations into a common concept and term that serves as a foundation and stepping stone for future developments in this direction.","accessible_pdf":"Accessible","authors":[{"affiliation":"Aarhus University","email":"meredithc@cs.au.dk","name":"Meredith Siang-Yun Chou"},{"affiliation":"Aarhus University","email":"hjschulz@cs.au.dk","name":"Hans-J\u00f6rg Schulz"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a5a6c51f-9a92-459b-bbf9-0192d4c7f09f","image_caption":"","keywords":["Charts","Diagrams","and Plots"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1263","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"a5a6c51f-9a92-459b-bbf9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T10:24:00.000Z","title":"Visual Fingerprints for Detecting Data Characteristics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"a610d354-3cae-4df3-ac69-db5a5a7572a6","abstract":"Constructing expressive and legible visualizations is a key activity for visualization designers.While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/.","accessible_pdf":"Accessible","authors":[{"affiliation":"Aarhus University","email":"johannes@ellemose.eu","name":"Johannes Ellemose"},{"affiliation":"Aarhus University","email":"elm@cs.au.dk","name":"Niklas Elmqvist"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a610d354-3cae-4df3-ac69-db5a5a7572a6","image_caption":"","keywords":["Visualization complexity","visualization literacy","perception","crowdsourcing","LLMs."],"open_access_supplemental_link":"https://osf.io/w85a4/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1260","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full25","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception","session_uid":"a610d354-3cae-4df3-ac69","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception"],"time_stamp":"2025-11-06T13:00:00.000Z","title":"Eye of the Beholder: Towards Measuring Visualization Complexity","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/D_j0t7Fy0w0"},{"UID":"a744e0f2-d891-4fd6-b3a0-d6e2f9227708","abstract":"With the increasing complexity and diversity of climate models, climate scientists often turn to multiple tools to understand and assess relationships between observations and model output. To explore ways to overcome that workflow complexity, we partnered with the Climate Modeling Alliance (CliMA) to develop CliMAScope. CliMAScope is an open source, interactive, web-based land surface climate model visualization tool that aims to unify analysis, comparison, validation, and correlation mapping of observational data with climate land model output. This paper presents a design study of CliMAScope, and explains how the tool's visual and interactive elements support the goal of comparing seamless climate model outputs to observational data.","accessible_pdf":"Accessible","authors":[{"affiliation":"Havard University","email":"linhphaaam@gmail.com","name":"Linh Pham"},{"affiliation":"Harvard University","email":"inbox.kevin.hu@gmail.com","name":"Kevin Hu"},{"affiliation":"University of Wisconsin - Madison","email":"ntwhite@wisc.edu","name":"Nathan White"},{"affiliation":"ArtCenter College of Design","email":"junaline4@gmail.com","name":"Minyoung Joo"},{"affiliation":"California Institute of Technology","email":"arenchon@caltech.edu","name":"Alexandre Renchon"},{"affiliation":"California Institute of Technology","email":"tapio@caltech.edu","name":"Tapio Schneider"},{"affiliation":"Jet Propulsion Lab","email":"krys.t.blackwood@jpl.nasa.gov","name":"Krys Blackwood"},{"affiliation":"California Institute of Technology","email":"santiago@caltech.edu","name":"Santiago Lombeyda"},{"affiliation":"California Institute of Technology (Caltech)","email":"hmushkin@caltech.edu","name":"Hillary Mushkin"},{"affiliation":"California Institute of Technology","email":"sd@scottdavidoff.com","name":"Scott Davidoff"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a744e0f2-d891-4fd6-b3a0-d6e2f9227708","image_caption":"","keywords":["visual analytics","design study climate model"],"open_access_supplemental_link":"https://github.com/datavisprogram/climascope","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1113","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"a744e0f2-d891-4fd6-b3a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:54:00.000Z","title":"Visualizing Climate Model Outputs with CliMAScope","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"a79b8861-5ff1-41ab-ab84-c564496de8de","abstract":"Open-world object detection (OWOD) extends traditional object detection to identifying both known and unknown object, necessitating continuous model adaptation as new annotations emerge. Current approaches face significant limitations: 1) data-hungry training due to reliance on a large number of crowdsourced annotations, 2) susceptibility to partial feature overfitting, and 3) limited flexibility due to required model architecture modifications. To tackle these issues, we present OW-CLIP, a visual analytics system that provides curated data and enables data-efficient OWOD model incremental training. OW-CLIP implements plug-and-play multimodal prompt tuning tailored for OWOD settings and introduces a novel Crop-Smoothing technique to mitigate partial feature overfitting. To meet the data requirements for the training methodology, we propose dual-modal data refinement methods that leverage large language models and cross-modal similarity for data generation and filtering. Simultaneously, we develope a visualization interface that enables users to explore and deliver high-quality annotations\u2014including class-specific visual feature phrases and fine-grained differentiated images. Quantitative evaluation demonstrates that OW-CLIP achieves competitive performance at 89% of state-of-the-art performance while requiring only 3.8% self-generated data, while outperforming SOTA approach when trained with equivalent data volumes.  A case study shows the effectiveness of the developed method and the improved annotation quality of our visualization system.","accessible_pdf":null,"authors":[{"affiliation":"Central South University","email":"jwduan@csu.edu.cn","name":"Junwen Duan"},{"affiliation":"Central South University","email":"234711076@csu.edu.cn","name":"Wei Xue"},{"affiliation":"Central South University","email":"scv.kangziyao@gmail.com","name":"Ziyao Kang"},{"affiliation":"Tsinghua University","email":"shixia@tsinghua.edu.cn","name":"Shixia Liu"},{"affiliation":"Central South University","email":"xiajiazhi@csu.edu.cn","name":"Jiazhi Xia"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a79b8861-5ff1-41ab-ab84-c564496de8de","image_caption":"","keywords":["Open-world object detection","data-efficient supervision","large language model","human-AI collaboration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.19870","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1774","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full31","session_room":"Hall E1","session_room_id":"e1","session_title":"VA for AI","session_uid":"a79b8861-5ff1-41ab-ab84","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VA for AI"],"time_stamp":"2025-11-05T08:42:00.000Z","title":"OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LajPfGQe32k"},{"UID":"a8c54e73-a078-47b8-94b9-8b8dfda73721","abstract":"Data-rich documents are commonly found across various fields such as business, finance, and science. However, a general limitation of these documents for reading is their reliance on text to convey data and facts. Visual representation of text aids in providing a satisfactory reading experience in comprehension and engagement. However, existing work emphasizes presenting the insights within phrases or sentences, rather than fully conveying data stories within the whole paragraphs and engaging readers. To provide readers with satisfactory data stories, this paper presents Narrative Player, a novel method that automatically revives data narratives with consistent and contextualized visuals. Specifically, it accepts a paragraph and corresponding data table as input and leverages LLMs to characterize the clauses and extract contextualized data facts. Subsequently, the facts are transformed into a coherent visualization sequence with a carefully designed optimization-based approach. Animations are also assigned between adjacent visualizations to enable seamless transitions. Finally, the visualization sequence, transition animations, and audio narration generated by text-to-speech technologies are rendered into a data video. The evaluation results showed that the automatic-generated data videos were well-received by participants and experts for enhancing reading.","accessible_pdf":"No","authors":[{"email":null,"name":"Zekai Shao"},{"email":null,"name":"Leixian Shen"},{"email":null,"name":"Haotian Li"},{"email":null,"name":"Yi Shan"},{"email":null,"name":"Huamin Qu"},{"email":null,"name":"Yun Wang"},{"email":null,"name":"Siming Chen"}],"award":"","doi":"10.1109/TVCG.2025.3530512","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"a8c54e73-a078-47b8-94b9-8b8dfda73721","image_caption":"","keywords":["Data visualization","Visualization","Videos","Animation","Data mining","User experience","Rain","Electronic mail","Text to speech","Sequential analysis"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2410.03268","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0632]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full26","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Stories and the Journalists Who Tell Them","session_uid":"a8c54e73-a078-47b8-94b9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Stories and the Journalists Who Tell Them"],"time_stamp":"2025-11-07T09:18:00.000Z","title":"Narrative Player: Reviving Data Narratives with Visuals","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/j1bJGzUi0jM"},{"UID":"a93e5d17-6b6f-48f8-a943-8abfdd53c3c1","abstract":"Scatterplots are among the most popular visualization methods for bivariate or multivariate data. While scatterplots scale well with the number of samples, visual clutter cannot be avoided with increasing data size. Density regularization is a common approach to declutter scatterplots. However, most existing density-equalizing algorithms are restricted to rectangular domains, which limits their applicability. In particular, they cannot operate within interactive lenses, a widely used approach for interactive data exploration. We present a numerical approach that generalizes density-equalizing transformations to domains of arbitrary shape, including concave regions. The definition of the regions of interest can be data-driven or interactive. We demonstrate the effectiveness of our method by implementing adaptive and flexible interactive lenses for enhanced data exploration in scatterplots, showcasing its versatility and potential for broader application.","accessible_pdf":null,"authors":[{"affiliation":"University of M\u00fcnster","email":"molchano@uni-muenster.de","name":"Vladimir Molchanov"},{"affiliation":"University of M\u00fcnster","email":"hennes.rave@uni-muenster.de","name":"Hennes Rave"},{"affiliation":"University of M\u00fcnster","email":"linsen@uni-muenster.de","name":"Lars Linsen"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"a93e5d17-6b6f-48f8-a943-8abfdd53c3c1","image_caption":"","keywords":["Clutter reduction","scatterplots","density equalization","virtual lenses."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1338","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"a93e5d17-6b6f-48f8-a943","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T11:00:00.000Z","title":"A Decluttering Lens for Scatterplots","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"aa5d2960-377f-4f57-a6a1-0cb8c110af24","abstract":"Experiments in visualization perception demonstrate that people can perceive positions highly accurately. However, position encodings can be susceptible to systematic biases depending on the intrinsic properties of the visualized data, such as its shape and cognitive processes of memory and perception. Using line charts as a case study, we investigate how the shape of data, such as local and global extrema, can bias the perception and recall of average data values. In two studies, participants estimated the average data values in a line chart by adjusting a slider with their mouse. We found that participants\u2019 estimates were systematically biased toward the direction of the global extremum. When multiple salient extrema were present, estimates appeared influenced by several extrema simultaneously but ultimately leaned toward the global extremum. \nNotably, the strength of this bias varied depending on whether participants were perceiving or recalling the mean. This work advances our understanding of how extrema influence perception and memory, potentially exaggerating or underestimating critical trends and contributing to a skewed interpretation of data. These findings offer valuable guidance for the design of narrative visualization tools and data storytelling strategies.","accessible_pdf":null,"authors":[{"affiliation":"Emory University","email":"tsavalia@umass.edu","name":"Tejas Savalia"},{"affiliation":"US Naval Research Lab","email":"andrew.lovett@nrl.navy.mil","name":"Andrew Lovett"},{"affiliation":"Northwestern University","email":"crceja@u.northwestern.edu","name":"Cristina Ceja"},{"affiliation":"University of Colorado Boulder","email":"rosie.cowell@colorado.edu","name":"Rosemary Cowell"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"aa5d2960-377f-4f57-a6a1-0cb8c110af24","image_caption":"","keywords":["Data visualization","Memory","Perception","Shape","Global Maxima and Minima"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1332","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"aa5d2960-377f-4f57-a6a1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T15:48:00.000Z","title":"Global Extrema Bias Perception and Recall of Average Data Values in Line Charts","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"aab34439-1bda-4a07-916a-80e89f82ff62","abstract":"Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yuheng Zhao"},{"email":null,"name":"Junjie Wang"},{"email":null,"name":"Linbin Xiang"},{"email":null,"name":"Xiaowen Zhang"},{"email":null,"name":"Zifei Guo"},{"email":null,"name":"Cagatay Turkay"},{"email":null,"name":"Yu Zhang"},{"email":null,"name":"Siming Chen"}],"award":"","doi":"10.1109/TVCG.2024.3496112","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"aab34439-1bda-4a07-916a-80e89f82ff62","image_caption":"","keywords":["Data visualization","Planning","Data analysis","Data models","Visual analytics","Analytical models","Market research","Data mining","Collaboration","Adaptation models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2411.05651","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024.3496112]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full1","session_room":"Hall E1","session_room_id":"e1","session_title":"Agentic Visualization and Intelligent Systems","session_uid":"aab34439-1bda-4a07-916a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Agentic Visualization and Intelligent Systems"],"time_stamp":"2025-11-05T15:21:00.000Z","title":"LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/2z0_8lHbM04"},{"UID":"abf4110d-55e9-4824-875f-5b8a60547704","abstract":"Data visualization tasks often require multi-step reasoning, and the interpretive strategies experts use-such as decomposing complex goals into smaller subtasks and selectively attending to key chart regions-are rarely made explicit. ViStruct is an automated pipeline that simulates these expert behaviours by breaking high-level visual questions into structured analytic steps and highlighting semantically relevant chart areas. Leveraging large language and vision-language models, ViStruct identifies chart components, maps subtasks to spatial regions, and presents visual attention cues to externalize expert-like reasoning flows. While not designed for direct novice instruction, ViStruct provides a replicable model of expert interpretation that can inform the development of future visual literacy tools. We evaluate the system on 45 tasks across 12 chart types and validate its outputs with trained visualization users, confirming its ability to produce interpretable and expert-aligned reasoning sequences.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Toronto","email":"oliver@dgp.toronto.edu","name":"Oliver Huang"},{"affiliation":"University of Toronto","email":"cnobre@cs.toronto.edu","name":"Carolina Nobre"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"abf4110d-55e9-4824-875f-5b8a60547704","image_caption":"","keywords":["Data Visualization","Task Decomposition","Large Language Models(LLMs)","Guidance System","Computer Vision"],"open_access_supplemental_link":"https://github.com/hivelabuoft/ViStruct","open_access_supplemental_question":"Our system and code are publicly available as an open-source interactive platform at https://vi-struct.vercel.app, including annotated examples, chart inputs, and model outputs. We have also open-sourced the expert review evaluation data used in our study to support transparency and facilitate future comparative work.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2506.21762","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1016","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"abf4110d-55e9-4824-875f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T08:39:00.000Z","title":"ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"ad323fdf-97c1-4e09-a814-a643243d280b","abstract":"Political sectarianism is fueled in part by misperceptions of political opponents: People commonly overestimate the support for extreme policies among members of the other party. These misperceptions inflame partisan animosity and may be used to justify extremism among one\u2019s own party. Research suggests that correcting partisan misperceptions\u2014by informing people about the actual views of outparty members\u2014may reduce one\u2019s own expressed support for political extremism, including partisan violence and anti-democratic actions. However, there remains a limited understanding of how the design of correction interventions drives these effects. The present study investigated how correction effects depend on different representations of outparty views communicated through data visualizations. Building on prior interventions that present the average outparty view, we consider the impact of visualizations that more fully convey the range of views among outparty members. We conducted an experiment with U.S.-based participants from Prolific (N=239 Democrats, N=244 Republicans). Participants made predictions about support for political violence and undemocratic practices among members of their political outparty. They were then presented with data from an earlier survey on the actual views of outparty members. Some participants viewed only the average response (Mean-Only condition), while other groups were shown visual representations of the range of views from 75% of the outparty (Mean+Interval condition) or the full distribution of responses (Mean+Points condition). Compared to a control group that was not informed about outparty views, we observed the strongest correction effects (i.e., lower support for political violence and undemocratic practices) among participants in the Mean-only and Mean+Points condition, while correction effects were weaker in the Mean+Interval condition. In addition, participants who observed the full distribution of out-party views (Mean+Points condition) were most accurate at later recalling the degree of support among the outparty. Our findings suggest that data visualizations can be an important tool for correcting pervasive distortions in beliefs about other groups. However, the way in which variability in outparty views is visualized can significantly shape how people interpret and respond to corrective information.","accessible_pdf":null,"authors":[{"affiliation":"University of North Carolina at Charlotte","email":"dmarkant@uncc.edu","name":"Doug Markant"},{"affiliation":"University of North Carolina at Charlotte","email":"ssah1@uncc.edu","name":"Subham Sah"},{"affiliation":"Simon Fraser University","email":"akarduni@sfu.ca","name":"Alireza Karduni"},{"affiliation":"University of North Carolina at Charlotte","email":"mrogha@uncc.edu","name":"Milad Rogha"},{"affiliation":"University of Florida","email":"mythai@cise.ufl.edu","name":"My Thai"},{"affiliation":"UNC Charlotte","email":"wdou1@uncc.edu","name":"Wenwen Dou"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ad323fdf-97c1-4e09-a814-a643243d280b","image_caption":"","keywords":["uncertainty visualization","political communication","partisan misperception"],"open_access_supplemental_link":"https://osf.io/8crsp/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2508.00233","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1695","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full26","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Stories and the Journalists Who Tell Them","session_uid":"ad323fdf-97c1-4e09-a814","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Stories and the Journalists Who Tell Them"],"time_stamp":"2025-11-07T09:30:00.000Z","title":"Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/j1bJGzUi0jM"},{"UID":"adaa08c8-ae8e-42b3-aeb5-506a2afe1dd1","abstract":"Accessibility is a foundational principle of effective visualization design, yet achieving perceptual accessibility remains a challenge for both experts and non-experts. Standards like the WCAG2 contrast ratio are intended to guide designers toward accessible color use but are increasingly misapplied as oversimplified and misunderstood shortcuts for achieving perceptual accessibility. This misuse highlights the need for more nuanced, perceptually grounded evaluation tools. In response, we present our Modular Accessible Perceptibility (MAP) contrast model and tool, which integrates advanced color appearance models to assess perceptibility and flag risks associated with simultaneous contrast. By augmenting the CAM16-UCS model with a CAM16 CCz simultaneous chromatic contrast calculation, our model can isolate and measure the predicted shift resulting from simultaneous contrast along the chroma axis. This enables a more refined evaluation of visual contrast that better reflects the complexities of human color perception. Additionally, our tool performs modularly segmented automated checks to assign MAP contrast index scores, offering a holistic assessment of a map\u2019s visual accessibility. This paper details the development and validation of the MAP model and tool, underscoring accessibility and inclusive design as fundamental rights, and demonstrating their utility in advancing perceptual accessibility in geovisualization.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Wisconsin-Madison","email":"limpisathian@wisc.edu","name":"P. William Limpisathian"},{"affiliation":"University of Wisconsin - Madison","email":"sccox@wisc.edu","name":"Susannah Cox"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"adaa08c8-ae8e-42b3-aeb5-506a2afe1dd1","image_caption":"","keywords":["accessibility","perception","cartography","design","WCAG","CIE","color appearance model"],"open_access_supplemental_link":"https://neurocarto.geography.wisc.edu/2025/05/30/maptoolkit/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1314","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"adaa08c8-ae8e-42b3-aeb5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T15:21:00.000Z","title":"Evolving CAM16-UCS for Modular Accessible Perceptibility and Simultaneous Contrast Detection Tool for Geovisualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"ae49090b-23a7-453b-8d5d-eb0ad8893a08","abstract":"Graph querying is the process of retrieving information from graph data using specialized languages (e.g., Cypher), often requiring programming expertise. Visual Graph Querying (VGQ) streamlines this process by enabling users to construct and execute queries via an interactive interface without resorting to complex coding. However, current VGQ tools only allow users to construct simple and specific query graphs, limiting users' ability to interactively express their query intent, especially for underspecified query intent. To address these limitations, we propose Envisage, an interactive visual graph querying system to enhance the expressiveness of VGQ in complex query scenarios by supporting intuitive graph structure construction and flexible parameterized rule specification. Specifically, Envisage comprises four stages: Query Expression allows users to interactively construct graph queries through intuitive operations; Query Verification enables the validation of constructed queries via rule verification and query instantiation; Progressive Query Execution can progressively execute queries to ensure meaningful querying results; and Result Analysis facilitates result exploration and interpretation. To evaluate Envisage, we conducted two case studies and in-depth user interviews with 14 graph analysts. The results demonstrate its effectiveness and usability in constructing, verifying, and executing complex graph queries.","accessible_pdf":"Accessible","authors":[{"affiliation":"Nanyang Technological University","email":"xiaolin004@e.ntu.edu.sg","name":"Xiaolin Wen"},{"affiliation":"Monash University","email":"qishuang.fu@monash.edu","name":"Qishuang Fu"},{"affiliation":"Nanyang Technological University","email":"shuangyu001@e.ntu.edu.sg","name":"Shuangyue Han"},{"affiliation":"Nanyang Technological University","email":"yguo017@e.ntu.edu.sg","name":"Yichen Guo"},{"affiliation":"Monash University","email":"joseph.liu@monash.edu","name":"Joseph Liu"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ae49090b-23a7-453b-8d5d-eb0ad8893a08","image_caption":"","keywords":["Visual graph querying","interactive query construction","graph data"],"open_access_supplemental_link":"https://github.com/Selvalim/VGQ-front","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.11999","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1567","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full17","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Graphs and Networks","session_uid":"ae49090b-23a7-453b-8d5d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Graphs and Networks"],"time_stamp":"2025-11-05T14:45:00.000Z","title":"Envisage: Towards Expressive Visual Graph Querying","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zH5jvaUZHGk"},{"UID":"aeb6a938-4d06-4c0d-828a-6152918115c3","abstract":"Dashboards remain ubiquitous tools for analyzing data and disseminating the findings. Understanding the range of dashboard designs, from simple to complex, can support development of authoring tools that enable end-users to meet their analysis and communication goals. Yet, there has been little work that provides a quantifiable, systematic, and descriptive overview of dashboard design patterns. Instead, existing approaches only consider a handful of designs, which limits the breadth of patterns that can be surfaced. More quantifiable approaches, inspired by machine learning (ML), are presently limited to single visualizations or capture narrow features of dashboard designs. To address this gap, we present an approach for modeling the content and composition of dashboards using a graph representation. The graph decomposes dashboard designs into nodes featuring content \u201cblocks\u2019; and uses edges to model \u201crelationships\u201d, such as layout proximity and interaction, between nodes. To demonstrate the utility of this approach, and its extension over prior work, we apply this representation to derive a census of 25,620 dashboards from Tableau Public, providing a descriptive overview of the core building blocks of dashboards in the wild and summarizing prevalent dashboard design patterns. We discuss concrete applications of both a graph representation for dashboard designs and the resulting census to guide the development of dashboard authoring tools, making dashboards accessible, and for leveraging AI/ML techniques. Our findings underscore the importance of meeting users where they are by broadly cataloging dashboard designs, both common and exotic.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Arjun Srinivasan"},{"email":null,"name":"Joanna Purich"},{"email":null,"name":"Michael Correll"},{"email":null,"name":"Leilani Battle"},{"email":null,"name":"Vidya Setlur"},{"email":null,"name":"Anamaria Crisan"}],"award":"","doi":"10.1109/TVCG.2024.3490259","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"aeb6a938-4d06-4c0d-828a-6152918115c3","image_caption":"","keywords":["Data visualization","Feature extraction","Manuals","Visualization","Authoring systems","Layout","Annotations","Training","Encoding","Systematics"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Large dataset of dashboards that we released","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3490259","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full9","session_room":"Hall M2","session_room_id":"m2","session_title":"Dastardly Dashboards","session_uid":"aeb6a938-4d06-4c0d-828a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dastardly Dashboards"],"time_stamp":"2025-11-05T09:18:00.000Z","title":"From Dashboard Zoo to Census: A Case Study With Tableau Public","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/Ga7NSmKd54M"},{"UID":"af26160d-884d-4338-b93f-a0ef201cc6c4","abstract":"Network visualizations are understudied in graphical perception. As a result, most network visualization designs still largely rely on designer intuition and algorithm optimizations rather than being guided by knowledge of human perception. The lack of perceptual understanding of network visualizations also limits the generalizability of past empirical evaluations, given their focus on performance over causal interpretation. To bridge this gap between perception and network visualization, we introduce a framework highlighting five key perceptual mechanisms used in node-link diagrams and adjacency matrices: attention, visual search, perceptual organization, ensemble coding, and object recognition. Our framework describes the role these perceptual mechanisms play in common network analytical tasks. We use the framework to revisit four past empirical investigations and outline future design experiments that can help produce more perceptually effective network visualizations. We anticipate this connection will afford translational understanding to guide more effective network visualization design and offer hypotheses for perception-aware network visualizations.","accessible_pdf":"Yes","authors":[{"email":null,"name":"S. Sandra Bae"},{"email":null,"name":"Kyle Cave"},{"email":null,"name":"Carsten G\u00f6rg"},{"email":null,"name":"Paul Rosen"},{"email":null,"name":"Danielle Szafir"},{"email":null,"name":"Cindy Xiong Bearfield"}],"award":"","doi":"10.1109/TVCG.2025.3541571","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"af26160d-884d-4338-b93f-a0ef201cc6c4","image_caption":"","keywords":["Visualization","Layout","Data visualization","Guidelines","Electronic mail","Clustering algorithms","Accuracy","Training","Taxonomy","Surveys"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-09-0828.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full17","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Graphs and Networks","session_uid":"af26160d-884d-4338-b93f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Graphs and Networks"],"time_stamp":"2025-11-05T15:09:00.000Z","title":"Bridging Network Science and Vision Science: Mapping Perceptual Mechanisms to Network Visualization Tasks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zH5jvaUZHGk"},{"UID":"b00813fe-c0fb-4bdb-8040-b6e338088247","abstract":"Humans inherently connect certain colors with particular concepts in semantically meaningful ways that facilitate visual communication. These colors are known as semantically resonant colors. For instance, we associate \u201csky\u201d and \u201cocean\u201d with shades of blue, and \u201ccherry\u201d with red. In this paper, we investigate how language models, including Word2Vec, RoBERTa, GPT-4o mini and the vision language model CLIP generate and represent nuanced semantically resonant colors for diverse concepts. To achieve this, we utilized a large dataset of color names and concepts, tailored models for the structure of each language model, and developed an interactive web interface, CONCEPT2COLOR, as a use case. Additionally, we conducted experiments and a detailed analysis to assess the ability of these models to generate meaningful colors. Through these experiments, we examined how factors such as model design, training data and context affect the color output. Our findings reveal the capabilities and limitations of language models in processing and generating semantically resonant colors for concepts, thus contributing insights into how they depict semantic color-concept connections. These insights have implications for data visualization, design, and human-computer interaction, where leveraging effective semantic color generation can enhance communication and user experience.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"ssalimaunti@cs.stonybrook.edu","name":"Shahreen Salim"},{"affiliation":"Stony Brook University","email":"tpial@cs.stonybrook.edu","name":"Tanzir Pial"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b00813fe-c0fb-4bdb-8040-b6e338088247","image_caption":"","keywords":["Tabular Data","Text/Document Data","Datasets","Methodologies","Software Prototype","Domain Agnostic","Color Machine Learning Techniques"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1506","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full27","session_room":"Hall M2","session_room_id":"m2","session_title":"The Color and the Shape","session_uid":"b00813fe-c0fb-4bdb-8040","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The Color and the Shape"],"time_stamp":"2025-11-06T08:42:00.000Z","title":"What is the Color of Serendipity? Investigating the Use of Language Models for Semantically Resonant Color Generation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gHCENa3C9X4"},{"UID":"b0a64ecf-d533-44dc-a3be-5f9194ce91d7","abstract":"Spatial time series visualization offers scientific research pathways and analytical decision-making tools across various spatiotemporal domains. Despite many advanced methodologies, the seamless integration of temporal and spatial information remains a challenge. The space-time cube (STC) stands out as a promising approach for the synergistic presentation of spatial and temporal information, with successful applications across various spatiotemporal datasets. However, the STC is plagued by well-known issues such as visual occlusion and depth ambiguity, which are further exacerbated when dealing with large-scale spatial time series data. In this study, we introduce a novel technical framework termed VolumeSTCube, designed for continuous spatiotemporal phenomena. It first leverages the concept of the STC to transform discretely distributed spatial time series data into continuously volumetric data. Subsequently, volume rendering and surface rendering techniques are employed to visualize the transformed volumetric data. Volume rendering is utilized to mitigate visual occlusion, while surface rendering provides pattern details by enhanced lighting information. Lastly, we design interactions to facilitate the exploration and analysis from temporal, spatial, and spatiotemporal perspectives. VolumeSTCube is evaluated through a computational experiment, a real-world case study with one expert, and a controlled user study with twelve non-experts, compared against a baseline from prior work, showing its superiority and effectiveness in large-scale spatial time series analysis.","accessible_pdf":"No","authors":[{"email":null,"name":"Zikun Deng"},{"email":null,"name":"Jiabao Huang"},{"email":null,"name":"Chenxi Ruan"},{"email":null,"name":"Jialing Li"},{"email":null,"name":"Shaowu Gao"},{"email":null,"name":"Yi Cai"}],"award":"","doi":"10.1109/TVCG.2025.3537115","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"b0a64ecf-d533-44dc-a3be-5f9194ce91d7","image_caption":"","keywords":["Spatiotemporal phenomena","Data visualization","Visualization","Time series analysis","Rendering (computer graphics)","Three-dimensional displays","Switches","Costs","Scalability","Market research"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.09917","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0657.R2]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full20","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Maps & Spatial Vis","session_uid":"b0a64ecf-d533-44dc-a3be","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Maps & Spatial Vis"],"time_stamp":"2025-11-05T09:30:00.000Z","title":"Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/nkeAR0PgfSw"},{"UID":"b12be8f4-754d-4d42-af8f-9c58c2cebf47","abstract":"Visual archives of political movements are rich cultural resources, yet often difficult to explore at scale due to complex visual semantics and limited interaction models. We present Posterity, an interactive visualization system for 784 digitized American labor posters (1900\u20132010), designed to support both historical contextualization and visual-semantic exploration. Posterity integrates curated metadata, CLIP-based multimodal embeddings, and unsupervised clustering to offer three coordinated views: a timeline aligned with key labor events, a 3D semantic cloud, and a similarity spiral responsive to image-, object-, or gesture-based input. Together, these views enable users to trace recurring visual motifs, discover rhetorical patterns, and explore labor movement narratives from multiple entry points. While developed for labor posters, the approach demonstrates potential for adaptation to other visual cultural heritage collections,\nparticularly those with rich metadata and symbolic content.","accessible_pdf":"Accessible","authors":[{"affiliation":"Havard University","email":"linhphaaam@gmail.com","name":"Linh Pham"},{"affiliation":"Harvard University","email":"daniel2rodrig@gmail.com","name":"Daniel Rodriguez-Rodriguez"},{"affiliation":"Harvard University","email":"jhuang10@pratt.edu","name":"Jingfei Huang"},{"affiliation":"Harvard University","email":"hysuk012@gmail.com","name":"Hui-Ying Suk"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b12be8f4-754d-4d42-af8f-9c58c2cebf47","image_caption":"","keywords":["digital humanities","image visualization","image interpretation","historical records","similarity retrieval"],"open_access_supplemental_link":"https://github.com/linhphaaam/posterity","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1286","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short4","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Explorations in Abstract and Physical Spaces","session_uid":"b12be8f4-754d-4d42-af8f","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explorations in Abstract and Physical Spaces"],"time_stamp":"2025-11-05T13:09:00.000Z","title":"Posterity: Balancing historical context and visual dynamism while visualizing a collection of American labor posters","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/pZq0H_l-8Bs"},{"UID":"b1e09d93-9979-43b1-b701-0cf201d500d6","abstract":"Implicit neural representations (INRs) have emerged as a powerful tool for compressing large-scale volume data. This opens up new possibilities for in situ visualization. However, the efficient application of INRs to distributed data remains an underexplored area. In this work, we develop a distributed volumetric neural representation and optimize it for in situ visualization. Our technique eliminates data exchanges between processes, achieving state-of-the-art compression speed, quality and ratios. Our technique also enables the implementation of an efficient strategy for caching large-scale simulation data in high temporal frequencies, further facilitating the use of reactive in situ visualization in a wider range of scientific problems. We integrate this system with the Ascent infrastructure and evaluate its performance and usability using real-world simulations.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Qi Wu"},{"email":null,"name":"Joseph A. Insley"},{"email":null,"name":"Victor A. Mateevitsi"},{"email":null,"name":"Silvio Rizzi"},{"email":null,"name":"Michael E. Papka"},{"email":null,"name":"Kwan-Liu Ma"}],"award":"","doi":"10.1109/TVCG.2024.3432710","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"b1e09d93-9979-43b1-b701-0cf201d500d6","image_caption":"","keywords":["Data visualization","Data models","Computational modeling","Training","Adaptation models","Neural networks","Programming"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2304.10516","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3432710","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full13","session_room":"Hall E2","session_room_id":"e2","session_title":"Fields, Fields, Fields","session_uid":"b1e09d93-9979-43b1-b701","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Fields, Fields, Fields"],"time_stamp":"2025-11-05T11:15:00.000Z","title":"Distributed Neural Representation for Reactive in situ Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mHta4w5FJQg"},{"UID":"b2ea7546-bba9-4e9e-9d93-b7e43b147009","abstract":"We introduce FlexPhys, a cookbook that researchers can use to operationalize data physicalization research questions through workshop design. While guidelines exist for running workshops in educational contexts, designing a data physicalization workshop when the goal is to answer research questions is an ad-hoc process for which little guidance exists, but for which many choices must be made (e.g., in terms of materials, tools, and data). We draw from our experience designing data physicalization workshops and from reviewing three existing workshops, to distill the cookbook's core ingredient (context and goal) and eight additional ingredients related to making (material, tool, technique), data encoding (data type, variable, mark/unit), and interactivity (interaction, sensory modality). We then show how FlexPhys can be used to describe and compare physicalization workshops, and to generate workshops that address specific data physicalization research questions.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Victoria","email":"bahare.bakhtiari97@gmail.com","name":"Bahare Bakhtiari"},{"affiliation":"Universit\u00e9 Claude Bernard Lyon 1","email":"aurelien@tabard.fr","name":"Aur\u00e9lien Tabard"},{"affiliation":"University of Victoria","email":"sowmya.somanath@gmail.com","name":"Sowmya Somanath"},{"affiliation":"University of Victoria","email":"cperin@uvic.ca","name":"Charles Perin"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b2ea7546-bba9-4e9e-9d93-b7e43b147009","image_caption":"","keywords":["data physicalization","methods","workshops"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://hal.science/hal-05151375","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1057","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"b2ea7546-bba9-4e9e-9d93","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T09:15:00.000Z","title":"FlexPhys: A Workshop Cookbook for Operationalizing Data Physicalization Research Questions","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"b3c081c3-338e-4dd1-8d57-3a35f981facc","abstract":"Traditional instance-based model analysis focuses mainly on misclassified instances. However, this approach overlooks the varying difficulty associated with different instances. Ideally, a robust model should recognize and reflect the challenges presented by intrinsically difficult instances. It is also valuable to investigate whether the difficulty perceived by the model aligns with that perceived by humans. To address this, we propose incorporating instance difficulty into the deep neural network evaluation process, specifically for supervised classification tasks on image data. Specifically, we consider difficulty measures from three perspectives -- data, model, and human -- to facilitate comprehensive evaluation and comparison. Additionally, we develop an interactive visual tool, DifficultyEyes, to support the identification of instances of interest based on various difficulty patterns and to aid in analyzing potential data or model issues. Case studies demonstrate the effectiveness of our approach.","accessible_pdf":null,"authors":[{"affiliation":"Eindhoven University of Technology","email":"l.meng1@tue.nl","name":"Linhao Meng"},{"affiliation":"Eindhoven University of Technology","email":"s.j.v.d.elzen@tue.nl","name":"Stef van den Elzen"},{"affiliation":"Eindhoven University of Technology","email":"a.vilanova@tue.nl","name":"Anna Vilanova"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b3c081c3-338e-4dd1-8d57-3a35f981facc","image_caption":"","keywords":["Visualization","deep neural network","difficulty"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://doi.org/10.48550/arXiv.2507.00881","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1194","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"b3c081c3-338e-4dd1-8d57","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T10:33:00.000Z","title":"Towards Difficulty-Aware Analysis of Deep Neural Networks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"b43c7d8b-fdda-4a75-818d-e1fb4e4dfe31","abstract":"Linear embeddings support interactive visual exploration by mapping high-dimensional (nD) data into a two-dimensional\nspace. Despite their popularity, selecting meaningful projection parameters remains a key challenge due to the infi nite 2n-dimensional\nparameter space. Once an informative projection is found, users often seek similar ones that emphasize specifi c items differently\nwhile preserving global structure. For instance: Do clusters become outliers under slight changes? Can grouped items separate\u2014or\nmerge\u2014through parameter adjustments? Which changes to the embedding parameters lead to such projections \u2014 and do they exist\nat all? Answering these questions effi ciently is critical for effective visual search. Yet, current methods\u2014such as projection tours or\nmanual parameter tuning\u2014are time-consuming and risk overlooking important views, including those of specifi c interest. We propose\nComposition Operators, a mathematical foundation for a novel set-of-point manipulation concept for linear embeddings\u2014such as Star\nCoordinates\u2014as an alternative approach to selecting informative embedding parameters in a more controllable manner with respect to\nthe desired outcome. Users specify item-based constraints on the projection result; the corresponding 2n parameters are then derived\nautomatically, eliminating the need to exhaustively search the entire parameter space to get a similar outcome. Neither the embedding\nspace nor the set of parameters is altered \u2013 only the mechanism for navigating and selecting parameters is redefi ned. We provide\nclosed-form solutions for this and demonstrate our interactive prototype on nD datasets from the UCI repository.","accessible_pdf":"Accessible","authors":[{"affiliation":"Ostfalia University of Applied Sciences","email":"di.lehmann@ostfalia.de","name":"Dirk Lehmann"},{"affiliation":"Institute for Information Engineering","email":"kaiblum95@web.de","name":"Kai M. Blum"},{"affiliation":"Universidad Rey Juan Carlos","email":"manuel.rubio@urjc.es","name":"Manuel Rubio-S\u00e1nchez"},{"affiliation":"Robert Bosch GmbH, Stuttgart, GERMANY","email":"konrad.simon2@de.bosch.com","name":"Konrad Simon"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b43c7d8b-fdda-4a75-818d-e1fb4e4dfe31","image_caption":"","keywords":["Star Coordinates","Multivariate Projections","Composition Operators","Multidimensional Data"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2004","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full11","session_room":"Hall E2","session_room_id":"e2","session_title":"Embeddings & Metrics","session_uid":"b43c7d8b-fdda-4a75-818d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Embeddings & Metrics"],"time_stamp":"2025-11-05T13:36:00.000Z","title":"Interactive Composition Operators: An Alternative Approach for Selecting Linear Embedding Parameters","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gvbjZE1CgLQ"},{"UID":"b56f8c0d-ce95-474d-9c61-f650be7278d5","abstract":"Temporal graphs are commonly used to represent complex systems and track the evolution of their constituents over time. Visualizing these graphs is crucial as it allows one to quickly identify anomalies, trends, patterns, and other properties that facilitate better decision-making. In this context, selecting an appropriate temporal resolution is essential for constructing and visually analyzing the layout. The choice of resolution is particularly important, especially when dealing with temporally sparse graphs. In such cases, changing the temporal resolution by grouping events (i.e., edges) from consecutive timestamps \u2014 a technique known as timeslicing \u2014 can aid in the analysis and reveal patterns that might not be discernible otherwise. However, selecting an appropriate temporal resolution is a challenging task. In this paper, we propose ZigzagNetVis, a methodology that suggests temporal resolutions potentially relevant for analyzing a given graph, i.e., resolutions that lead to substantial topological changes in the graph structure. ZigzagNetVis achieves this by leveraging zigzag persistent homology, a well-established technique from Topological Data Analysis (TDA). To improve visual graph analysis, ZigzagNetVis incorporates the colored barcode, a novel timeline-based visualization inspired by persistence barcodes commonly used in TDA. We also contribute with a web-based system prototype that implements suggestion methodology and visualization tools. Finally, we demonstrate the usefulness and effectiveness of ZigzagNetVis through a usage scenario, a user study with 27 participants, and a detailed quantitative evaluation.","accessible_pdf":"No","authors":[{"email":null,"name":"Raphael Tinarrage"},{"email":null,"name":"Jean R. Ponciano"},{"email":null,"name":"Claudio D. G. Linhares"},{"email":null,"name":"Agma J. M. Traina"},{"email":null,"name":"Jorge Poco"}],"award":"","doi":"10.1109/TVCG.2025.3528197","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"b56f8c0d-ce95-474d-9c61-f650be7278d5","image_caption":"","keywords":["Visualization","Filtration","Layout","Prototypes","Market research","Data analysis","Computer science","Complex systems","Surveys","Standards"],"open_access_supplemental_link":null,"open_access_supplemental_question":"particularly substantial supplemental material","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2304.03828","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-06-0415 / TVCG3528197]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full24","session_room":"Hall M2","session_room_id":"m2","session_title":"Ordering and Layout","session_uid":"b56f8c0d-ce95-474d-9c61","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Ordering and Layout"],"time_stamp":"2025-11-05T14:00:00.000Z","title":"ZigzagNetVis: Suggesting temporal resolutions for graph visualization using zigzag persistence","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/1kLUiSSxIuM"},{"UID":"b5d2c402-def7-4b9d-a6df-ac4ad7572111","abstract":"Connectomics, a subfield of neuroscience, aims to map and analyze synapse-level wiring diagrams of the nervous system. While recent advances in deep learning have accelerated automated neuron and synapse segmentation, reconstructing accurate connectomes still demands extensive human proofreading to correct segmentation errors.\nWe present SynAnno, an interactive tool designed to streamline and enhance the proofreading of synaptic annotations in large-scale connectomics datasets. SynAnno integrates into existing neuroscience workflows by enabling guided, neuron-centric proofreading. To address the challenges posed by the complex spatial branching of neurons, it introduces a structured workflow with an optimized traversal path and a 3D mini-map for tracking progress. In addition, SynAnno incorporates fine-tuned machine learning models to assist with error detection and correction, reducing the manual burden and increasing proofreading efficiency.\nWe evaluate SynAnno through a user and case study involving seven neuroscience experts. Results show that SynAnno significantly accelerates synapse proofreading while reducing cognitive load and annotation errors through structured guidance and visualization support. The source code and interactive demo are available at: https://github.com/PytorchConnectomics/SynAnno.","accessible_pdf":"Accessible","authors":[{"affiliation":"Technical University of Munich","email":"leander.lauenburg@gmail.com","name":"Leander Lauenburg"},{"affiliation":"Harvard University","email":"jakob.troidl@googlemail.com","name":"Jakob Troidl"},{"affiliation":"Boston College","email":"gohaina@bc.edu","name":"Adam Gohain"},{"affiliation":"Harvard University","email":"linzudi@gmail.com","name":"Zudi Lin"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"},{"affiliation":"Boston College","email":"donglai.wei@bc.edu","name":"Donglai Wei"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b5d2c402-def7-4b9d-a6df-ac4ad7572111","image_caption":"","keywords":["Connectomics","Synaptic Annotations","Neuron-Centric","Proofreading Workflow"],"open_access_supplemental_link":"https://github.com/PytorchConnectomics/SynAnno","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1718","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full24","session_room":"Hall M2","session_room_id":"m2","session_title":"Ordering and Layout","session_uid":"b5d2c402-def7-4b9d-a6df","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Ordering and Layout"],"time_stamp":"2025-11-05T13:12:00.000Z","title":"SynAnno: Interactive Guided Proofreading of Synaptic Annotations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/1kLUiSSxIuM"},{"UID":"b5fec96f-039e-4af2-93d6-898b9a2caece","abstract":"Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors\u2019 essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA\u2019s functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.","accessible_pdf":"Accessible","authors":[{"affiliation":"Kobe University","email":"237x012x@stu.kobe-u.ac.jp","name":"Naoki Okami"},{"affiliation":"Kobe University","email":"kazuzaka53@gmail.com","name":"Kazuki Miyake"},{"affiliation":"Kobe University","email":"naohisa.sakamoto@people.kobe-u.ac.jp","name":"Naohisa Sakamoto"},{"affiliation":"RIKEN Center for Computational Science","email":"jorji@riken.jp","name":"Jorji Nonaka"},{"affiliation":"Link\u00f6ping University","email":"tfujiwara@ucdavis.edu","name":"Takanori Fujiwara"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b5fec96f-039e-4af2-93d6-898b9a2caece","image_caption":"","keywords":["Tensor decomposition","tensor analysis","contrastive learning","dimensionality reduction","interpretability","supercomputer"],"open_access_supplemental_link":"https://github.com/vizlab-kobe/tulca","open_access_supplemental_question":"We provide an open-source library and an open-source visual interface, with easy installation and thoroughly documented source code (following the Python standard). In addition, to help replicate our results, we provide datasets and source code used for evaluations.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.19988","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2057","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full31","session_room":"Hall E1","session_room_id":"e1","session_title":"VA for AI","session_uid":"b5fec96f-039e-4af2-93d6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VA for AI"],"time_stamp":"2025-11-05T08:54:00.000Z","title":"Visual Analytics Using Tensor Unified Linear Comparative Analysis","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LajPfGQe32k"},{"UID":"b71e9ec2-8579-410a-94c0-a9e2812a2712","abstract":"The detection and analysis of features in fluid flow are important tasks in fluid mechanics and flow visualization. One recent class of methods to approach this problem is to first compute objective optimal reference frames, relative to which the input vector field becomes as steady as possible. However, existing methods either optimize locally over a fixed neighborhood, which might not match the extent of interesting features well, or perform global optimization, which is costly. We propose a novel objective method for the computation of optimal reference frames that automatically adapts to the flow field locally, without having to choose neighborhoods a priori. We enable adaptivity by formulating this problem as a moving least squares approximation, through which we determine a continuous field of reference frames. To incorporate fluid features into the computation of the reference frame field, we introduce the use of a scalar guidance field into the moving least squares approximation. The guidance field determines a curved manifold on which a regularly sampled input vector field becomes a set of irregularly spaced samples, which then forms the input to the moving least squares approximation. Although the guidance field can be any scalar field, by using a field that corresponds to flow features the resulting reference frame field will adapt accordingly. We show that using an FTLE field as the guidance field results in a reference frame field that adapts better to local features in the flow than prior work. However, our moving least squares framework is formulated in a very general way, and therefore other types of guidance fields could be used in the future to adapt to local fluid features.","accessible_pdf":null,"authors":[{"affiliation":"King Abdullah University of Science and Technology - KAUST","email":"julio.reyramirez@kaust.edu.sa","name":"Julio Rey Ramirez"},{"affiliation":"KAUST","email":"peter.rautek@kaust.edu.sa","name":"Peter Rautek"},{"affiliation":"Friedrich-Alexander-University Erlangen-N\u00fcrnberg","email":"tobias.guenther@fau.de","name":"Tobias G\u00fcnther"},{"affiliation":"KAUST","email":"markus.hadwiger@kaust.edu.sa","name":"Markus Hadwiger"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b71e9ec2-8579-410a-94c0-a9e2812a2712","image_caption":"","keywords":["Scientific visualization","unsteady flow","reference frame optimization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1172","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full14","session_room":"Hall M1","session_room_id":"m1","session_title":"Flow and Topology","session_uid":"b71e9ec2-8579-410a-94c0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Flow and Topology"],"time_stamp":"2025-11-05T14:45:00.000Z","title":"Locally Adapted Reference Frame Fields using Moving Least Squares","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ar3ziIYBxhc"},{"UID":"b75e4e38-595d-4c86-a824-7251ec305f19","abstract":"This article introduces constraint-based breakpoints, a technique for designing responsive visualizations for a wide variety of screen sizes and datasets. Breakpoints in responsive visualization define when different visualization designs are shown. Conventionally, breakpoints are static, pre-defined widths, and as such do not account for changes to the visualized dataset or visualization parameters. To guarantee readability and efficient use of space across datasets, these static breakpoints would require manual updates. Constraint-based breakpoints solve this by evaluating visualization-specific constraints on the size of visual elements, overlapping elements, and the aspect ratio of the visualization and available space. Once configured, a responsive visualization with constraint-based breakpoints can adapt to different screen sizes for any dataset. We describe a framework that guides designers in creating a stack of visualization designs for different display sizes and defining constraints for each of these designs. We demonstrate constraint-based breakpoints for different data types and their visualizations: geographic data (choropleth map, proportional circle map, Dorling cartogram, hexagonal grid map, bar chart, waffle chart), network data (node-link diagram, adjacency matrix, arc diagram), and multivariate data (scatterplot, heatmap).","accessible_pdf":"No","authors":[{"email":null,"name":"Sarah Sch\u00f6ttler"},{"email":null,"name":"Jason Dykes"},{"email":null,"name":"Jo Wood"},{"email":null,"name":"Uta Hinrichs"},{"email":null,"name":"Benjamin Bach"}],"award":"","doi":"10.1109/TVCG.2024.3410097","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"b75e4e38-595d-4c86-a824-7251ec305f19","image_caption":"","keywords":["Data visualization","Visualization","Libraries","Manuals","Bars","Media","Layout"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2409.01339","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024.3410097]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full5","session_room":"Hall M2","session_room_id":"m2","session_title":"Authoring and Design","session_uid":"b75e4e38-595d-4c86-a824","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Authoring and Design"],"time_stamp":"2025-11-05T15:45:00.000Z","title":"Constraint-Based Breakpoints for Responsive Visualization Design and Development","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/S3LGLxyblpM"},{"UID":"b859563f-62a0-4ddc-be29-b8e583eb10a7","abstract":"Immersive analytics is gaining attention across multiple domains due to its capability to facilitate intuitive data analysis in expansive environments through user interaction with data. However, creating immersive analytics systems for specific tasks is challenging due to the need for programming expertise and significant development effort. Despite the introduction of various immersive visualization authoring toolkits, domain experts still face hurdles in adopting immersive analytics into their workflow, particularly when faced with dynamically changing tasks and data in real time. To lower such technical barriers, we introduce XROps, a web-based authoring system that allows users to create immersive analytics applications through interactive visual programming, without the need for low-level scripting or coding. XROps enables dynamic immersive analytics authoring by allowing users to modify each step of the data visualization process with immediate feedback, enabling them to build visualizations on-the-fly and adapt to changing environments. It also supports the integration and visualization of real-time sensor data from XR devices\u2014a key feature of immersive analytics\u2014facilitating the creation of various analysis scenarios. We evaluated the usability of XROps through a user study and demonstrate its efficacy and usefulness in several example scenarios.","accessible_pdf":"No","authors":[{"email":null,"name":"Suemin Jeon"},{"email":null,"name":"JunYoung Choi"},{"email":null,"name":"Haejin Jeong"},{"email":null,"name":"Won-Ki Jeong"}],"award":"","doi":"10.1109/TVCG.2025.3546467","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"b859563f-62a0-4ddc-be29-b8e583eb10a7","image_caption":"","keywords":["Data visualization","Visualization","Programming","Real-time systems","Three-dimensional displays","Encoding","Programming profession","Workflow management software","Training","Electronic mail"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.10043","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3546467","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full18","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive & Ubiquitous Analytics","session_uid":"b859563f-62a0-4ddc-be29","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive & Ubiquitous Analytics"],"time_stamp":"2025-11-06T14:00:00.000Z","title":"XROps: a Visual Workflow Management System for Dynamic Immersive Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/6hTTrGMsPK8"},{"UID":"b8d95afa-0b83-46b6-a0b5-a48369a85ff7","abstract":"Process Discovery, a key area of Process Mining, constructs models from event logs containing sequences of activity executions. Current automatic approaches suffer from opaque algorithms, limited incorporation of domain knowledge, and static, hard-to-explore visualizations. To address these limitations, we propose an interactive discovery process involving users in constructing process trees. This allows real-time interaction, dynamic domain expertise integration, and better understanding of the relationship between logs and models. We demonstrate this with a prototype featuring four linked views, and introduce improved process tree visualizations.","accessible_pdf":null,"authors":[{"affiliation":"Eindhoven University of Technology","email":"w.b.v.d.heijden@student.tue.nl","name":"Wouter van der Heijden"},{"affiliation":"Eindhoven University of Technology","email":"s.j.v.d.elzen@tue.nl","name":"Stef van den Elzen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b8d95afa-0b83-46b6-a0b5-a48369a85ff7","image_caption":"","keywords":["Visual Analytics","Process Mining","Process Trees."],"open_access_supplemental_link":"https://github.com/stefvandenelzen/process-tree-builder","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1190","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full23","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Of Trees and Treemaps","session_uid":"b8d95afa-0b83-46b6-a0b5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Of Trees and Treemaps"],"time_stamp":"2025-11-05T11:15:00.000Z","title":"Interactive Discovery and Visualization of Process Trees","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/yiiG1AZHJ1E"},{"UID":"b9fa4e27-b3e0-425b-b3c6-af1ebecd478a","abstract":"We propose manvr3d, a VR platform for immersive, AI-assisted human-in-the-loop cell tracking. Life scientists reconstruct the developmental history of organisms at the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. However, reconstruction of cell trajectories and lineage trees is a highly time consuming and error prone task. Common tools are often limited to 2D image display, which greatly limits spatial understanding and navigation. Deep Learning-based algorithms accelerate this process, yet depend heavily on manually-annotated, high-quality ground truth data and curation. In this work, we bridge the gap between Deep Learning-based cell tracking software and 3D/VR visualization to create a hybrid AI-human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the third dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists. We present here the technical architecture of our platform and first analysis of performance. Our code is released open source.","accessible_pdf":null,"authors":[{"affiliation":"Center for  Advanced Systems Understanding (CASUS)","email":"s.pantze@hzdr.de","name":"Samuel Pantze"},{"affiliation":"Institut Pasteur","email":"tinevez@pasteur.fr","name":"Jean-Yves Tinevez"},{"affiliation":"Technische Universit\u00e4t Dresden","email":"matthew.mcginity@tu-dresden.de","name":"Matthew McGinity"},{"affiliation":"Helmholtz-Zentrum Dresden-Rossendorf e.V","email":"ulrik.guenther@hzdr.de","name":"Ulrik G\u00fcnther"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"b9fa4e27-b3e0-425b-b3c6-af1ebecd478a","image_caption":"","keywords":["Systems Biology","Virtual Reality","Microscopy","Cell Tracking","Volume Rendering","Eye Tracking"],"open_access_supplemental_link":"https://osf.io/rn9h7/","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2505.03440","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1134","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"b9fa4e27-b3e0-425b-b3c6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T15:48:00.000Z","title":"manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"ba183a14-a911-4177-8f18-c35ee2220c67","abstract":"Effective visualization retrieval necessitates a clear definition of similarity. Despite the growing body of work in specialized visualization retrieval systems, a systematic approach to understanding visualization similarity remains absent. We introduce the Similarity Framework for Visualization Retrieval (Safire), a conceptual model that frames visualization similarity along two dimensions: comparison criteria and representation modalities. Comparison criteria identify the aspects that make visualizations similar, which we divide into primary facets (data, visual encoding, interaction, style, metadata) and derived properties (data-centric and human-centric measures). Safire connects what to compare with how comparisons are executed through representation modalities. We categorize existing representation approaches into four groups based on their levels of information content and visualization determinism: raster image, vector image, specification, and natural language description, together guiding what is computable and comparable. We analyze several visualization retrieval systems using Safire to demonstrate its practical value in clarifying similarity considerations. Our findings reveal how particular criteria and modalities align across different use cases. Notably, the choice of representation modality is not only an implementation detail but also an important decision that shapes retrieval capabilities and limitations. Based on our analysis, we provide recommendations and discuss broader implications for multimodal learning, AI applications, and visualization reproducibility.","accessible_pdf":null,"authors":[{"affiliation":"Harvard Medical School","email":"huyen_nguyen@hms.harvard.edu","name":"Huyen N. Nguyen"},{"affiliation":"Harvard Medical School","email":"nils@hms.harvard.edu","name":"Nils Gehlenborg"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ba183a14-a911-4177-8f18-c35ee2220c67","image_caption":"","keywords":["Visualization retrieval","similarity framework","visualization similarity","representation modality","comparison"],"open_access_supplemental_link":null,"open_access_supplemental_question":"To promote transparency and reproducibility, our work introduces Safire, a systematic framework that makes explicit the comparison criteria and representation modalities underlying visualization retrieval systems, helping researchers clearly articulate their similarity definitions and approaches. By analyzing existing systems through this lens, we provide actionable insights and recommendations to enhance the reproducibility and extensibility of future retrieval methods.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/p47z5","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1160","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"ba183a14-a911-4177-8f18","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T09:33:00.000Z","title":"Safire: Similarity Framework for Visualization Retrieval","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"ba87a101-1d66-45b7-91a0-0410f31e50fc","abstract":"Managing water resources within California\u2019s Central Valley is a complex task involving analyzing a myriad of factors e.g. climate, demand, and government policies. This results in opaque management decisions which are concentrated to a small group of people while the majority of the valley is left out of the decision-making process, leading to inequitable distribution. This paper assesses the design for Watering the Future, an application aiming to educate non-experts in water management about water distribution systems by illustrating regional water supplies and the factors behind them in a more understandable manner. We sought feedback from a domain expert in water management and conducted a user study to assess the effectiveness of our graph designs, of which results suggested increased levels of engagement while still communicating the required information, suggesting an enhanced level of accessibility to the unfamiliar field of water management.","accessible_pdf":null,"authors":[{"affiliation":"University of California, Davis","email":"sayuniar@ucdavis.edu","name":"Sarah Yuniar"},{"affiliation":"University of California, Davis","email":"ykawakami@ucdavis.edu","name":"Yuya Kawakami"},{"affiliation":"University of California, Davis","email":"log.gstang@gmail.com","name":"Logan Tang"},{"affiliation":"University of California, Davis","email":"ma@cs.ucdavis.edu","name":"Kwan-Liu Ma"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ba87a101-1d66-45b7-91a0-0410f31e50fc","image_caption":"","keywords":["Visualization design study","public-facing visualization","visual metaphor"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1310","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full9","session_room":"Hall M2","session_room_id":"m2","session_title":"Dastardly Dashboards","session_uid":"ba87a101-1d66-45b7-91a0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dastardly Dashboards"],"time_stamp":"2025-11-05T09:30:00.000Z","title":"Watering the Future: Assessing a Visualization Design for Accessible Comprehension of Water Management","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/Ga7NSmKd54M"},{"UID":"bba75087-a544-4e3c-85f9-947b8b3537dc","abstract":"Connectomics, a subfield of neuroscience, reconstructs structural and functional brain maps at synapse-level resolution. These complex spatial maps consist of tree-like neurons interconnected by synapses. Motif analysis is a widely used method for identifying recurring subgraph patterns in connectomes. These motifs, thus, potentially represent fundamental units of information processing. However, existing computational tools often oversimplify neurons as mere nodes in a graph, disregarding their intricate morphologies. In this paper, we introduce MoMo, a novel interactive visualization framework for analyzing neuron morphology-aware motifs in large connectome graphs. First, we propose an advanced graph data structure that integrates both neuronal morphology and synaptic connectivity. This enables highly efficient, parallel subgraph isomorphism searches, allowing for interactive morphological motif queries. Second, we develop a sketch-based interface that facilitates the intuitive exploration of morphology-based motifs within our new data structure. Users can conduct interactive motif searches on state-of-the-art connectomes and visualize results as interactive 3D renderings. We present a detailed goal and task analysis for motif exploration in connectomes, incorporating neuron morphology. Finally, we evaluate MoMo through case studies with four domain experts, who asses the tool\u2019s usefulness and effectiveness in motif exploration, and relevance to real-world neuroscience research. The source code for MoMo is available here: https://github.com/VCG/momo","accessible_pdf":null,"authors":[{"affiliation":"Harvard University","email":"michael.shewarega@rwth-aachen.de","name":"Michael Shewarega"},{"affiliation":"Harvard University","email":"jakob.troidl@googlemail.com","name":"Jakob Troidl"},{"affiliation":"New Jersey Institute of Technology","email":"oaa9@njit.edu","name":"Oliver Alvarado Rodriguez"},{"affiliation":"New Jersey Institute of Technology","email":"md724@njit.edu","name":"Mohammad Dindoost"},{"affiliation":"Zuse Institue","email":"harth@zib.de","name":"Philipp Harth"},{"affiliation":"University of W\u00fcrzburg","email":"hannah.haberkern@uni-wuerzburg.de","name":"Hannah Haberkern"},{"affiliation":"RWTH Aachen","email":"johannes.stegmaier@lfb.rwth-aachen.de","name":"Johannes Stegmaier"},{"affiliation":"New Jersey Institute of Technology","email":"bader@njit.edu","name":"David Bader"},{"affiliation":"Harvard University","email":"pfister@seas.harvard.edu","name":"Hanspeter Pfister"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"bba75087-a544-4e3c-85f9-947b8b3537dc","image_caption":"","keywords":["Visual motif analysis","Scientific visualization","Neuroscience","Connectomics."],"open_access_supplemental_link":"https://github.com/VCG/momo","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://www.biorxiv.org/content/10.1101/2025.07.02.662847v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1700","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full4","session_room":"Hall E2","session_room_id":"e2","session_title":"Analytics & Reasoning","session_uid":"bba75087-a544-4e3c-85f9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analytics & Reasoning"],"time_stamp":"2025-11-06T14:45:00.000Z","title":"MoMo - Combining Neuron Morphology and Connectivity for Interactive Motif Analysis in Connectomes","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/vzBq3yA2yUs"},{"UID":"bbb737fd-8e8d-4862-9427-e0a928f2051b","abstract":"Photoinduced electronic transitions are complex quantum-mechanical processes where electrons move between energy levels due to the absorption of light. This induces dynamics i.e., coupled changes in the electronic structure and nuclear geometry, that drive physical and chemical processes of importance in diverse fields ranging from photobiology and materials design to medicine. The evolving electronic structure can be characterized by two electron density fields: hole and particle natural transition orbitals (NTOs). A study of the two density fields helps understand the movement of electronic charge from one part of the molecule to another, specifically the donor and acceptor regions. Previous works in this area rely on side-by-side visual comparisons of isosurfaces, statistical approaches, or visual analysis of bivariate fields restricted to limited time instances. We propose a new method to analyze time-varying bivariate fields with a large number of instances, as pertinent to understand electronic structure changes during light-induced dynamics. Since the NTO fields depend on the nuclear geometry, the nuclear motion leads to a large number of bivariate field instances. Structures like tracking graphs have been used to analyze time-varying univariate fields. This article presents a structured and practical approach to feature-directed visual exploration of time-varying bivariate fields using continuous scatterplots (CSPs) and image moment-based descriptors, tailored for studying the evolving electronic structure following photoexcitation. The CSP of the bivariate field at every time step is represented using an image moment vector of length 4. The collection of all image moment vector descriptors is considered as a point cloud in R4 and visualized using principal component analysis. Choosing an appropriate pair of principal components results in a representation of the point cloud as a curve on the plane. This representation supports tasks such as identifying interesting time steps, identifying patterns within the bivariate field, and tracking their evolution over time. We present two case studies on excited-state dynamics in molecular systems that demonstrate how the time-varying bivariate field analysis helps provide application-specific insights.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Mohit Sharma"},{"email":null,"name":"Talha Bin Masood"},{"email":null,"name":"Nanna Holmgaard List"},{"email":null,"name":"Ingrid Hotz"},{"email":null,"name":"Vijay Natarajan"}],"award":"","doi":"10.1109/TVCG.2025.3543619","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"bbb737fd-8e8d-4862-9427-e0a928f2051b","image_caption":"","keywords":["Visualization","Atoms","Principal component analysis","Geometry","Electrons","Orbits","Pipelines","Manuals","Vectors","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Our supplementary material provides high-level implementation details, design choices, and approximate runtime estimates for key pipeline steps. While no source code or precise benchmarks are provided, the document enhances methodological transparency and helps readers understand how the system can be reproduced or extended.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2502.17118","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[10.1109/TVCG 2025.3543619]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full22","session_room":"Hall E2","session_room_id":"e2","session_title":"Multivariate and Time","session_uid":"bbb737fd-8e8d-4862-9427","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Multivariate and Time"],"time_stamp":"2025-11-05T09:06:00.000Z","title":"Continuous Scatterplot and Image Moments for Time-Varying Bivariate Field Analysis of Electronic Structure Evolution","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-Il9UvlqnNA"},{"UID":"bbe3a438-83bd-4b6a-a694-75d70e7ed2f4","abstract":"Dimensionality reduction is used as an important tool for unraveling the complexities of high-dimensional datasets in many fields of science, such as cell biology, chemical informatics, and physics. Visualizations of the dimensionally-reduced data enable scientists to delve into the intrinsic structures of their datasets and align them with established hypotheses. Visualization researchers have thus proposed many dimensionality reduction methods and interactive systems designed to uncover latent structures. At the same time, different scientific domains have formulated guidelines or common workflows for using dimensionality reduction techniques and visualizations for their respective fields. In this work, we present a critical analysis of the usage of dimensionality reduction in scientific domains outside of computer science. First, we conduct a bibliometric analysis of 21,249 academic publications that use dimensionality reduction to observe differences in the frequency of techniques across fields. Next, we conduct a survey of a 71-paper sample from four fields: biology, chemistry, physics, and business. Through this survey, we uncover common workflows, processes, and usage patterns, including the mixed use of confirmatory data analysis to validate a dataset and projection method and exploratory data analysis to then generate more hypotheses. We also find that misinterpretations and inappropriate usage is common, particularly in the visual interpretation of the resulting dimensionally reduced view. Lastly, we compare our observations with recent works in the visualization community in order to match work within our community to potential areas of impact outside our community. By comparing the usage found within scientific fields to the recent research output of the visualization community, we offer both validation of the progress of visualization research into dimensionality reduction and a call for action to produce techniques that meet the needs of scientific use...","accessible_pdf":"No","authors":[{"email":null,"name":"Dylan Cashman"},{"email":null,"name":"Mark Keller"},{"email":null,"name":"Hyeon Jeon"},{"email":null,"name":"Bum Chul Kwon"},{"email":null,"name":"Qianwen Wang"}],"award":"","doi":"10.1109/TVCG.2025.3567989","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"bbe3a438-83bd-4b6a-a694-75d70e7ed2f4","image_caption":"","keywords":["Surveys","Data visualization","Dimensionality reduction","Data analysis","Measurement","Reviews","Physics","Electronic mail","Computer science","Bibliometrics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2503.08836","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-12-1098.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full10","session_room":"Hall E2","session_room_id":"e2","session_title":"Dimensionality Reduction and Parameter Space Analysis","session_uid":"bbe3a438-83bd-4b6a-a694","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dimensionality Reduction and Parameter Space Analysis"],"time_stamp":"2025-11-05T15:21:00.000Z","title":"A Critical Analysis of the Usage of Dimensionality Reduction in Four Domains","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/lJXNA9vtCPA"},{"UID":"bbfc6b4e-96d6-4981-87da-8bb285d256dc","abstract":"Storyline visualizations represent character interactions over time. When these characters belong to different groups, a new research question emerges: how can we balance optimization of readability across the groups while preserving the overall narrative structure of the story? Traditional algorithms that optimize global readability metrics (like minimizing crossings) can introduce quality biases between the different groups based on their cardinality and other aspects of the data. Visual consequences of these biases are: making characters of minority groups disproportionately harder to follow, and visually deprioritizing important characters when their curves become entangled with numerous secondary characters. We present F2Stories, a modular framework that addresses these challenges in storylines by offering three complementary optimization modes: (1) fairnessMode ensures that no group bears a disproportionate burden of visualization complexity regardless of their representation in the story; (2) focusMode allows prioritizing a group of characters while maintaining good readability for secondary characters; and (3) standardMode globally optimizes classical aesthetic metrics. Our approach is based on Mixed Integer Linear Programming (MILP), offering optimality guarantees, precise balancing of competing metrics through weighted objectives, and the flexibility to incorporate complex fairness concepts as additional constraints without the need to redesign the entire algorithm. We conducted an extensive experimental analysis to demonstrate how F2Stories enables more fair or focus group-prioritized storyline visualizations while maintaining adherence to established layout constraints. Our evaluation includes comprehensive results from a detailed case study that shows the effectiveness of our approach in real-world narrative contexts. An open access copy of this paper and all supplemental materials are available at https://osf.io/e2qvy/.","accessible_pdf":null,"authors":[{"affiliation":"Universit\u00e0 degli Studi di Perugia","email":"tommaso.piselli@gmail.com","name":"Tommaso Piselli"},{"affiliation":"University of Perugia","email":"giuseppe.liotta@unipg.it","name":"Giuseppe Liotta"},{"affiliation":"University of Perugia","email":"fabrizio.montecchiani@unipg.it","name":"Fabrizio Montecchiani"},{"affiliation":"TU Wien","email":"noellenburg@ac.tuwien.ac.at","name":"Martin N\u00f6llenburg"},{"affiliation":"TU Wien","email":"dibartolomeo.sara@gmail.com","name":"Sara Di Bartolomeo"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"bbfc6b4e-96d6-4981-87da-8bb285d256dc","image_caption":"","keywords":["Storyline layouts","optimization","fairness"],"open_access_supplemental_link":"https://osf.io/e2qvy/","open_access_supplemental_question":"We did everything according to the Open Practices Committee page (i.e. pre-print @https://www.ac.tuwien.ac.at/files/tr/ac-tr-25-001.pdf and open source code in osf@https://osf.io/e2qvy/) . We plan to apply to the Graphics Replicability Stamp Initiative and made sure that our experiments are replicable.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://www.ac.tuwien.ac.at/files/tr/ac-tr-25-001.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1587","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full15","session_room":"Hall E2","session_room_id":"e2","session_title":"From Data to Meaning","session_uid":"bbfc6b4e-96d6-4981-87da","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["From Data to Meaning"],"time_stamp":"2025-11-06T10:39:00.000Z","title":"F^2Stories: A Modular Framework for Multi-Objective Optimization of Storylines with a Focus on Fairness","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/v-iY-PKyXHs"},{"UID":"be118d02-63d2-4463-a135-af2c23073ad2","abstract":"Virtual sampling has been proposed as an alternative of pre-integration to reduce artifacts caused by an insufficient sampling of the ray profiles in volume-rendering applications. According to the originally recommended implementation, a 3D tricubic B-spline reconstruction is applied to evaluate true samples along the rays, while between the true samples, virtual samples are calculated by using a 1D Catmull-Rom spline interpolation. The virtual samples can be evaluated much faster, but they can still well approximate the true samples. Therefore, the rendering performance can be drastically improved at the cost of a very slight quality degradation. Nevertheless, in this paper, we show that the original virtual sampling scheme does not exploit the fact, that the applied tricubic B-spline reconstruction filter together with its analytic derivative filters provide Hermite samples. Therefore, it is more natural to use Hermite interpolation along the rays rather than Catmull-Rom spline interpolation. From a sampling-theoretical point of view, this modification requires half of the true samples to guarantee the same reconstruction quality. Furthermore, concerning the evaluation of the true samples, we also investigate a recently published GPU-accelerated triquadratic B-spline filtering as an alternative of the tricubic B-spline filtering.","accessible_pdf":null,"authors":[{"affiliation":"Budapest University of Technology and Economics","email":"cseb@iit.bme.hu","name":"Bal\u00e1zs Cs\u00e9bfalvi"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"be118d02-63d2-4463-a135-af2c23073ad2","image_caption":"","keywords":["Direct volume rendering","reconstruction filtering","virtual sampling."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1216","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"be118d02-63d2-4463-a135","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T10:33:00.000Z","title":"Virtual Ray Sampling for Direct Volume Rendering using Hermite Interpolation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"bf10121c-1a25-4391-b766-450e53b2348e","abstract":"When a reader encounters a word in English, they split the word into smaller orthographic units in the process of recognizing\nits meaning. For example, \u201crough\u201d, when split according to phonemes, is decomposed as r-ou-gh (not as r-o-ugh or r-ough), where\neach group of letters corresponds to a sound. Since there are many ways to segment a group of letters, this constitutes a computational\noperation that has to be solved by the reading brain, many times per minute, in order to achieve the recognition of words in text\nnecessary for reading. In English, the irregular relationships between groups of letters and sounds, and the wide variety of possible\ngroupings make this operation harder than in more regular languages such as Italian. If this segmentation takes a significant amount of\ntime in the process of recognizing a word, it is conceivable that providing segmentation information in the text itself could help the\nreading process by reducing its computational cost. In this paper we explore whether and how different visual interventions from the\nvisualization literature could communicate segmentation information for reading and word recognition. We ran a series of pre-registered\nlexical decision experiments with 192 participants that tested five main types of visual segmentations: outlines, spacing, connections,\nunderlines and color. The evidence indicates that, even with a moderate amount of training, these visual interventions always slow\ndown word identification, but each to a different extent (between 32.7ms\u2014color technique\u2014and 70.7ms\u2014connection technique).\nThese findings are important because they indicate that, at least for typical adult readers with a moderate amount of specific training in\nthese visual interventions, accelerating the lexical decision task is unlikely. Importantly, the results also offer an empirical measurement\nof the cost of a common set of visual manipulations of text, which can be useful for practitioners seeking to visualize alongside or within\ntext without impacting reading performance. Finally, the interaction between typographically encoded information and visual variables\npresented unique patterns that deviate from existing theories, suggesting new directions for future inquiry.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Victoria","email":"matttermuende@gmail.com","name":"Matthew Termuende"},{"affiliation":"Microsoft","email":"kevlar@microsoft.com","name":"Kevin Larson"},{"affiliation":"University of Victoria","email":"nacenta@gmail.com","name":"Miguel Nacenta"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"bf10121c-1a25-4391-b766-450e53b2348e","image_caption":"","keywords":["Reading","Word Recognition","Text Visualization","Text Interaction","Phonological Cues"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/79Y5S","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1789","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full34","session_room":"Hall M2","session_room_id":"m2","session_title":"Vis & Language","session_uid":"bf10121c-1a25-4391-b766","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Vis & Language"],"time_stamp":"2025-11-06T10:51:00.000Z","title":"The Impact of Visual Segmentation on Lexical Word Recognition","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JQEgOnSg18o"},{"UID":"bf2f93ab-ab67-4a81-8726-6819e84553be","abstract":"Sentiment contagion occurs when attitudes toward one topic are influenced by attitudes toward others. \nDetecting and understanding this phenomenon is essential for analyzing topic evolution and informing social policies.\nPrior research has developed models to simulate the contagion process through hypothesis testing and has visualized user\u2013topic correlations to aid comprehension. \nNevertheless, the vast volume of topics and the complex interrelationships on social media present two key challenges: (1) efficient construction of large-scale sentiment contagion networks, and (2) in-depth explorations of these networks.\nTo address these challenges, we introduce a causality-based framework that efficiently constructs and explains sentiment contagion.\nWe further propose a map-like visualization technique that encodes time using a horizontal axis, enabling efficient visualization of causality-based sentiment flow while maintaining scalability through limitless spatial segmentation. \nBased on the visualization, we develop CausalMap, a system that supports analysts in tracing sentiment contagion pathways and assessing the influence of different demographic groups.\nFurthermore, we conduct comprehensive evaluations\u2014\u2014including two use cases, a task-based user study, an expert interview, and an algorithm evaluation\u2014\u2014to validate the usability and effectiveness of our approach.","accessible_pdf":null,"authors":[{"affiliation":"State Key Lab of CAD&CG, Zhejiang University","email":"renzhongli@zju.edu.cn","name":"Renzhong Li"},{"affiliation":"Zhejiang University","email":"sn_ye@zju.edu.cn","name":"Shuainan Ye"},{"affiliation":"Zhejiang University","email":"yuchenlin@zju.edu.cn","name":"Yuchen Lin"},{"affiliation":"Zhejiang University","email":"zhoubuwei@zju.edu.cn","name":"Buwei Zhou"},{"affiliation":"Zhejiang University","email":"kang264@zju.edu.cn","name":"Zhining Kang"},{"affiliation":"Michigan State University","email":"pengtaiq@msu.edu","name":"Tai-Quan Peng"},{"affiliation":"independent researcher","email":"723981485@qq.com","name":"Wenhao Fu"},{"affiliation":"Zhejiang University","email":"tangtan@zju.edu.cn","name":"Tan Tang"},{"affiliation":"Zhejiang University","email":"ycwu@zju.edu.cn","name":"Yingcai Wu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"bf2f93ab-ab67-4a81-8726-6819e84553be","image_caption":"","keywords":["Social Media","Causal Analysis","Visual Analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1075","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full6","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Best Full Papers & Test of Time Awards","session_uid":"bf2f93ab-ab67-4a81-8726","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Best Full Papers & Test of Time Awards"],"time_stamp":"2025-11-04T14:46:00.000Z","title":"Causality-based Visual Analytics of Sentiment Contagion in Social Media Topics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/eNFqK9YEju4"},{"UID":"c06df49a-9aa2-40f9-94b9-23789ec137b8","abstract":"Modern scientific discovery encounters significant challenges in integrating the rapidly expanding and heterogeneous body of knowledge required for driving breakthroughs in biomedicine and drug development. While traditional hypothesis-driven research has proven effective, it is constrained by human cognitive limitations, the complexity of biological systems, and the high costs associated with trial-and-error experimentation. Deep learning models, particularly graph neural networks (GNNs), have accelerated scientific progress. However, the sheer volume of predictions they generate makes manual selection for experimental validation impractical. Attempts to leverage large language models (LLMs) for filtering predictions and generating novel hypotheses have been impeded by issues such as hallucinations and the lack of structured knowledge grounding, which undermine their reliability. To address these challenges, we propose HypoChainer, a collaborative visualization framework that integrates human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance scientific discovery and validation visually. HypoChainer operates through three key stages: (1) Exploration and Contextualization: Domain experts employ retrieval-augmented LLMs (RAGs) and dimensionality reduction techniques to extract insights and research entry points from vast GNN predictions, supplemented by interactive explanations for in-depth understanding; (2) Hypothesis Chain Formation: Experts iteratively explore the relationships between KG information relevant to the predictions and semantically linked nodes consistent with the hypothesis, gaining knowledge and insights while refining the hypothesis through suggestions from LLMs and KGs; and (3) Validation Prioritization: Predictions are filtered and prioritized based on the refined hypothesis chains and KG-supported evidence, identifying high-priority candidates for experimental validation. Weak points in the hypothesis chain are further optimized through visual analytics of the retrieval results. We evaluated the effectiveness of HypoChainer in hypothesis construction and scientific discovery through case studies in two distinct domains and expert interviews.","accessible_pdf":null,"authors":[{"affiliation":"ShanghaiTech University","email":"shishh2023@shanghaitech.edu.cn","name":"Shaohan Shi"},{"affiliation":"ShanghaiTech University","email":"jianghr2023@shanghaitech.edu.cn","name":"Haoran Jiang"},{"affiliation":"ShanghaiTech University","email":"yaoyj2024@shanghaitech.edu.cn","name":"Yunjie Yao"},{"affiliation":"Shanghai Clinical Research and Trial Center","email":"cjiang_fdu@yeah.net","name":"Chang Jiang"},{"affiliation":"ShanghaiTech University","email":"liquan@shanghaitech.edu.cn","name":"Quan Li"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c06df49a-9aa2-40f9-94b9-23789ec137b8","image_caption":"","keywords":["Large Language Model","Visual Analytics","Iterative Human-AI Collaboration","Knowledge Graph","Hypothesis Construction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.17209","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1399","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full28","session_room":"Hall E1","session_room_id":"e1","session_title":"The VIS in GenAI","session_uid":"c06df49a-9aa2-40f9-94b9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The VIS in GenAI"],"time_stamp":"2025-11-05T13:12:00.000Z","title":"HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zLchh59XyM4"},{"UID":"c1893c22-b83f-4f15-82d7-c8f37bd65213","abstract":"Recovering a continuous colormap from a single 2D scalar field visualization can be quite challenging, especially in the absence of a corresponding color legend. In this paper, we propose a novel colormap recovery approach that extracts the colormap from a color-encoded 2D scalar field visualization by simultaneously predicting the colormap and underlying data using a decoupling-and-reconstruction strategy. Our approach first separates the input visualization into colormap and data using a decoupling module, then reconstructs the visualization with a differentiable color-mapping module. To guide this process, we design a reconstruction loss between the input and reconstructed visualizations, which serves both as a constraint to ensure strong correlation between colormap and data during training, and as a self-supervised optimizer for fine-tuning the predicted colormap of unseen visualizations during inferencing. To ensure smoothness and correct color ordering in the extracted colormap, we introduce a compact colormap representation using cubic B-spline curves and an associated color order loss. We evaluate our method quantitatively and qualitatively on a synthetic dataset and a collection of real-world visualizations from the VIS30K dataset~\\cite{Chen21}. Additionally, we demonstrate its utility in two prototype applications-colormap adjustment and colormap transfer-and explore its generalization  to  visualizations with color legends and ones encoded using discrete color palettes.","accessible_pdf":"Accessible","authors":[{"affiliation":"Shandong University","email":"liuhongxu_0227@163.com","name":"Hongxu Liu"},{"affiliation":"Shandong University","email":"xinyu.chen0468@gmail.com","name":"Xinyu Chen"},{"affiliation":"Shandong University","email":"zhenghaoyang1229@163.com","name":"Haoyang Zheng"},{"affiliation":"Shandong University","email":"manyili@sdu.edu.cn","name":"Manyi Li"},{"affiliation":"Shandong University","email":"liuzhenfannn@163.com","name":"Zhenfan Liu"},{"affiliation":"University of Maryland College Park","email":"fumeng.p.yang@gmail.com","name":"Fumeng Yang"},{"affiliation":"Renmin University of China","email":"cloudseawang@gmail.com","name":"Yunhai Wang"},{"affiliation":"Shandong Univ.","email":"changhe.tu@gmail.com","name":"Changhe Tu"},{"affiliation":"Shandong University","email":"qiong.zn@sdu.edu.cn","name":"Qiong Zeng"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c1893c22-b83f-4f15-82d7-c8f37bd65213","image_caption":"","keywords":["Colormap","color design","2D scalar field visualization","reverse engineering"],"open_access_supplemental_link":"https://osf.io/gb2tx/?view_only=4d2a269b59bc4144b2806ec2d1a34e11","open_access_supplemental_question":"This work addresses the challenge of recovering a continuous colormap from a 2D scalar field visualization without a corresponding color legend. We propose a novel approach that simultaneously predicts the colormap and underlying data using a decoupling-and-reconstruction strategy, offering a new method for colormap recovery in visualization.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"http://arxiv.org/abs/2507.20632","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1143","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full13","session_room":"Hall E2","session_room_id":"e2","session_title":"Fields, Fields, Fields","session_uid":"c1893c22-b83f-4f15-82d7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Fields, Fields, Fields"],"time_stamp":"2025-11-05T10:27:00.000Z","title":"Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualizations without a Legend","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mHta4w5FJQg"},{"UID":"c22daf54-0b62-4772-ac74-43e651c0184d","abstract":"Interactively visualizing large finite element simulation data on High-Performance Computing (HPC) systems poses several difficulties. Some of these relate to unstructured data, which, even on a single node, is much more expensive to render compared to structured volume data. Worse yet, in the data parallel rendering context, such data with highly non-convex spatial domain boundaries will cause rays along its silhouette to enter and leave a given rank's domains at different distances. This straddling, in turn, poses challenges for both ray marching, which usually assumes successive elements to share a face, and compositing, which usually assumes a single fragment per pixel per rank. We holistically address these issues using a combination of three inter-operating techniques: first, we use a highly optimized GPU ray marching technique that, given an entry point, can march a ray to its exit point with high-performance by exploiting an exclusive-or (XOR) based compaction scheme. Second, we use hardware-accelerated ray tracing to efficiently find the proper entry points for these marching operations. Third, we use a \u201cdeep\u201d compositing scheme to properly handle cases where different ranks\u2019 ray segments interleave in depth. We use GPU-to-GPU remote direct memory access (RDMA) to achieve interactive frame rates of 10\u201315 frames per second and higher for our motivating use case, the Fun3D NASA Mars Lander.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Alper Sahistan"},{"email":null,"name":"Serkan Demirci"},{"email":null,"name":"Ingo Wald"},{"email":null,"name":"Stefan Zellmann"},{"email":null,"name":"Jo\u00e3o Barbosa"},{"email":null,"name":"Nate Morrical"},{"email":null,"name":"U\u011fur G\u00fcd\u00fckbay"}],"award":"","doi":"10.1109/TVCG.2024.3427335","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"c22daf54-0b62-4772-ac74-43e651c0184d","image_caption":"","keywords":["Rendering (computer graphics)","Data visualization","Finite element analysis","Sorting","Graphics processing units","Distributed databases","Scalability"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG3427335]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full36","session_room":"Hall M1","session_room_id":"m1","session_title":"Volume","session_uid":"c22daf54-0b62-4772-ac74","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Volume"],"time_stamp":"2025-11-05T14:00:00.000Z","title":"Visualization of Large Non-Trivially Partitioned Unstructured Data with Native Distribution on High-Performance Computing Systems","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-M293vb-AIg"},{"UID":"c35d8073-d3b1-4978-bdd4-0b428299f354","abstract":"What impressions might readers form with visualizations that go beyond the data they encode? In this paper, we build on recent work that demonstrates the socio-indexical function of visualization, showing that visualizations communicate more than the data they explicitly encode. Bridging this with prior work examining public discourse about visualizations, we contribute an analytic framework for describing inferences about an artifact\u2019s social provenance. Via a series of attribution-elicitation surveys, we offer descriptive evidence that these social inferences: (1) can be studied asynchronously, (2) are not unique to a particular sociocultural group or a function of limited data literacy, and (3) may influence assessments of trust. Further, we demonstrate (4) how design features act in concert with the topic and underlying messages of an artifact\u2019s data to give rise to such \u2018beyond-data\u2019 readings. We conclude by discussing the design and research implications of inferences about social provenance, and why we believe broadening the scope of research on human factors in visualization to include sociocultural phenomena can yield actionable design recommendations to address urgent challenges in public data communication.","accessible_pdf":"Accessible","authors":[{"affiliation":"Massachusetts Institute of Technology","email":"amyraefoxphd@gmail.com","name":"Amy Rae Fox"},{"affiliation":"Massachusetts Institute of Technology","email":"mjmorgen@mit.edu","name":"Michelle Morgenstern"},{"affiliation":"Massachusetts Institute of Technology","email":"gmj@mit.edu","name":"Graham Jones"},{"affiliation":"MIT","email":"arvindsatya@mit.edu","name":"Arvind Satyanarayan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c35d8073-d3b1-4978-bdd4-0b428299f354","image_caption":"","keywords":["semiotics","socio-indexicality","social provenance","engagement","visualization psychology","public data communication"],"open_access_supplemental_link":"https://doi.org/10.17605/OSF.IO/23HYX","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1006","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full30","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Trust No One","session_uid":"c35d8073-d3b1-4978-bdd4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Trust No One"],"time_stamp":"2025-11-07T08:42:00.000Z","title":"Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mavB31i3NwM"},{"UID":"c663a254-dd80-4021-88aa-f0840fc3024e","abstract":"The interactive visual analysis of set-typed data, i.e., data with attributes that are of type set, is a rewarding area of research and applications. Valuable prior work has contributed solutions that enable the study of such data with individual set-typed dimensions. In this paper, we present CrossSet, a novel method for the joint study of two set-typed dimensions and their interplay. Based on a task analysis, we describe a new, multi-scale approach to the interactive visual exploration and analysis of such data. Two set-typed data dimensions are jointly visualized using a hierarchical matrix layout, enabling the analysis of the interactions between two set-typed attributes at several levels, in addition to the analysis of individual such dimensions. CrossSet is anchored at a compact, large-scale overview that is complemented by drill-down opportunities to study the relations between and within the set-typed dimensions, enabling an interactive visual multi-scale exploration and analysis of bivariate set-typed data. Such an interactive approach makes it possible to study single set-typed dimensions in detail, to gain an overview of the interaction and association between two such dimensions,\nto refine one of the dimensions to gain additional details at several levels, and to drill down to the specific interactions of individual set-elements from the set-typed dimensions. To demonstrate the effectiveness and efficiency of CrossSet, we have evaluated the new method in the context of several application scenarios.","accessible_pdf":null,"authors":[{"affiliation":"VRVis Research Center","email":"matkovic@vrvis.at","name":"Kresimir Matkovic"},{"affiliation":"VRVis Research Center","email":"splechtna@vrvis.at","name":"Rainer Splechtna"},{"affiliation":"Virginia Tech","email":"gracanin@vt.edu","name":"Denis Gracanin"},{"affiliation":"University of Bergen","email":"helwig.hauser@uib.no","name":"Helwig Hauser"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c663a254-dd80-4021-88aa-f0840fc3024e","image_caption":"","keywords":["set-typed data","bivariate visual data exploration and analysis"],"open_access_supplemental_link":"https://arxiv.org/abs/2508.00424","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.00424","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1622","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full22","session_room":"Hall E2","session_room_id":"e2","session_title":"Multivariate and Time","session_uid":"c663a254-dd80-4021-88aa","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Multivariate and Time"],"time_stamp":"2025-11-05T08:30:00.000Z","title":"CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-Il9UvlqnNA"},{"UID":"c7fde6c0-1dc4-4ad4-8413-50cc367ed451","abstract":"Visualization design study is a widely adopted approach for developing tailored visual solutions to domain-specific problems through close interdisciplinary collaboration. While the visualization community has proposed generalizable frameworks, there is a growing need for domain-aware methodologies that address discipline-specific challenges and refine design study practices. To investigate how domain characteristics and collaborator roles influence the design study process, we conducted interviews with 15 experts, including domain specialists from the humanities, arts, applied sciences, and artificial intelligence, as well as visualization researchers and developers, with direct experience in design studies. Our findings reveal tensions and opportunities that arise from differing expectations, communication styles, and levels of engagement among collaborators at various stages of the design process, including problem formulation, co-design, and evaluation. We highlight how domain-specific norms and role dynamics shape collaboration and influence the trajectory of visualization projects. Based on these insights, we offer practical considerations to help visualization researchers anticipate domain-specific challenges, foster mutual understanding, and adapt their methods accordingly. Our study contributes to ongoing efforts to support more context-sensitive, sustainable, and inclusive design study practices across diverse application domains.","accessible_pdf":null,"authors":[{"affiliation":"King's College London","email":"yiwen.xing@kcl.ac.uk","name":"Yiwen Xing"},{"affiliation":"King's College London","email":"maria-teresa.ortoleva@kcl.ac.uk","name":"Maria Teresa Ortoleva"},{"affiliation":"Kings College London","email":"rita.borgo@kcl.ac.uk","name":"Rita Borgo"},{"affiliation":"King's College London","email":"alfie.abdulrahman@gmail.com","name":"Alfie Abdul-Rahman"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c7fde6c0-1dc4-4ad4-8413-50cc367ed451","image_caption":"","keywords":["Design study","collaboration","interview study"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://kclpure.kcl.ac.uk/portal/en/publications/collaborating-across-domains-and-roles-an-interview-study-of-visu","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1584","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full5","session_room":"Hall M2","session_room_id":"m2","session_title":"Authoring and Design","session_uid":"c7fde6c0-1dc4-4ad4-8413","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Authoring and Design"],"time_stamp":"2025-11-05T15:21:00.000Z","title":"Collaborating across Domains and Roles: An Interview Study of Visualization Design Practices","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/S3LGLxyblpM"},{"UID":"c9fcc45e-0f81-49be-817e-b813e0b61c89","abstract":"Research on affective visualization design has shown that color is an especially powerful feature for influencing the emotional connotation of visualizations. Associations between colors and emotions are largely driven by lightness (e.g., lighter colors are associated with positive emotions, whereas darker colors are associated with negative emotions). Designing visualizations to have all light or all dark colors to convey particular emotions may work well for visualizations in which colors represent categories and spatial channels encode data values. However, this approach poses a problem for visualizations that use color to represent spatial patterns in data (e.g., colormap data visualizations) because lightness contrast is needed to reveal fine details in spatial structure. In this study, we found it is possible to design colormaps that have strong lightness contrast to support spatial vision while communicating clear affective connotation. We also found that affective connotation depended not only on the color scales used to construct the colormaps, but also the frequency with which colors appeared in the map, as determined by the underlying dataset (data-dependence hypothesis). These results emphasize the importance of data-aware design, which accounts for not only the design features that encode data (e.g., colors, shapes, textures), but also how those design features are instantiated in a visualization, given the properties of the data.","accessible_pdf":null,"authors":[{"affiliation":"University of Wisconsin - Madison","email":"hbraun5@wisc.edu","name":"Halle Braun"},{"affiliation":"University of Wisconsin-Madison","email":"kushinm11@gmail.com","name":"Kushin Mukherjee"},{"affiliation":"Woodwell Climate Research Center","email":"sgorelik@woodwellclimate.org","name":"Seth Gorelik"},{"affiliation":"University of Wisconsin - Madison","email":"kschloss@wisc.edu","name":"Karen Schloss"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"c9fcc45e-0f81-49be-817e-b813e0b61c89","image_caption":"","keywords":["Visual reasoning","visual communication","color cognition","affective science","emotion","scalar field","data-aware design"],"open_access_supplemental_link":"https://github.com/SchlossVRL/color_scales_affect","open_access_supplemental_question":"We have posted all of our data and analysis code on GitHub and have documented our code.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/p3bva_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1682","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full27","session_room":"Hall M2","session_room_id":"m2","session_title":"The Color and the Shape","session_uid":"c9fcc45e-0f81-49be-817e","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["The Color and the Shape"],"time_stamp":"2025-11-06T09:06:00.000Z","title":"Affective color scales for colormap data visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gHCENa3C9X4"},{"UID":"ca2928b5-c272-464f-b52b-a3c0a4f7bc80","abstract":"The Fire We Share proposes a care-centered, consequence-aware visualization framework that engages wildfire data not as static metrics, but as living archives of ecological and social entanglement. By combining plant-inspired forms, event-based mapping, and layered storytelling, the project foregrounds fire as a shared temporal condition that cuts across natural cycles, human voices, and policy contexts. Through ritualized interaction with pinecone metaphors and fractured tree rings, participants encounter wildfire not only as data, but as memory, testimony, and ethical relation. Rather than simplifying information into digestible visuals, The Fire We Share reimagines wildfire as a textured, wounded archive\u2014embodied, relational, and radically ethical.","accessible_pdf":null,"authors":[{"affiliation":"California State University","email":"chenwangdesign@gmail.com","name":"chen wang"},{"affiliation":"California State University","email":"mengtanlin@csu.fullerton.edu","name":"Mengtan Lin"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ca2928b5-c272-464f-b52b-a3c0a4f7bc80","image_caption":"","keywords":["wildfire visualization; ecological memory;\nritual interface; data as grief and care;\nalgorithmic storytelling; broken symmetry;\nrelational design"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1125","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap1","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Pictorial","session_uid":"ca2928b5-c272-464f-b52b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Pictorial"],"time_stamp":"2025-11-06T10:27:00.000Z","title":"The Fire We Share","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/NZ0Wwrqv6fA"},{"UID":"cab72096-2336-4e27-a4e2-c416dd102259","abstract":"Within the nucleus of a cell, the genetic material (DNA) is condensed, supercoiled, and wrapped around proteins called histones, so to form chromatin. Unlike its traditional visualization, the organization of chromatin is 3D and can be measured with whole-genome sequencing technologies such as Hi-C sequencing, the results of which are often graphically represented as flat 2D heat-maps or a 1D curve that looks like a hairball. Here we discuss a novel approach to represent these and other genomic data by transforming the visualization of the genome from a 1D curve embedded in 3D into a structured 3D volume. This abstraction prioritizes 3D spatial proximity of chromatin and enables advanced techniques like clipping, thresholding, and contouring. Our method allows for multi-variable encoding, thereby enhancing our ability to interpret multivariate relationships, offering a more complete and effective representation of genomic structures, chromosome organization, and data relationships.","accessible_pdf":null,"authors":[{"affiliation":"Los Alamos National Laboratory","email":"bujack@lanl.gov","name":"Roxana Bujack"},{"affiliation":"Los Alamos National Laboratory","email":"dhr@lanl.gov","name":"David Rogers"},{"affiliation":"Los Alamos National Laboratory","email":"croth@lanl.gov","name":"Cullen Roth"},{"affiliation":"Los Alamos National Laboratory","email":"emsmall@lanl.gov","name":"Eric Small"},{"affiliation":"Los Alamos National Laboratory","email":"baners4@lanl.gov","name":"Banerjee Shounak"},{"affiliation":"Los Alamos National Laboratory","email":"vrindavenu7@gmail.com","name":"Vrinda Venu"},{"affiliation":"Los Alamos National Laboratory","email":"shawns@lanl.gov","name":"Shawn Starkenburg"},{"affiliation":"Los Alamos National Laboratory","email":"crsteadman@lanl.gov","name":"Steadman, Christina Rene"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"cab72096-2336-4e27-a4e2-c416dd102259","image_caption":"","keywords":["Genome","volume","DNA","3D Hi-C","volumetric","abstraction."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1220","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"cab72096-2336-4e27-a4e2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T11:18:00.000Z","title":"Volumetric Visualization of the Genome","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"cb542d49-92c6-4cec-bc55-289fb2a5e04c","abstract":"Our society increasingly depends on intelligent systems to solve complex problems, ranging from recommender systems suggesting the next movie to watch to AI models assisting in medical diagnoses for hospitalized patients. With the iterative improvement of diagnostic accuracy and efficiency, AI holds significant potential to mitigate medical misdiagnoses by preventing numerous deaths and reducing an economic burden of approximately \u20ac 450 billion annually. However, a key obstacle to AI adoption lies in the lack of transparency, that is, many automated systems provide predictions without revealing the underlying processes. This opacity can hinder experts\u2019 ability to trust and rely on AI systems. Visual analytics (VA) provides a compelling solution by combining AI models with interactive visualizations. These specialized charts and graphs empower users to incorporate their domain expertise to refine and improve the models, bridging the gap between AI and human understanding. In this work, the author defines, categorizes, and explores how VA solutions can foster trust across the stages of a typical AI pipeline. The author proposes a design space for innovative visualizations and presents an overview of our previously developed VA dashboards, which support critical tasks within the various pipeline stages, including data processing, feature engineering, hyperparameter tuning, understanding, debugging, refining, and comparing models.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Angelos Chatzimparmpas"}],"award":"","doi":"10.1109/MCG.2025.3533806","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"cb542d49-92c6-4cec-bc55-289fb2a5e04c","image_caption":"","keywords":["Biological system modeling","Visual analytics","Refining","Motion pictures","Iterative methods","Artificial intelligence","Medical diagnostic imaging","Recommender systems","Complexity theory","Problem-solving","Trusted computing","Explainable AI"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2025.3533806","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga2","session_room":"Hall E1","session_room_id":"e1","session_title":"Visual Analytics Methods, Tools, and Infrastructure","session_uid":"cb542d49-92c6-4cec-bc55","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visual Analytics Methods, Tools, and Infrastructure"],"time_stamp":"2025-11-06T10:15:00.000Z","title":"Visual Analytics for Explainable and Trustworthy Artificial Intelligence","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JfetpDJuh5Q"},{"UID":"cdb6dc71-86fb-401f-a559-fa6c53347af5","abstract":"Morse-Cerf theory considers a one-parameter family of smooth\nfunctions defined on a manifold and studies the evolution of their\ncritical points with the parameter. This paper presents an adaptation\nof Morse-Cerf theory to a family of piecewise-linear (PL) functions.\nThe vertex diagram and Cerf diagram are introduced as representations\nof the evolution of critical points of the PL function. The\ncharacterization of a crossing in the vertex diagram based on the\nhomology of the lower links of vertices leads to the definition of a\ntopological descriptor for time-varying scalar fields. An algorithm\nfor computing the Cerf diagram and a measure for comparing two\nCerf diagrams are also described together with experimental results\non time-varying scalar fields.","accessible_pdf":null,"authors":[{"affiliation":"Indian Institute of Science, Bangalore","email":"amritendud@iisc.ac.in","name":"Amritendu Dhar"},{"affiliation":"TCG CREST","email":"apratimn@gmail.com","name":"Apratim Chakraborty"},{"affiliation":"Indian Institute of Science","email":"vijayn@iisc.ac.in","name":"Vijay Natarajan"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"cdb6dc71-86fb-401f-a559-fa6c53347af5","image_caption":"","keywords":["Time-varying scalar fields","topological descriptors","critical points","Morse theory"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.00725","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1268","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short6","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization for Science","session_uid":"cdb6dc71-86fb-401f-a559","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization for Science"],"time_stamp":"2025-11-05T11:09:00.000Z","title":"Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ne2Eq3Co7xU"},{"UID":"cdeb2b03-68fd-4816-8611-544145e35ba2","abstract":"Connected scatterplots visualize time-series data by connecting the points on a scatterplot based on temporal sequence. Viewers are prone to misinterpret the direction of time in these visualizations, possibly because they encode time with an unexpected rule \u2013 along the connected line (TIME IS A LINE) instead of from left to right (RIGHT IS LATER) as conventional in line charts. In this paper, we use the connected scatterplot to illustrate a perspective on visualization comprehension centered around expectations of encoding rules. People have initial expectations of encoding rules for visualizations that can stem from conventional practices or metaphors, and these expectations have been recognized as a potential factor influencing visualization comprehension. We present three preregistered experiments (n = 1429 in total) demonstrating two kinds of design interventions to strengthen the correct expectation for time and testing their effectiveness in reducing errors for understanding realistic connected scatterplots. We found that visual treatments that suppress the incorrect chart-type expectation and directional cues that emphasize the correct expectation both led viewers to expect TIME IS A LINE more. An explicit directional cue (arrows), ideally redundantly encoded with another cue (trace-line effect or animation), was most effective for reducing misinterpretations. Our findings not only provide practical guidelines for designing connected scatterplots but also contribute theoretical insights to inform the design of novel visualizations that challenge interpretability by defying expectations.","accessible_pdf":null,"authors":[{"affiliation":"Northeastern University","email":"wenxuxmn@gmail.com","name":"Wen Xu"},{"affiliation":"Northeastern University","email":"l.padilla@northeastern.edu","name":"Lace Padilla"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"cdeb2b03-68fd-4816-8611-544145e35ba2","image_caption":"","keywords":["Visualization","cognition","graphical convention","conceptual metaphor","connected scatterplot"],"open_access_supplemental_link":"https://osf.io/k45es/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/4mc2b","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1602","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full15","session_room":"Hall E2","session_room_id":"e2","session_title":"From Data to Meaning","session_uid":"cdeb2b03-68fd-4816-8611","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["From Data to Meaning"],"time_stamp":"2025-11-06T10:51:00.000Z","title":"Shifting Expectations for Encoding Rules Mitigates Misinterpretation of Connected Scatterplots","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/v-iY-PKyXHs"},{"UID":"ce206043-1fa9-45b7-801b-2910c1dec585","abstract":"In sports analytics, tactical visualization is widely used to convey valuable insights. However, due to the complex domain knowledge and contextual information involved in tactical visualizations, it is challenging for users to connect high-level tactical insights to corresponding visual patterns. This requires users to engage in a reasoning process to interpret insights within game contexts, which remains insufficiently supported in existing visual-text linking studies. In this work, we propose InsightChaser, a novel approach to bridge tactical insights and soccer visualizations through visual-text linking and visual reasoning enhancement. InsightChaser constructs knowledge graphs to represent both visual elements and contextual game information. Integrating large language models (LLMs), our approach retrieves relevant visual elements and establishes explicit links with insights. Moreover, InsightChaser utilizes LLMs to enhance these visual-text links by providing reasoning explanations and visual effects. We further develop an interactive visualization system that supports navigation and explanation of enhanced visual-text links. Users can explore linked tactical insights interactively and reason through enhanced visual explanations. We conduct two case studies using real-world soccer data and a user study to demonstrate the effectiveness of our approach.","accessible_pdf":null,"authors":[{"affiliation":"Zhejiang University","email":"ziao_liu@outlook.com","name":"Ziao Liu"},{"affiliation":"Zhejiang University","email":"zhao_ws@zju.edu.cn","name":"Wenshuo Zhao"},{"affiliation":"Zhejiang University","email":"xxie@zju.edu.cn","name":"Xiao Xie"},{"affiliation":"Zhejiang University","email":"caoanqi28@163.com","name":"Anqi Cao"},{"affiliation":"Zhejiang University","email":"wuyihong0606@gmail.com","name":"Yihong Wu"},{"affiliation":"Zhejiang University","email":"zhang_hui@zju.edu.cn","name":"Hui Zhang"},{"affiliation":"Zhejiang University","email":"ycwu@zju.edu.cn","name":"Yingcai Wu"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ce206043-1fa9-45b7-801b-2910c1dec585","image_caption":"","keywords":["Sports visualization","tactical analysis","visual-text linking"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1066","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full16","session_room":"Hall M1","session_room_id":"m1","session_title":"Games, Sports, and Music","session_uid":"ce206043-1fa9-45b7-801b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Games, Sports, and Music"],"time_stamp":"2025-11-06T10:15:00.000Z","title":"InsightChaser: Enhancing Visual Reasoning of Sports Tactical Visualization with Visual-Text Linking","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/w72IpVgxklQ"},{"UID":"cf7e7577-abbd-426a-bf21-3f2fbffe5aad","abstract":"Visualization knowledge bases enable computational reasoning and recommendation over a visualization design space. These systems evaluate design trade-offs using numeric weights assigned to different features (e.g., binning a variable). Feature weights can be learned automatically by fitting a model to a collection of chart pairs, in which one chart is deemed preferable to the other. To date, labeled chart pairs have been drawn from published empirical research results; however, such pairs are not comprehensive, resulting in a training corpus that lacks many design variants and fails to systematically assess potential trade-offs. To improve knowledge base coverage and accuracy, we contribute data augmentation techniques for generating and labeling chart pairs. We present methods to generate novel chart pairs based on design permutations and by identifying under-assessed features\u2014leading to an expanded corpus with thousands of new chart pairs, now in need of labels. Accordingly, we next compare varied methods to scale labeling efforts to annotate chart pairs, in order to learn updated feature weights. We evaluate our methods in the context of the Draco knowledge base, demonstrating improvements to both feature coverage and chart recommendation performance.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Washington","email":"hyeokk@uw.edu","name":"Hyeok Kim"},{"affiliation":"University of Washington","email":"jheer@uw.edu","name":"Jeffrey Heer"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"cf7e7577-abbd-426a-bf21-3f2fbffe5aad","image_caption":"","keywords":["Visualization design knowledge base","data augmentation"],"open_access_supplemental_link":"https://osf.io/fqpdh/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.02216","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1016","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full5","session_room":"Hall M2","session_room_id":"m2","session_title":"Authoring and Design","session_uid":"cf7e7577-abbd-426a-bf21","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Authoring and Design"],"time_stamp":"2025-11-05T14:45:00.000Z","title":"Data Augmentation for Visualization Design Knowledge Bases","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/S3LGLxyblpM"},{"UID":"d117a849-8b10-47ad-babd-f387afab0563","abstract":"We present a systematic review on tasks, interactions, and visualization widgets (refer to tangible entities that are used to accomplish data exploration tasks through specific interactions) in the context of tangible data exploration. Tangible widgets have been shown to reduce cognitive load, enable more natural interactions, and support the completion of complex data exploration tasks. Yet, the field lacks a structured understanding of how task types, interaction methods, and widget designs are coordinated, limiting the ability to identify recurring design patterns and opportunities for innovation. To address this gap, we conduct a systematic review to analyze existing work and characterize the current design of data exploration tasks, interactions, and tangible visualization widgets. We next reflect based on our findings and propose a research agenda to inform the development of a future widget design toolkit for tangible data exploration. Our systematic review and supplemental materials are available at physicalviswidget.github.io and osf.io/vjw5e/.","accessible_pdf":"Accessible","authors":[{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"lyeveldie721@gmail.com","name":"Haonan Yao"},{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"lingyun.yu@xjtlu.edu.cn","name":"Lingyun Yu"},{"affiliation":"Xi'an Jiaotong-Liverpool University","email":"yaolijie0219@gmail.com","name":"Lijie Yao"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"d117a849-8b10-47ad-babd-f387afab0563","image_caption":"","keywords":["Visualization widget","Tangible interaction","Data exploration"],"open_access_supplemental_link":"https://osf.io/vjw5e/","open_access_supplemental_question":"Original dataset of our systematic review","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://doi.org/10.48550/arXiv.2507.00775","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1348","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short2","session_room":"Hall M1","session_room_id":"m1","session_title":"Visualization in-the-wild","session_uid":"d117a849-8b10-47ad-babd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization in-the-wild"],"time_stamp":"2025-11-06T08:48:00.000Z","title":"Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LzapyJvE_UY"},{"UID":"d207baab-2afc-4d7a-bbfd-cf81768ec83c","abstract":"Explorative flow visualization allows domain experts to analyze complex flow structures by interactively investigating flow patterns. However, traditional visual interfaces often rely on specialized graphical representations and interactions, which require additional effort to learn and use. Natural language interaction offers a more intuitive alternative, but teaching machines to recognize diverse scientific concepts and extract corresponding structures from flow data poses a significant challenge. In this paper, we introduce an automated framework that aligns flow pattern representations with the semantic space of large language models (LLMs), eliminating the need for manual labeling. Our approach encodes streamline segments using a denoising autoencoder and maps the generated flow pattern representations to LLM embeddings via a projector layer. This alignment empowers semantic matching between textual embeddings and flow representations through an attention mechanism, enabling the extraction of corresponding flow patterns based on textual descriptions. To enhance accessibility, we develop an interactive interface that allows users to query and visualize flow structures using natural language. Through case studies, we demonstrate the effectiveness of our framework in enabling intuitive and intelligent flow exploration.","accessible_pdf":null,"authors":[{"affiliation":"Sun Yat-sen University","email":"zhangwh79@mail2.sysu.edu.cn","name":"Weihan Zhang"},{"affiliation":"Sun Yat-sen University","email":"taoj23@mail.sysu.edu.cn","name":"Jun Tao"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"d207baab-2afc-4d7a-bbfd-cf81768ec83c","image_caption":"","keywords":["Flow visualization","natural language","streamlines"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.06300","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1349","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full34","session_room":"Hall M2","session_room_id":"m2","session_title":"Vis & Language","session_uid":"d207baab-2afc-4d7a-bbfd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Vis & Language"],"time_stamp":"2025-11-06T10:27:00.000Z","title":"Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JQEgOnSg18o"},{"UID":"d22e24df-9562-498b-96d4-e9b19056c72d","abstract":"Traditional volume visualization (VolVis) methods, like direct volume rendering, suffer from rigid transfer function designs and high computational costs. Although novel view synthesis approaches enhance rendering efficiency, they require additional learning effort for non-experts and lack support for semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an interactive system that enables users to explore, query, and edit volumetric scenes using natural language. NLI4VolVis integrates multi-view semantic segmentation and vision-language models to extract and understand semantic components in a scene. We introduce a multi-agent large language model architecture equipped with extensive function-calling tools to interpret user intents and execute visualization tasks. The agents leverage external tools and declarative VolVis commands to interact with the VolVis engine powered by 3D editable Gaussians, enabling open-vocabulary object querying, real-time scene editing, best-view selection, and 2D stylization. We validate our system through case studies and a user study, highlighting its improved accessibility and usability in volumetric data exploration. We strongly recommend readers check out our case studies, demo video, and source code at https://nli4volvis.github.io/.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Notre Dame","email":"kai@nd.edu","name":"Kuangshi Ai"},{"affiliation":"University of Notre Dame","email":"ktang2@nd.edu","name":"Kaiyuan Tang"},{"affiliation":"University of Notre Dame","email":"chaoli.wang@nd.edu","name":"Chaoli Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"d22e24df-9562-498b-96d4-e9b19056c72d","image_caption":"","keywords":["Volume visualization","novel view synthesis","natural language interaction","open-vocabulary querying","editable 3D Gaussian splatting","multi-agent collaboration"],"open_access_supplemental_link":"https://github.com/KuangshiAi/nli4volvis","open_access_supplemental_question":"We provide open-source code, a detailed demo video, and a project page showcasing diverse case studies across multiple datasets. These materials support transparency and reproducibility of our novel multi-agent natural language interaction system for volume visualization.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.12621","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1444","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full6","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Best Full Papers & Test of Time Awards","session_uid":"d22e24df-9562-498b-96d4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Best Full Papers & Test of Time Awards"],"time_stamp":"2025-11-04T15:04:00.000Z","title":"NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/eNFqK9YEju4"},{"UID":"d2d68e75-88a1-4f4a-a1c5-7d8cb6706524","abstract":"Virtual Reality (VR) broadcasting has emerged as a promising medium for providing immersive viewing experiences of major sports events such as tennis. However, current VR broadcast systems often lack an effective camera language and do not adequately incorporate dynamic, in-game visualizations, limiting viewer engagement and narrative clarity. To address these limitations, we analyze 400 out-of-play segments from eight major tennis broadcasts to develop a tennis-specific design framework that effectively combines cinematic camera movements with embedded visualizations. We further refine our framework by examining 25 cinematic VR animations, comparing their camera techniques with traditional tennis broadcasts to identify key differences and inform adaptations for VR. Based on data extracted from the broadcast videos, we reconstruct a simulated game that captures the players' and ball's motion and trajectories. Leveraging this design framework and processing pipeline, we develope Beyond the Broadcast, a VR tennis viewing system that integrates embedded visualizations with adaptive camera motions to construct a comprehensive and engaging narrative. Our system dynamically overlays tactical information and key match events onto the simulated environment, enhancing viewer comprehension and narrative engagement while ensuring perceptual immersion and viewing comfort. A user study involving tennis viewers demonstrate that our approach outperforms traditional VR broadcasting methods in delivering an immersive, informative viewing experience.","accessible_pdf":"Accessible","authors":[{"affiliation":"Fudan University","email":"peter650059@hotmail.com","name":"Jun-Hsiang Yao"},{"affiliation":"Fudan University","email":"23110980031@m.fudan.edu.cn","name":"Jielin Feng"},{"affiliation":"Hunan University","email":"xinfangtian0206@163.com","name":"Xinfang Tian"},{"affiliation":"University of Nottingham","email":"kai.xu@nottingham.ac.uk","name":"Kai Xu"},{"affiliation":"Al-Farabi Kazakh National University","email":"gulshat.aa@gmail.com","name":"Gulshat Amirkhanova"},{"affiliation":"Fudan University","email":"simingchen3@gmail.com","name":"Siming Chen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"d2d68e75-88a1-4f4a-a1c5-7d8cb6706524","image_caption":"","keywords":["Tennis","VR Broadcasting","Embedded Visualization","Camera Motion","Immersive Experience"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.20006","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1732","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full16","session_room":"Hall M1","session_room_id":"m1","session_title":"Games, Sports, and Music","session_uid":"d2d68e75-88a1-4f4a-a1c5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Games, Sports, and Music"],"time_stamp":"2025-11-06T10:27:00.000Z","title":"Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/w72IpVgxklQ"},{"UID":"d3492f20-d356-4c55-992d-dbc0fa0fe0af","abstract":"Random forests are a machine learning method used to automatically classify datasets and consist of a multitude of decision trees. While these random forests often have higher performance and generalize better than a single decision tree, they are also harder to interpret. This paper presents a visualization method and system to increase interpretability of random forests. We cluster similar trees which enables users to interpret how the model performs in general without needing to analyze each individual decision tree in detail, or interpret an oversimplified summary of the full forest. To meaningfully cluster the decision trees, we introduce a new distance metric that takes into account both the decision rules as well as the predictions of a pair of decision trees. We also propose two new visualization methods that visualize both clustered and individual decision trees: (1) The Feature Plot, which visualizes the topological position of features in the decision trees, and (2) the Rule Plot, which visualizes the decision rules of the decision trees. We demonstrate the efficacy of our approach through a case study on the \u201cGlass\u201d dataset, which is a relatively complex standard machine learning dataset, as well as a small user study.","accessible_pdf":"Accessible","authors":[{"affiliation":"Maastricht University","email":"max.sondag@hotmail.com","name":"Max Sondag"},{"affiliation":"Leipzig University","email":"cmeinecke@informatik.uni-leipzig.de","name":"Christofer Meinecke"},{"affiliation":"Eindhoven University of Technology","email":"d.a.c.collaris@tue.nl","name":"Dennis Collaris"},{"affiliation":"University of Cologne","email":"landesberger@cs.uni-koeln.de","name":"Tatiana von Landesberger"},{"affiliation":"Eindhoven University of Technology","email":"s.j.v.d.elzen@tue.nl","name":"Stef van den Elzen"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"d3492f20-d356-4c55-992d-dbc0fa0fe0af","image_caption":"","keywords":["Random Forest","Decision Tree","Tree clustering"],"open_access_supplemental_link":"https://github.com/maxie12/RandomForestVis","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.22665","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1432","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full12","session_room":"Hall E1","session_room_id":"e1","session_title":"Explanation, Exploration, and Model Configuration","session_uid":"d3492f20-d356-4c55-992d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explanation, Exploration, and Model Configuration"],"time_stamp":"2025-11-06T08:42:00.000Z","title":"Cluster-Based Random Forest Visualization and Interpretation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/h8CsAxh4LWg"},{"UID":"d410cc14-60e6-4301-976d-3e142bd3f2e2","abstract":"This article presents a review and analysis of evaluation practices within the visualization and visual analytics (VIS) domain, with a focus on domain application work accepted at the IEEE VIS conference from 2018 to 2022. Through the analysis of 140 pertinent article, we establish a detailed classification principle for evaluation practices, using the Who, When, What, and How indicators. This principle covers facets such as analysis methods, targets, scenarios, participant expertise, and stages of occurrence. By systematically categorizing the application domains presented in these works, we apply our established classification principle to discern and categorize the evaluation practices within them, identifying the prevailing characteristics and trends. The article explores the variety of evaluation methods employed across different application domains and observes the distinctions in their usage. In conclusion, we provide insights and highlight concerns for conducting evaluations in upcoming domain application research. Our findings are intended to inform and guide subsequent studies in a similar context.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yiwen Xing"},{"email":null,"name":"Gabriel Dias Cantareira"},{"email":null,"name":"Rita Borgo"},{"email":null,"name":"Alfie Abdul-Rahman"}],"award":"","doi":"10.1109/TVCG.2024.3460181","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"d410cc14-60e6-4301-976d-3e142bd3f2e2","image_caption":"","keywords":["Data visualization","Market research","Reviews","Artificial intelligence","Visual analytics","Filters","Bibliographies"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://kclpure.kcl.ac.uk/ws/portalfiles/portal/300746529/Evaluation_TVCG_camera_ready.pdf","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024.3460181]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full21","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Models, Methods, and Typologies","session_uid":"d410cc14-60e6-4301-976d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Models, Methods, and Typologies"],"time_stamp":"2025-11-07T08:54:00.000Z","title":"A Review and Analysis of Evaluation Practices in VIS Domain Applications","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/moykjX8QeZI"},{"UID":"d41df7c8-a398-4c22-a542-fd9dfff83de3","abstract":"AI is the workhorse of modern data analytics and omnipresent across many sectors. Large language models and multimodal foundation models are today capable of generating code, charts, visualizations, etc. How will these massive developments of AI in data analytics shape future data visualizations and visual analytics workflows? What is the potential of AI to reshape methodology and design of future visual analytics applications? What will be our role as visualization researchers in the future? What are opportunities, open challenges, and threats in the context of an increasingly powerful AI? This Visualization Viewpoints discusses these questions in the special context of biomedical data analytics as an example of a domain in which critical decisions are taken based on complex and sensitive data, with high requirements on transparency, efficiency, and reliability. We map recent trends and developments in AI on the elements of interactive visualization and visual analytics workflows and highlight the potential of AI to transform biomedical visualization as a research field. Given that agency and responsibility have to remain with human experts, we argue that it is helpful to keep the focus on human-centered workflows, and to use visual analytics as a tool for integrating \u201cAI-in-the-loop.\u201d This is in contrast to the more traditional term \u201chuman-in-the-loop.\u201d which focuses on incorporating human expertise into AI-based systems.","accessible_pdf":"No","authors":[{"email":null,"name":"Katja B\u00fchler"},{"email":null,"name":"Thomas H\u00f6llt"},{"email":null,"name":"Thomas Schultz"},{"email":null,"name":"Pere-Pau V\u00e1zquez"}],"award":"","doi":"10.1109/MCG.2024.3517293","event_id":"v-cga","event_title":"CG&A Invited Partnership Presentations","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"d41df7c8-a398-4c22-a542-fd9dfff83de3","image_caption":"","keywords":["Data analysis","Shape measurement","Visual analytics","Large language models","Data visualization","Transforms","Market research","Human in the loop","Reliability","Artificial intelligence","Generative AI","Biomedical image processing"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.15876","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/MCG.2024.3517293","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"cga1","session_room":"Room 0.96 + 0.97","session_room_id":"0_96_0_97","session_title":"Reflections and Looking Forward","session_uid":"d41df7c8-a398-4c22-a542","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Reflections and Looking Forward"],"time_stamp":"2025-11-07T09:18:00.000Z","title":"AI-in-The-Loop: The Future of Biomedical Visual Analytics Applications in the Era of AI","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/29PgqT6NWUQ"},{"UID":"d916cf6d-cda6-481e-b404-40f8fb053501","abstract":"Implicit neural representation (INR) has been a powerful paradigm for effectively compressing time-varying volumetric data. However, the optimization process can span days or even weeks due to its reliance on coordinate-based inputs and outputs for modeling volumetric data. To address this issue, we introduce a divide-and-conquer INR (DCINR), significantly accelerating the compressing process of time-varying volumetric data in hours. Our approach starts by dividing the data set into a set of non-overlapping blocks. Then, we apply a block selection strategy to weed out redundant blocks to reduce the computation cost without sacrificing performance. In parallel, each selected block is modeled by a tiny INR, with the size of the INR being adapted to match the information richness in the block. The block size is determined by maximizing the average network capacity. After optimization, the optimized INRs are utilized to decompress the data set. By evaluating our approach across various time-varying volumetric data sets, DCINR surpasses learning-based and lossy compression approaches in compression ratio, visual fidelity, and various performance metrics. Additionally, this method operates within a comparable compression time to that of lossy compressors, achieves extreme compression ratios ranging from thousands to tens of thousands, and preserves features with high quality.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Jun Han"},{"email":null,"name":"Fan Yang"}],"award":"","doi":"10.1109/TVCG.2025.3564255","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"d916cf6d-cda6-481e-b404-40f8fb053501","image_caption":"","keywords":["Spatiotemporal phenomena","Training","Data models","Optimization","Data compression","Image coding","Data visualization","Partitioning algorithms","Graphics processing units","Entropy"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3564255","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full36","session_room":"Hall M1","session_room_id":"m1","session_title":"Volume","session_uid":"d916cf6d-cda6-481e-b404","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Volume"],"time_stamp":"2025-11-05T13:24:00.000Z","title":"DCINR: a Divide-and-Conquer Implicit Neural Representation for Compressing Time-Varying Volumetric Data in Hours","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-M293vb-AIg"},{"UID":"d997f9c2-6bfc-421f-bfe6-26e0072cbe76","abstract":"We present an authoring tool, called CAST+ (Canis Studio Plus), that enables the interactive creation of chart animations through the direct manipulation of keyframes. It introduces the visual specification of chart animations consisting of keyframes that can be played sequentially or simultaneously, and animation parameters (e.g., duration, delay). Building on Canis (Ge et al. 2020), a declarative chart animation grammar that leverages data-enriched SVG charts, CAST+ supports auto-completion for constructing both keyframes and keyframe sequences. It also enables users to refine the animation specification (e.g., aligning keyframes across tracks to play them together, adjusting delay) with direct manipulation. We report a user study conducted to assess the visual specification and system usability with its initial version. We enhanced the system's expressiveness and usability: CAST+ now supports the animation of multiple types of visual marks in the same keyframe group with new auto-completion algorithms based on generalized selection. This enables the creation of more expressive animations, while reducing the number of interactions needed to create comparable animations. We present a gallery of examples and four usage scenarios to demonstrate the expressiveness of CAST+. Finally, we discuss the limitations, comparison, and potentials of CAST+ as well as directions for future research.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yuancheng Shen"},{"email":null,"name":"Yue Zhao"},{"email":null,"name":"Yunhai Wang"},{"email":null,"name":"Tong Ge"},{"email":null,"name":"Haoyan Shi"},{"email":null,"name":"Bongshin Lee"}],"award":"","doi":"10.1109/TVCG.2024.3491504","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"d997f9c2-6bfc-421f-bfe6-26e0072cbe76","image_caption":"","keywords":["Animation","Visualization","Programming","Grammar","Data visualization","Authoring systems","Planning","Delays","Videos","Usability"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3491504","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full19","session_room":"Hall E1","session_room_id":"e1","session_title":"Interaction, Provenance, and Collaboration","session_uid":"d997f9c2-6bfc-421f-bfe6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Interaction, Provenance, and Collaboration"],"time_stamp":"2025-11-06T15:09:00.000Z","title":"Authoring Data-Driven Chart Animations Through Direct Manipulation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/253qu_2o0K0"},{"UID":"da094703-efa1-46d2-8f43-76453818ceb3","abstract":"Dimensionality reduction (DR) techniques help analysts to understand patterns in high-dimensional spaces. These techniques, often represented by scatter plots, are employed in diverse science domains and facilitate similarity analysis among clusters and data samples. For datasets containing many granularities or when analysis follows the information visualization mantra, hierarchical DR techniques are the most suitable approach since they present major structures beforehand and details on demand. This work presents HUMAP, a novel hierarchical dimensionality reduction technique designed to be flexible on preserving local and global structures and preserve the mental map throughout hierarchical exploration. We provide empirical evidence of our technique\u2019s superiority compared with current hierarchical approaches and show a case study applying HUMAP for dataset labelling.","accessible_pdf":"No","authors":[{"email":null,"name":"Wilson E. Marc\u00edlio-Jr"},{"email":null,"name":"Danilo M. Eler"},{"email":null,"name":"Fernando V. Paulovich"},{"email":null,"name":"Rafael M. Martins"}],"award":"","doi":"10.1109/TVCG.2024.3471181","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"da094703-efa1-46d2-8f43-76453818ceb3","image_caption":"","keywords":["Layout","Dimensionality reduction","Manifolds","Visualization","Kernel","Organizations","Graphics processing units","Engines","Deep learning","Data visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2106.07718","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG3471181]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full11","session_room":"Hall E2","session_room_id":"e2","session_title":"Embeddings & Metrics","session_uid":"da094703-efa1-46d2-8f43","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Embeddings & Metrics"],"time_stamp":"2025-11-05T13:48:00.000Z","title":"HUMAP: Hierarchical Uniform Manifold Approximation and Projection","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gvbjZE1CgLQ"},{"UID":"da138859-6a6c-4df2-813b-6a70e20d04a1","abstract":"In-depth analysis of competitive debates is essential for participants to develop argumentative skills and refine strategies, and further improve their debating performance. However, manual analysis of unstructured and unlabeled textual records of debating is time-consuming and ineffective, as it is challenging to reconstruct contextual semantics and track logical connections from raw data. To address this, we propose Conch, an interactive visualization system that systematically analyzes both what is debated and how it is debated. In particular, we propose a novel parallel spiral visualization that compactly traces the multidimensional evolution of clash points and participant interactions throughout debate process. In addition, we leverage large language models with well-designed prompts to automatically identify critical debate elements such as clash points, disagreements, viewpoints, and strategies, enabling participants to understand the debate context comprehensively. Finally, through two case studies on real-world debates and a carefully-designed user study, we demonstrate Conch's effectiveness and usability for competitive debate analysis.","accessible_pdf":"Accessible","authors":[{"affiliation":"Huazhong University of Science and Technology","email":"qianhechen01@gmail.com","name":"Qianhe Chen"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"},{"affiliation":"Huazhong University of Science and Technology","email":"sakaaanayu@gmail.com","name":"Yixin Yu"},{"affiliation":"Huazhong University of Science and Technology","email":"addinistrator.fallindepart@gmail.com","name":"Xiyuan Zhu"},{"affiliation":"Huazhong University of Science and Technology","email":"2874849065@qq.com","name":"Xuerou Yu"},{"affiliation":"School of Journalism and Information Communication, Huazhong University of Science and Technology","email":"rex_wang@hust.edu.cn","name":"Ran Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"da138859-6a6c-4df2-813b-6a70e20d04a1","image_caption":"","keywords":["Competitive debate","debate analysis","clash point","visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Our work introduces Conch, an interactive system that analyzes competitive debates through a novel spiral visualization and a hierarchical semantic framework. We provide all prompts, the full strategy framework, and the mathematical model of Conch in the supplemental materials, and we plan to open-source the implementation after further organization and documentation.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.14482/","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1157","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full3","session_room":"Hall E1","session_room_id":"e1","session_title":"Analysts, Assemble!","session_uid":"da138859-6a6c-4df2-813b","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analysts, Assemble!"],"time_stamp":"2025-11-06T13:00:00.000Z","title":"Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/fkfdha6pCeI"},{"UID":"dc011ac4-7257-4423-9aa4-a46452c10067","abstract":"Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines---providing a blank chart and a chart with mismatched data---shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.","accessible_pdf":null,"authors":[{"affiliation":"Harvard","email":"jlsun@college.harvard.edu","name":"Johnathan Sun"},{"affiliation":"Harvard","email":"vrli@college.harvard.edu","name":"Victoria Li"},{"affiliation":"Harvard","email":"wattenberg@gmail.com","name":"Martin Wattenberg"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"dc011ac4-7257-4423-9aa4-a46452c10067","image_caption":"","keywords":["AI","Workflow Design","Human-Machine Analysis."],"open_access_supplemental_link":"https://github.com/johnathansun/lvlm-vis-data-understanding","open_access_supplemental_question":"Our source code is thoroughly documented, and we provide original, full datasets on Github for end-to-end verification and reproducibility. Our supplemental material contains additional analytics and graphics to document our experimental setup and results.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1345","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"dc011ac4-7257-4423-9aa4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T11:09:00.000Z","title":"Does visualization help AI understand data?","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"dde3f095-6834-4f0c-b006-206d9e2e9e69","abstract":"This research discusses the figurative tensions that arise when using portraits to represent individuals behind a dataset. In the broader effort to communicate European data related to depression, the Kiel Science Communication Network (KielSCN) team attempted to engage a wider audience by combining interactive data graphics with AI-generated images of people. This article examines the project\u2019s decisions and results, reflecting on the reaction from the audience when information design incorporates figurative representations of individuals within the data.","accessible_pdf":null,"authors":[{"affiliation":"Kiel Science Communication Network","email":"jahrend@hfk-bremen.de","name":"Julia C. Ahrend"},{"affiliation":"Muthesius University  of Fine Arts & Design","email":"bdoege@kielscn.de","name":"Bj\u00f6rn D\u00f6ge"},{"affiliation":"Kiel Science Communication Network","email":"td@muthesius.de","name":"Tom Duscher"},{"affiliation":"University of Groningen","email":"d.rodighiero@rug.nl","name":"Dario Rodighiero"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"dde3f095-6834-4f0c-b006-206d9e2e9e69","image_caption":"","keywords":["Visual Science Communication; Interactive Information Design; AI-Generated Images"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1103","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap1","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Pictorial","session_uid":"dde3f095-6834-4f0c-b006","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Pictorial"],"time_stamp":"2025-11-06T11:03:00.000Z","title":"AI-Generated Images for Representing Individuals \u2013 Navigating the Thin Line Between Care and Bias","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/NZ0Wwrqv6fA"},{"UID":"de570d65-b80f-4e25-9d54-88f11281e2c5","abstract":"Effective data visualizations enhance perception, support cognitive processing, and facilitate informed decision-making by aligning with human perceptual strengths. Conversely, poorly designed visualizations can impede comprehension, introduce interpretive bias, and diminish the perceived credibility of the conveyed message. This paper investigates the extent to which visual embellishments influence perceived message credibility in data visualizations. We conducted two crowdsourced experiments to examine both holistic and component-level effects of embellishment. In the first experiment, participants evaluated the relative credibility of plain bar charts versus two embellished variants\u2014cartoon-style and image-style\u2014across topics. Participants provided both comparative judgments and qualitative feedback. In the second experiment, we systematically isolated the influence of specific design elements\u2014color, font, and bar style\u2014on credibility perceptions through controlled variations. Our findings reveal that the impact of embellishments on perceived message credibility is complex and context-dependent. While certain embellishments, such as the use of color and image style bars, enhanced credibility, others\u2014most notably hand-drawn fonts and cartoon-style bars\u2014significantly undermined it. By operationalizing trust through the lens of message credibility, this work offers empirical insight into the design factors that shape viewers' perceptions. We conclude by proposing actionable design guidelines to support the creation of visualizations that are effective for communication and credible.","accessible_pdf":null,"authors":[{"affiliation":"Georgia Institute of Technology","email":"hsong300@gatech.edu","name":"Hayeong Song"},{"affiliation":"Georgia Institute of Technology","email":"aeree@gatech.edu","name":"Aeree Cho"},{"affiliation":"Georgia Tech","email":"cxiong@gatech.edu","name":"Cindy Xiong Bearfield"},{"affiliation":"Georgia Institute of Technology","email":"john.stasko@cc.gatech.edu","name":"John Stasko"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"de570d65-b80f-4e25-9d54-88f11281e2c5","image_caption":"","keywords":["Data Visualization","Visual Embellishment","Trust","Credibility","Chart Design","Perception"],"open_access_supplemental_link":"https://osf.io/ph4b8/files/osfstorage","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1250","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full30","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Trust No One","session_uid":"de570d65-b80f-4e25-9d54","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Trust No One"],"time_stamp":"2025-11-07T08:54:00.000Z","title":"Visualizing Trust: How Chart Embellishments Influence Perceptions of Credibility","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mavB31i3NwM"},{"UID":"deb36a29-0227-4522-a7d0-42623a49081e","abstract":"While word clouds pack a lot of data into a relatively small area, it is unclear whether readers actually benefit from all of that information, or if they are only processing a few of the words shown. We sought to determine if words outside of a reader's central vision were contributing to their interpretation by leveraging the semantic priming effect: a phenomenon in which participants will more quickly recognize a word if they have been recently primed with a word that is semantically related. We presented participants with word clouds containing related and unrelated words to see whether they might prime this lexical decision task more strongly than single words alone. We showed that the peripheral contents of a cloud do affect participant performance at this task, though more work is needed to understand the impacts of these differences in real-world settings.","accessible_pdf":"Accessible","authors":[{"affiliation":"Stanford University","email":"xingyiz@stanford.edu","name":"Xingyi Zhang"},{"affiliation":"Carleton College","email":"jakejasmer@gmail.com","name":"Jake Jasmer"},{"affiliation":"Carleton College","email":"masaddee@carleton.edu","name":"Ethan Masadde"},{"affiliation":"Carleton College","email":"ealexander@carleton.edu","name":"Eric Alexander"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"deb36a29-0227-4522-a7d0-42623a49081e","image_caption":"","keywords":["word clouds","semantic priming","text perception"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1328","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short7","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Perception & Semantics","session_uid":"deb36a29-0227-4522-a7d0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Perception & Semantics"],"time_stamp":"2025-11-06T15:30:00.000Z","title":"Semantic Priming in Word Clouds","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/b4RjKKX9erE"},{"UID":"dee3e240-f398-4a18-8460-cb8f4ad56df1","abstract":"Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demonstrate the capabilities of this workflow with multiple case studies on networks derived from social media usage and also evaluate the workflow with qualitative feedback from experts.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Hsiao-Ying Lu"},{"email":null,"name":"Takanori Fujiwara"},{"email":null,"name":"Ming-Yi Chang"},{"email":null,"name":"Yang-chih Fu"},{"email":null,"name":"Anders Ynnerman"},{"email":null,"name":"Kwan-Liu Ma"}],"award":"","doi":"10.1109/TVCG.2024.3423728","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"dee3e240-f398-4a18-8460-cb8f4ad56df1","image_caption":"","keywords":["Artificial neural networks","Visual analytics","Feature extraction","Semantics","Social networking (online)","Data mining","Vectors"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Our work emphasizes interpretability through a key design feature: a composite variable construction step that transforms complex, nonlinear neural network outputs into intuitive, linear features. This process is clearly documented and fully reproducible. Additionally, our visual analytics interface integrates expert knowledge, enabling users to construct and test hypotheses about combinatorial effects of variables with ease.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2303.09590","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-02-0125]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full17","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Graphs and Networks","session_uid":"dee3e240-f398-4a18-8460","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Graphs and Networks"],"time_stamp":"2025-11-05T15:45:00.000Z","title":"Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zH5jvaUZHGk"},{"UID":"dfb38658-4e3c-42b8-9333-ac8f04df3106","abstract":"Game-Based Learning has proven to be an effective method for enhancing engagement with educational material. However, gaining a deeper understanding of player strategies remains challenging. Sequential game-state and action-based tracking tools often gather extensive data that can be difficult to interpret as long-term strategy. This data presents unique problems to visualization, as it can be fairly natural, noisy data but is constrained within synthetic, controlled environments, leading to issues such as overplotting which can make interpretation complicated. We propose an animated visual encoding tool that utilizes kinetic visualization to address these issues. This tool enables researchers to construct animated data narratives through the configuration of parameter interpolation curves and blending layers. Finally, we demonstrate the usefulness of the tool while addressing specific interests as outlined by a domain expert collaborator.","accessible_pdf":"Accessible","authors":[{"affiliation":"The University of Oklahoma","email":"bradenroper@ou.edu","name":"Braden Roper"},{"affiliation":"The University of Oklahoma","email":"will.thompson@ou.edu","name":"William Thompson"},{"affiliation":"University of Oklahoma","email":"cweaver@ou.edu","name":"Chris Weaver"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"dfb38658-4e3c-42b8-9333-ac8f04df3106","image_caption":"","keywords":["Kinetic visualization","kinetic queries","animated encoding","game-based learning"],"open_access_supplemental_link":"https://osf.io/tf245","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.01134","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1277","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full16","session_room":"Hall M1","session_room_id":"m1","session_title":"Games, Sports, and Music","session_uid":"dfb38658-4e3c-42b8-9333","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Games, Sports, and Music"],"time_stamp":"2025-11-06T11:15:00.000Z","title":"Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/w72IpVgxklQ"},{"UID":"dfb477b8-fd21-4346-b7db-27bbbc2b16a0","abstract":"We present a novel diffusion-based framework for synthesizing 2D vector fields from sparse, coherent inputs (i.e., streamlines) while maintaining physical plausibility. Our method employs a conditional denoising diffusion probabilistic model with classifier-free guidance, enabling progressive reconstruction that preserves both geometric and physical constraints. Experimental results demonstrate our method's ability to synthesize plausible vector fields that adhere to physical laws while maintaining fidelity to sparse input observations, outperforming traditional optimization-based approaches in terms of flexibility and physical consistency.","accessible_pdf":null,"authors":[{"affiliation":"University of Houston","email":"nguyenpkk95@gmail.com","name":"Nguyen Phan"},{"affiliation":"University of Houston","email":"ricsf2000@gmail.com","name":"Ricardo Morales Vargas"},{"affiliation":"University of Houston","email":"sdelaesp@cougarnet.uh.edu","name":"Sebastian Espriella"},{"affiliation":"University of Houston","email":"guoning.chen@gmail.com","name":"Guoning Chen"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"dfb477b8-fd21-4346-b7db-27bbbc2b16a0","image_caption":"","keywords":["Vector field synthesis","diffusion models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1320","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short11","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Short Paper Session: Multi-modal and Spatial","session_uid":"dfb477b8-fd21-4346-b7db","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Short Paper Session: Multi-modal and Spatial"],"time_stamp":"2025-11-06T08:39:00.000Z","title":"Vector Field Synthesis with Sparse Streamlines Using Diffusion Model","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"e14aa8da-3794-45ba-9dd7-5e0ced3449c7","abstract":"Detecting and interpreting common patterns in relational data is crucial for understanding complex topological structures across various domains. These patterns, or network motifs, can often be detected algorithmically. However, visual inspection remains vital for exploring and discovering patterns. This paper focuses on presenting motifs within BioFabric network visualizations---a unique technique that opens opportunities for research on scaling to larger networks, design variations, and layout algorithms to better expose motifs. Our goal is to show how highlighting motifs can assist users in identifying and interpreting patterns in BioFabric visualizations. To this end, we leverage existing motif simplification techniques. We replace edges with glyphs representing fundamental motifs such as staircases, cliques, paths, and connector nodes. The results of our controlled experiment and usage scenarios demonstrate that motif simplification for BioFabric is useful for detecting and interpreting network patterns. Our participants were faster and more confident using the simplified view without sacrificing accuracy. The efficacy of our current motif simplification approach depends on which extant layout algorithm is used. We hope our promising findings on user performance will motivate future research on layout algorithms tailored to maximizing motif presentation. Our supplemental material is available at https://osf.io/f8s3g/?view_only=7e2df9109dfd4e6c85b89ed828320843","accessible_pdf":null,"authors":[{"affiliation":"University of Konstanz","email":"fuchs@dbvis.inf.uni-konstanz.de","name":"Johannes Fuchs"},{"affiliation":"Northeastern University","email":"c.dunne@northeastern.edu","name":"Cody Dunne"},{"affiliation":"University of Konstanz","email":"maria-viktoria.heinle@uni-konstanz.de","name":"Maria-Viktoria Heinle"},{"affiliation":"University of Konstanz","email":"keim@uni-konstanz.de","name":"Daniel Keim"},{"affiliation":"TU Wien","email":"dibartolomeo.sara@gmail.com","name":"Sara Di Bartolomeo"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e14aa8da-3794-45ba-9dd7-5e0ced3449c7","image_caption":"","keywords":["BioFabric","Network Visualization","Motif Simplification","Glyph Design","Quantitative Experiment"],"open_access_supplemental_link":"https://osf.io/f8s3g/?view_only=7e2df9109dfd4e6c85b89ed828320843","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://osf.io/preprints/osf/5d9q6_v1","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1698","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full17","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Graphs and Networks","session_uid":"e14aa8da-3794-45ba-9dd7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Graphs and Networks"],"time_stamp":"2025-11-05T14:57:00.000Z","title":"Motif Simplification for BioFabric Network Visualizations: Improving Pattern Recognition and Interpretation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/zH5jvaUZHGk"},{"UID":"e201eb31-fc6e-4bc1-8741-5f4ee5f98e1e","abstract":"Sensitivity analyses of simulation ensembles determine how simulation parameters influence the simulation's outcome. Commonly, one global numerical sensitivity value is computed per simulation parameter. However, when considering 3D spatial simulations, the analysis of localized sensitivities in different spatial regions is of importance in many applications. For analyzing the spatial variation of parameter sensitivity, one needs to compute a spatial sensitivity scalar field per simulation parameter. Given n simulation parameters, we obtain multi-field data consisting of n scalar fields when considering all simulation parameters. We propose an interactive visual analytics solution to analyze the multi-field sensitivity data. It supports the investigation of how strongly and in what way individual parameters influence the simulation outcome, in which spatial regions this is happening, and what the interplay of the simulation parameters is. Its central component is an overview visualization of all sensitivity fields that avoids 3D occlusions by linearizing the data using an adapted scheme of data-driven space-filling curves. The spatial sensitivity values are visualized in a combination of a Horizon Graph and a line chart. We validate our approach by applying it to synthetic and real-world ensemble data.","accessible_pdf":"No","authors":[{"email":null,"name":"Marina Evers"},{"email":null,"name":"Simon Leistikow"},{"email":null,"name":"Hennes Rave"},{"email":null,"name":"Lars Linsen"}],"award":"","doi":"10.1109/TVCG.2024.3433001","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"e201eb31-fc6e-4bc1-8741-5f4ee5f98e1e","image_caption":"","keywords":["Sensitivity analysis","Data visualization","Three-dimensional displays","Rendering (computer graphics)","Task analysis","Analytical models","Visual analytics"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2408.03817","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-02-0090]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full10","session_room":"Hall E2","session_room_id":"e2","session_title":"Dimensionality Reduction and Parameter Space Analysis","session_uid":"e201eb31-fc6e-4bc1-8741","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dimensionality Reduction and Parameter Space Analysis"],"time_stamp":"2025-11-05T15:33:00.000Z","title":"Interactive Visual Analysis of Spatial Sensitivities","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/lJXNA9vtCPA"},{"UID":"e3b6a06f-ce81-4ceb-a411-6cd298b41326","abstract":"Given the difficulties and inherent problems of data, how do we approach data representations in a decolonial way? This annotated portfolio by Kontinentalist illustrates our evolving practice and attempt at forging a path grounded in a community manifesto that looks to challenge the need for accuracy and certainty, centring design in Indigenous vernacular and knowledge, and challenging reductivism.","accessible_pdf":null,"authors":[{"affiliation":"Kontinentalist","email":"peiying@kontinentalist.com","name":"Pei Ying Loh"},{"affiliation":"Kontinentalist","email":"nabilah@kontinentalist.com","name":"Nabilah Said"},{"affiliation":"Kontinentalist","email":"griselda@kontinentalist.com","name":"Griselda Gabriele"},{"affiliation":"Kontinentalist","email":"munirah@kontinentalist.com","name":"Munirah Mansoor"},{"affiliation":"Kontinentalist","email":"zafirah@kontinentalist.com","name":"Zafirah Zein"},{"affiliation":"Kontinentalist","email":"amanda@kontinentalist.com","name":"Amanda Teo"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e3b6a06f-ce81-4ceb-a411-6cd298b41326","image_caption":"","keywords":["Data Storytelling","Decolonial Methodologies","Coloniality","Data Feminism","Indigenous Knowledge"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1046","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap1","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Pictorial","session_uid":"e3b6a06f-ce81-4ceb-a411","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Pictorial"],"time_stamp":"2025-11-06T11:15:00.000Z","title":"Rejecting Colonial Practices in Data Storytelling","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/NZ0Wwrqv6fA"},{"UID":"e4f2ed49-3ceb-485d-8d71-8463039a522b","abstract":"Human-AI collaborative tools attract attentions from the data storytelling community to lower the barrier of expertise and streamline the workflow. The recent advance in large-scale generative AI techniques, e.g., large language models (LLMs) and text-to-image models, has the potential to enhance data storytelling with their power in visual and narration generation. After two years since these techniques were publicly available, it is important to reflect our progress of applying them and have an outlook for future opportunities. To achieve the goal, we compare the collaboration patterns of the latest tools with those of earlier ones using a dedicated framework for understanding human-AI collaboration in data storytelling. Through comparison, we identify consistently widely studied patterns, e.g., human-creator + AI-assistant, and newly explored or emerging ones, e.g., AI-creator + human-reviewer. The benefits of these AI techniques and implications to human-AI collaboration are also revealed. We further propose future directions to hopefully ignite innovations.","accessible_pdf":null,"authors":[{"affiliation":"Microsoft Research Asia","email":"haotian.li@microsoft.com","name":"Haotian Li"},{"affiliation":"Microsoft Research","email":"wangyun@microsoft.com","name":"Yun Wang"},{"affiliation":"The Hong Kong University of Science and Technology","email":"huamin@cse.ust.hk","name":"Huamin Qu"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e4f2ed49-3ceb-485d-8d71-8463039a522b","image_caption":"","keywords":["Data storytelling","human-AI collaboration"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1270","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"e4f2ed49-3ceb-485d-8d71","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T10:15:00.000Z","title":"Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"e59ba2c9-15c9-4694-8ff0-fce03bce127e","abstract":"Designing synthetic routes for novel molecules is pivotal in various fields like medicine and chemistry. In this process, researchers need to explore a set of synthetic reactions to transform starting molecules into intermediates step by step until the target novel molecule is obtained. However, designing synthetic routes presents challenges for researchers. First, researchers need to make decisions among numerous possible synthetic reactions at each step, considering various criteria (e.g., yield, experimental duration, and the count of experimental steps) to construct the synthetic route. Second, they must consider the potential impact of one choice at each step on the overall synthetic route. To address these challenges, we proposed SynthLens, a visual analytics system to facilitate the iterative construction of synthetic routes by exploring multiple possibilities for synthetic reactions at each step of construction. Specifically, we have introduced a tree-form visualization in SynthLensto compare and evaluate all the explored routes at various exploration steps, considering both the exploration step and multiple criteria. Our system empowers researchers to consider their construction process comprehensively, guiding them toward promising exploration directions to complete the synthetic route. We validated the usability and effectiveness of SynthLensthrough a quantitative evaluation and expert interviews, highlighting its role in facilitating the design process of synthetic routes. Finally, we discussed the insights of SynthLensto inspire other multi-criteria decision-making scenarios with visual analytics.","accessible_pdf":"No","authors":[{"email":null,"name":"Qipeng Wang"},{"email":null,"name":"Rui Sheng"},{"email":null,"name":"Shaolun Ruan"},{"email":null,"name":"Xiaofu Jin"},{"email":null,"name":"Chuhan Shi"},{"email":null,"name":"Min Zhu"}],"award":"","doi":"10.1109/TVCG.2025.3552134","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"e59ba2c9-15c9-4694-8ff0-fce03bce127e","image_caption":"","keywords":["Decision making","Visual analytics","Chemicals","Drugs","Chemical reactions","Artificial intelligence","Transforms","Planning","Data mining","Costs"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2412.00729","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0663.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full23","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Of Trees and Treemaps","session_uid":"e59ba2c9-15c9-4694-8ff0","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Of Trees and Treemaps"],"time_stamp":"2025-11-05T11:03:00.000Z","title":"SynthLens: Visual Analytics for Facilitating Multi-step Synthetic Route Design","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/yiiG1AZHJ1E"},{"UID":"e5d145d6-1bb2-45a6-9333-b5123f205c08","abstract":"Matrix reordering permutes the rows and columns of a matrix to reveal meaningful visual patterns, such as blocks that represent clusters. A comprehensive collection of matrices, along with a scoring method for measuring the quality of visual patterns in these matrices, contributes to building a benchmark. This benchmark is essential for selecting or designing suitable reordering algorithms for revealing specific patterns. In this paper, we build a matrix-reordering benchmark, ReorderBench, with the goal of evaluating and improving matrix-reordering techniques. This is achieved by generating a large set of representative and diverse matrices and scoring these matrices with a convolution- and entropy-based method. Our benchmark contains 2,835,000 binary matrices and 5,670,000 continuous matrices, each generated to exhibit one of four visual patterns: block, off-diagonal block, star, or band, along with 450 real-world matrices featuring hybrid visual patterns. We demonstrate the usefulness of ReorderBench through three main applications in matrix reordering: 1) evaluating different reordering algorithms, 2) creating a unified scoring model to measure the visual patterns in any matrix, and 3) developing a deep learning model for matrix reordering.","accessible_pdf":"No","authors":[{"email":null,"name":"Jiangning Zhu"},{"email":null,"name":"Zheng Wang"},{"email":null,"name":"Zhiyang Shen"},{"email":null,"name":"Lai Wei"},{"email":null,"name":"Fengyuan Tian"},{"email":null,"name":"Mengchen Liu"},{"email":null,"name":"Shixia Liu"}],"award":"","doi":"10.1109/TVCG.2025.3560345","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"e5d145d6-1bb2-45a6-9333-b5123f205c08","image_caption":"","keywords":["Visualization","Measurement","Benchmark testing","Heuristic algorithms","Approximation algorithms","Stars","Symmetric matrices","Indexes","Deep learning","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2408.12169","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2025.3560345","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full11","session_room":"Hall E2","session_room_id":"e2","session_title":"Embeddings & Metrics","session_uid":"e5d145d6-1bb2-45a6-9333","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Embeddings & Metrics"],"time_stamp":"2025-11-05T14:00:00.000Z","title":"ReorderBench : A Benchmark for Matrix Reordering","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/gvbjZE1CgLQ"},{"UID":"e60994ea-7658-41d3-afd2-a1175f9d87c1","abstract":"We present drillboards, a technique for adaptive visualization dashboards consisting of a hierarchy of coordinated charts that the user can drill down to reach a desired level of detail depending on their expertise, interest, and desired effort. This functionality allows different users to personalize the same dashboard to their specific needs and expertise. The technique is based on a formal vocabulary of chart representations and rules for merging multiple charts of different types and data into single composite representations. The drillboard hierarchy is created by iteratively applying these rules starting from a baseline dashboard, with each consecutive operation yielding a new dashboard with fewer charts and progressively more abstract and simplified views. We also present an authoring tool for building drillboards and show how it can be applied to an agricultural dataset with hundreds of expert users. Our evaluation asked three domain experts to author drillboards for their own datasets, which we then showed to casual end-users with favorable outcomes.","accessible_pdf":"No","authors":[{"email":null,"name":"Sungbok Shin"},{"email":null,"name":"Inyoup Na"},{"email":null,"name":"Niklas Elmqvist"}],"award":"","doi":"10.1109/TVCG.2025.3542606","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"e60994ea-7658-41d3-afd2-a1175f9d87c1","image_caption":"","keywords":["Data visualization","Internet","Navigation","Electronic mail","Aggregates","Vocabulary","Training","Semantics","Merging","Explainable AI"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2410.12744","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0647.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full9","session_room":"Hall M2","session_room_id":"m2","session_title":"Dastardly Dashboards","session_uid":"e60994ea-7658-41d3-afd2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dastardly Dashboards"],"time_stamp":"2025-11-05T09:06:00.000Z","title":"Drillboards: Adaptive Visualization Dashboards for Dynamic Personalization of Visualization Experiences","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/Ga7NSmKd54M"},{"UID":"e61c0f73-21a5-4e16-bdb6-4038e74770ae","abstract":"Chinese calligraphy is a quintessential element of Chinese cultural heritage. Analyzing and comparing calligraphic styles not only enhances the appreciation, learning, and advancement of calligraphy but also provides valuable insights into ancient China. However, such analysis remains challenging due to the limited scalability and possible inconsistencies of qualitative methods, as well as usability and misalignment issues in conventional quantitative approaches. We propose Calli-VA, a visual analytics system, to address these challenges. Calli-VA extracts character images and their corresponding strokes from original works and characterizes each character using systematic criteria. During analysis, the system defines the analysis scope by overview and uncovers relationships between characters. Explanation and recommendation mechanisms are integrated to help users understand patterns and guide further exploration. A documentation feature allows users to record and share their findings. We demonstrate the effectiveness of Calli-VA through three case studies and expert feedback.","accessible_pdf":null,"authors":[{"affiliation":"Beijing Normal University","email":"jinchengli@bnu.edu.cn","name":"Jincheng Li"},{"affiliation":"\u5317\u4eac\u5e08\u8303\u5927\u5b66 Beijing Normal University (\u4e2d\u56fd\u5927\u9646 Mainland, China)","email":"202422081016@mail.bnu.edu.cn","name":"Jinpeng Wu"},{"affiliation":"Peking University","email":"tanshaocong0108@gmail.com","name":"Shaocong Tan"},{"affiliation":"Beijing Normal University","email":"202431081039@mail.bnu.edu.cn","name":"Lin Du"},{"affiliation":"University of Oxford","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"Peking University","email":"chaofanyang@pku.edu.cn","name":"Chaofan Yang"},{"affiliation":"Peking University","email":"2206595287@pku.edu.cn","name":"Jiadi Zhang"},{"affiliation":"Syracuse University","email":"rxu@syr.edu","name":"Rebecca Ruige Xu"},{"affiliation":"Peking University","email":"shirui@pku.edu.cn","name":"Rui Shi"},{"affiliation":"Beijing Normal University","email":"bailu@bnu.edu.cn","name":"Lu Bai"},{"affiliation":"Peking University","email":"xiaoru.yuan@pku.edu.cn","name":"Xiaoru Yuan"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e61c0f73-21a5-4e16-bdb6-4038e74770ae","image_caption":"","keywords":["Chinese calligraphy style analysis","image analysis","digital humanities","visual analytics."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1335","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full3","session_room":"Hall E1","session_room_id":"e1","session_title":"Analysts, Assemble!","session_uid":"e61c0f73-21a5-4e16-bdb6","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analysts, Assemble!"],"time_stamp":"2025-11-06T13:12:00.000Z","title":"Calli-VA: A Visual Analytics System for Analyzing and Comparing Chinese Calligraphic Styles","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/fkfdha6pCeI"},{"UID":"e6aa88dc-1973-4b48-80b2-ac68e82b3d04","abstract":"Collaborative causal loop diagrams (C-CLDs) help decision makers to model complex systems and processes, but existing tools offer little support for documenting the model-building process or capturing the provenance of stakeholder contributions. In this paper, we map the C-CLD design space to derive concrete requirements for documentation and transparency. We introduce Perspectiva, an interactive prototype shaped by those requirements and refined through iterative feedback. Perspectiva enables side-by-side navigation and comparison of CLDs, codifies changes and conflicting relationships, and preserves term provenance and contributor attribution. Its core features include anchored nodes for topological consistency, node interaction, hover-activated provenance pop-ups, and colour-coded encodings. In user studies with domain and visualisation experts, participants reported that Perspectiva improved navigation, comparison, and provenance tracking relative to static diagrams as well as highlighting opportunities for enhancement and future research.","accessible_pdf":null,"authors":[{"affiliation":"Monash University","email":"jessica.bounassar@monash.edu","name":"Jessica Bou Nassar"},{"affiliation":"Monash University","email":"yyio0001@student.monash.edu","name":"Yu Xuan Yio"},{"affiliation":"Monash University","email":"nath0002@student.monash.edu","name":"Nethara Athukorala"},{"affiliation":"Monash University","email":"simrandhawan21s@gmail.com","name":"Simran -"},{"affiliation":"Monash University","email":"songhai.fan@monash.edu","name":"Songhai Fan"},{"affiliation":"Monash University","email":"cynthia.huang@monash.edu","name":"Cynthia Huang"},{"affiliation":"Simon Fraser University","email":"lyn@sfu.ca","name":"Lyn Bartram"},{"affiliation":"Monash University","email":"tgdwyer@gmail.com","name":"Tim Dwyer"},{"affiliation":"Monash University","email":"sarah.goodwin@monash.edu","name":"Sarah Goodwin"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e6aa88dc-1973-4b48-80b2-ac68e82b3d04","image_caption":"","keywords":["Causal loop diagram","Collaborative","Conceptual Modelling","Systems thinking","Documentation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1130","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"e6aa88dc-1973-4b48-80b2","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T10:33:00.000Z","title":"Out of the Loop: Enhancing Documentation and Transparency  in Causal Loop Diagrams to Capture Multiple Perspectives","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"e7cc7c10-4310-4753-b586-467106cdc883","abstract":"Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"amitkumar.das@stonybrook.edu","name":"Amit Kumar Das"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e7cc7c10-4310-4753-b586-467106cdc883","image_caption":"","keywords":["Misinformation","Large Language Models","Data Extraction"],"open_access_supplemental_link":"https://github.com/vhcailab/MisVisFix","open_access_supplemental_question":"We provide comprehensive supplementary materials including all source code, evaluation datasets, prompts, and detailed experimental results to ensure full reproducibility. All materials are publicly accessible through both the submission system and a dedicated GitHub repository (https://github.com/vhcailab/MisVisFix), enabling researchers to replicate our findings and build upon our interactive dashboard system.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.04679","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1999","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full9","session_room":"Hall M2","session_room_id":"m2","session_title":"Dastardly Dashboards","session_uid":"e7cc7c10-4310-4753-b586","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Dastardly Dashboards"],"time_stamp":"2025-11-05T08:42:00.000Z","title":"MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/Ga7NSmKd54M"},{"UID":"e8ed00a7-2d3b-46b8-9edd-6853dd9c2f8c","abstract":"Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.","accessible_pdf":null,"authors":[{"affiliation":"Southern University of Science and Technology","email":"wanghc1999@gmail.com","name":"Huanchen Wang"},{"affiliation":"Southern University of Science and Technology","email":"zhangwc2024@mail.sustech.edu","name":"Wencheng Zhang"},{"affiliation":"Southern University of Science and Technology","email":"wangzq_2021@outlook.com","name":"Zhiqiang Wang"},{"affiliation":"George Mason University","email":"zlu6@gmu.edu","name":"Zhicong Lu"},{"affiliation":"Southern University of Science and Technology","email":"mayx@sustech.edu.cn","name":"Yuxin Ma"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e8ed00a7-2d3b-46b8-9edd-6853dd9c2f8c","image_caption":"","keywords":["Visual analytics","multi-modal model","corruption robustness","image captioning"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1416","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full12","session_room":"Hall E1","session_room_id":"e1","session_title":"Explanation, Exploration, and Model Configuration","session_uid":"e8ed00a7-2d3b-46b8-9edd","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Explanation, Exploration, and Model Configuration"],"time_stamp":"2025-11-06T08:30:00.000Z","title":"VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/h8CsAxh4LWg"},{"UID":"e8f720e9-fa87-43c1-9d57-b8cc150f3cd3","abstract":"Biomedical data harmonization is essential for enabling exploratory analyses and meta-studies, but the process of schema matching\u2014identifying semantic correspondences between elements of disparate datasets (schemas)\u2014remains a labor-intensive and error-prone task. Even state-of-the-art automated methods often yield low accuracy when applied to biomedical schemas due to the large number of attributes and nuanced semantic differences between them. We present BDIViz, a novel visual analytics system designed to streamline the schema matching process for biomedical data. Through formative studies with domain experts, we identified key requirements for an effective solution and developed interactive visualization techniques that address both scalability challenges and semantic ambiguity. BDIViz employs an ensemble approach that combines multiple matching methods with LLM-based validation, summarizes matches through interactive heatmaps, and provides coordinated views that enable users to quickly compare attributes and their values. Our method-agnostic design allows the system to integrate various schema matching algorithms and adapt to application-specific needs. Through two biomedical case studies and a within-subject user study with  domain experts, we demonstrate that BDIViz significantly improves matching accuracy while reducing cognitive load and curation time compared to baseline approaches.","accessible_pdf":"Accessible","authors":[{"affiliation":"New York University","email":"eden.wu@nyu.edu","name":"Eden Wu"},{"affiliation":"New York University","email":"d.turakhia@nyu.edu","name":"Dishita Turakhia"},{"affiliation":"New York University","email":"guandewu@nyu.edu","name":"Guande Wu"},{"affiliation":"New York University","email":"christos.koutras@nyu.edu","name":"Christos Koutras"},{"affiliation":"New York University School of Medicine","email":"sarah.keegan@nyulangone.org","name":"Sarah Keegan"},{"affiliation":"New York University School of Medicine","email":"wenke.liu@nyulangone.org","name":"Wenke Liu"},{"affiliation":"NYU Grossman School of Medicine","email":"beata.szeitz@nyulangone.org","name":"Beata Szeitz"},{"affiliation":"NYU Grossman School of Medicine","email":"david.fenyo@nyulangone.org","name":"David Fenyo"},{"affiliation":"New York University","email":"csilva@nyu.edu","name":"Claudio Silva"},{"affiliation":"New York University","email":"juliana.freire@nyu.edu","name":"Juliana Freire"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"e8f720e9-fa87-43c1-9d57-b8cc150f3cd3","image_caption":"","keywords":["Schema matching","Biomedical data harmonization","Data visualization","User-in-the-loop","LLM-based schema matching"],"open_access_supplemental_link":"https://github.com/VIDA-NYU/bdi-viz","open_access_supplemental_question":"Our work provides a reproducible pipeline, including thoroughly documented source code, public Docker images for streamlined deployment, and a live server that allows users to directly interact with the system. We also share source code, experiment datasets, and other supplemental materials to ensure transparency and enable others to re-analyze or extend our results.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.16117","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1204","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full8","session_room":"Hall M1","session_room_id":"m1","session_title":"Biomedical Visualization","session_uid":"e8f720e9-fa87-43c1-9d57","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Biomedical Visualization"],"time_stamp":"2025-11-06T14:45:00.000Z","title":"BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rgq3gdIQGxk"},{"UID":"eabf678a-dc8f-4794-9da1-3c30f6b26896","abstract":"The past decade has witnessed the superior power of deep neural networks (DNNs) in applications across various domains. However, training a high-quality DNN remains a non-trivial task due to its massive number of parameters. Visualization has shown great potential in addressing this situation, as evidenced by numerous recent visualization works that aid in DNN training and interpretation. These works commonly employ a strategy of logging training-related data and conducting post-hoc analysis. Based on the results of offline analysis, the model can be further trained or fine-tuned. This strategy, however, does not cope with the increasing complexity of DNNs, because (1) the time-series data collected over the training are usually too large to be stored entirely; (2) the huge I/O overhead significantly impacts the training efficiency; (3) post-hoc analysis does not allow rapid human-interventions (e.g., stop training with improper hyper-parameter settings to save computational resources). To address these challenges, we propose an in-situ visualization and analysis framework for the training of DNNs. Specifically, we employ feature extraction algorithms to reduce the size of training-related data in-situ and use the reduced data for real-time visual analytics. The states of model training are disclosed to model designers in real-time, enabling human interventions on demand to steer the training. Through concrete case studies, we demonstrate how our in-situ framework helps deep learning experts optimize DNNs and improve their analysis efficiency.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Guan Li"},{"email":null,"name":"Junpeng Wang"},{"email":null,"name":"Yang Wang"},{"email":null,"name":"Guihua Shan"},{"email":null,"name":"Ying Zhao"}],"award":"","doi":"10.1109/TVCG.2023.3339585","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"eabf678a-dc8f-4794-9da1-3c30f6b26896","image_caption":"","keywords":["Data visualization","Training","Data models","Analytical models","Feature extraction","Artificial neural networks","Computational modeling"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Our work introduces a novel in-situ visualization framework that performs real-time feature extraction and visual analytics during DNN training.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023.3339585]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full31","session_room":"Hall E1","session_room_id":"e1","session_title":"VA for AI","session_uid":"eabf678a-dc8f-4794-9da1","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VA for AI"],"time_stamp":"2025-11-05T09:18:00.000Z","title":"An In-Situ Visual Analytics Framework for Deep Neural Networks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LajPfGQe32k"},{"UID":"eb4ba833-1dae-43a7-bb32-83629a667c08","abstract":"Neural radiance fields (NeRFs) have achieved impressive view synthesis results by learning an implicit volumetric representation from multi-view images. To project the implicit representation into an image, NeRF employs volume rendering that approximates the continuous integrals of rays as an accumulation of the colors and densities of the sampled points. Although this approximation enables efficient rendering, it ignores the direction information in point intervals, resulting in ambiguous features and limited reconstruction quality. In this paper, we propose a learning method that utilizes learnable view-dependent features to improve scene representation and reconstruction. We model the volume rendering integral with a piecewise constant volume density and spherical harmonic-guided view-dependent features, facilitating ambiguity elimination while preserving the rendering efficiency. In addition, we introduce a regularization term that restricts the anisotropic representation effect to be local, with negligible effect on geometry representations, and that encourages recovering the correct geometry. Our method is flexible and can be plugged into NeRF-based frameworks. Extensive experiments show that the proposed representation can boost the rendering quality of various NeRFs and achieve state-of-the-art rendering performance on both synthetic and real-world scenes.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yifan Wang"},{"email":null,"name":"Jun Xu"},{"email":null,"name":"Yuan Zeng"},{"email":null,"name":"Yi Gong"}],"award":"","doi":"10.1109/TVCG.2025.3554692","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"eb4ba833-1dae-43a7-bb32-83629a667c08","image_caption":"","keywords":["Rendering (computer graphics)","Neural radiance field","Anisotropic","Geometry","Image reconstruction","Solid modeling","Harmonic analysis","Anisotropic magnetoresistance","Training","Shape"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2311.18311","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-07-0599]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full13","session_room":"Hall E2","session_room_id":"e2","session_title":"Fields, Fields, Fields","session_uid":"eb4ba833-1dae-43a7-bb32","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Fields, Fields, Fields"],"time_stamp":"2025-11-05T10:39:00.000Z","title":"Improving Neural Volume Rendering via Learning View-Dependent Integral Approximation","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/mHta4w5FJQg"},{"UID":"ebdd5b65-b5cb-4d4b-bd78-92ff8cc2f243","abstract":"Online user studies of visualizations, visual encodings, and interaction techniques are ubiquitous in visualization research. Yet, designing, conducting, and analyzing studies effectively is still a major burden. Although various packages support such user studies, most solutions address only facets of the experiment life cycle, make reproducibility difficult, or do not cater to nuanced study designs or interactions. We introduce reVISit 2, a software framework that supports visualization researchers at all stages of designing and conducting browser-based user studies. ReVISit supports researchers in the design, debug & pilot, data collection, analysis, and dissemination experiment phases by providing both technical affordances (such as replay of participant interactions) and sociotechnical aids (such as a mindfully maintained community of support). It is a proven system that can be (and has been) used in publication-quality studies \u2014 which we demonstrate through a series of experimental replications. We reflect on the design of the system via interviews and an analysis of its technical dimensions. Through this work, we seek to elevate the ease with which studies are conducted, improve the reproducibility of studies within our community, and support the construction of advanced interactive studies.","accessible_pdf":"Accessible","authors":[{"affiliation":"University of Utah","email":"zcutler@sci.utah.edu","name":"Zach Cutler"},{"affiliation":"University of Utah","email":"jwilburn@sci.utah.edu","name":"Jack Wilburn"},{"affiliation":"Worcester Polytechnic Institute","email":"hilsonshrestha@gmail.com","name":"Hilson Shrestha"},{"affiliation":"Worcester Polytechnic Institute","email":"yding5@wpi.edu","name":"Yiren Ding"},{"affiliation":"University of Utah","email":"briancbollen@sci.utah.edu","name":"Brian Bollen"},{"affiliation":"University of Utah","email":"abrar.nadib@gmail.com","name":"Khandaker Abrar Nadib"},{"affiliation":"University of Utah","email":"hetingying.hty@gmail.com","name":"Tingying He"},{"affiliation":"University of Utah","email":"mcnutt.andrew@gmail.com","name":"Andrew McNutt"},{"affiliation":"Worcester Polytechnic Institute","email":"ltharrison@wpi.edu","name":"Lane Harrison"},{"affiliation":"University of Utah","email":"alexander.lex@gmail.com","name":"Alexander Lex"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ebdd5b65-b5cb-4d4b-bd78-92ff8cc2f243","image_caption":"","keywords":["User studies","crowdsourcing","visualization experiments"],"open_access_supplemental_link":"https://osf.io/e8anx/","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.03876","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1777","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full7","session_room":"Hall E","session_room_id":"e1_e2","session_title":"Best Full Papers","session_uid":"ebdd5b65-b5cb-4d4b-bd78","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Best Full Papers"],"time_stamp":"2025-11-04T13:40:00.000Z","title":"ReVISit 2: A Full Experiment Life Cycle User Study Framework","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/3EozS40EBjc"},{"UID":"ed3195e2-8726-4d85-acb7-c5ed2dc361bb","abstract":"Multi-agent workflows have become a powerful approach to solve complicated tasks by decomposing them into multiple sub-tasks and assigning the sub-tasks to specialized agents. However, designing optimal workflows remains challenging due to the expansive design space. Current practices rely heavily on practitioner intuition and expertise, often resulting in design fixation or an unstructured trial-and-error exploration. To address these challenges, this work introduces FlowForge, an interactive visualization tool to facilitate multi-agent workflow creation through i) a structured visual explore of the design space and ii) in-situ guidance based on design patterns. Based on formative studies and literature review, FlowForge organizes the workflow design process into three levels (i.e., task planning, agent assignment, and agent optimization), ranging from abstract to concrete. A structured visual exploration of the design space enable users to transition from high-level concepts to detailed implementations, and to compare alternative solutions across multiple performance metrics. Additionally, drawing from established workflow design patterns, FlowForge provides contextually relevant in-situ suggestions at each level as users navigate the design space. Use cases and expert interviews demonstrate the usability and effectiveness of FlowForge, while also yielding valuable observations into how practitioners explore the design space and leverage guidance in workflow creation.","accessible_pdf":null,"authors":[{"affiliation":"University of Minnesota","email":"panzhan1hao@gmail.com","name":"Pan Hao"},{"affiliation":"University of Minnesota","email":"dongyeop@umn.edu","name":"Dongyeop Kang"},{"affiliation":"University of Minnesota Twin Cities","email":"hinds084@umn.edu","name":"Nicholas Hinds"},{"affiliation":"University of Minnesota","email":"qianwen@umn.edu","name":"Qianwen Wang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ed3195e2-8726-4d85-acb7-c5ed2dc361bb","image_caption":"","keywords":["LLM workflows","Multi-agent Workflows","design space","hierarchical visualization"],"open_access_supplemental_link":"https://github.com/Visual-Intelligence-UMN/FlowForge","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2507.15559","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1729","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full2","session_room":"Hall E2","session_room_id":"e2","session_title":"Algorithms and Workflows","session_uid":"ed3195e2-8726-4d85-acb7","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Algorithms and Workflows"],"time_stamp":"2025-11-06T13:24:00.000Z","title":"FlowForge: Guiding the Creation of Multi-agent Workflows with Interactive Visualizations as a Thinking Scaffold","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/wUoqSN_s3tw"},{"UID":"ed5a7810-a23f-4fb7-a81a-9559869613a1","abstract":"Trajectory data consisting of a low number of smooth parametric curves are standard data sets in visualization. For a visual analysis, not only the behavior of the individual trajectories is of interest but also the relation of the trajectories to each other. Moving objects represented by the trajectories may rotate around each other or around a moving center. We present an approach to compute and visually analyze such rotational behavior in an objective way. We introduce trajectory vorticity (TRV), a measure of rotational behavior of a low number of trajectories. We show that it is objective and that it can be introduced in two independent ways: by approaches for unsteadiness minimization and by considering the relative spin tensor. We compare TRV against single-trajectory methods and apply it to a number of constructed and real trajectory data sets, including drifting buoys in the Atlantic, midge swarm tracking data, pedestrian tracking data, pigeon flocks, and a simulated vortex street.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Anke Friederici"},{"email":null,"name":"Holger Theisel"},{"email":null,"name":"Tobias G\u00fcnther"}],"award":"","doi":"10.1109/TVCG.2024.3421555","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"ed5a7810-a23f-4fb7-a81a-9559869613a1","image_caption":"","keywords":["Trajectory","Tensors","Rotation measurement","Data visualization","Vectors","Three-dimensional displays","Observers"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023-09-0596.R1]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full14","session_room":"Hall M1","session_room_id":"m1","session_title":"Flow and Topology","session_uid":"ed5a7810-a23f-4fb7-a81a","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Flow and Topology"],"time_stamp":"2025-11-05T15:45:00.000Z","title":"Trajectory Vorticity - Computation and Visualization of Rotational Trajectory Behavior in an Objective Way","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ar3ziIYBxhc"},{"UID":"efe038da-ab5b-4531-9235-89a36836d893","abstract":"Psychomare is an artistic research project that explores the visualization of nightmares through a psychoanalytic and XR-based performance methodology. By treating dreams as symbolic data and nightmares as distortions of subjective recognition, the project translates psychological fear into tangible visual forms. Utilizing AI-driven imagery, embodied dance performance, and virtual production technologies, Psychomare creates an immersive dreamscape where the dancer confronts surreal nightmare entities derived from personal and collective dream memories. Drawing on Lacanian theory, the work proposes that nightmares emerge from a misrecognition of the self\u2014a mirrored distortion of unconscious desires and fears projected onto dream imagery. This symbolic misrecognition becomes the conceptual core of the project, guiding its aesthetic and choreographic strategies. Feedback from the dancer, audience, and a psychoanalyst reveals strong emotional resonance and aesthetic depth, suggesting that the visualization of nightmares can foster self-reflection, emotional confrontation, and collective empathy. This project offers a new model for integrating immersive art, psychoanalytic theory, and technological mediation as a path of collective care.","accessible_pdf":null,"authors":[{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"jarryyyhuang@gmail.com","name":"Jiayang Huang"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"anijiati587@connect.hkust-gz.edu.cn","name":"Joshua Nijiati Alimujiang"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"kzhangcma@hkust-gz.edu.cn","name":"Kang Zhang"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"daveyip@hkust-gz.edu.cn","name":"David Yip"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"efe038da-ab5b-4531-9235-89a36836d893","image_caption":"","keywords":["Nightmares","XR Performance","Psychoanalytic Art Practice","Dream Visualization","Collective Emotion."],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1036","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap2","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Papers","session_uid":"efe038da-ab5b-4531-9235","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Papers"],"time_stamp":"2025-11-06T08:30:00.000Z","title":"Psychomare: A Psychoanalytic and XR-Based Artistic Exploration into Nightmare Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VhkjwheU9Bc"},{"UID":"efeed64e-a269-420a-a742-b9816bd710fd","abstract":"Recent years have witnessed growing interest in understanding the sensitivity of machine learning to training data characteristics. While researchers have claimed the benefits of activities such as a human-in-the-loop approach of interactive label correction for improving model performance, there have been limited studies to quantitatively probe the relationship between the cost of label correction and the associated benefit in model performance. We employ a simulation-based approach to explore the efficacy of label correction under diverse task conditions, namely different datasets, noise properties, and machine learning algorithms. We measure the impact of label correction on model performance under the best-case scenario assumption: perfect correction (perfect human and visual systems), serving as an upper-bound estimation of the benefits derived from visual interactive label correction. The simulation results reveal a trade-off between the label correction effort expended and model performance improvement. Notably, task conditions play a crucial role in shaping the trade-off. Based on the simulation results, we develop a set of recommendations to help practitioners determine conditions under which interactive label correction is an effective mechanism for improving model performance.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Yixuan Wang"},{"email":null,"name":"Jieqiong Zhao"},{"email":null,"name":"Jiayi Hong"},{"email":null,"name":"Ronald G. Askin"},{"email":null,"name":"Ross Maciejewski"}],"award":"","doi":"10.1109/TVCG.2024.3468352","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"efeed64e-a269-420a-a742-b9816bd710fd","image_caption":"","keywords":["Costs","Labeling","Machine learning","Data models","Noise","Analytical models","Training"],"open_access_supplemental_link":null,"open_access_supplemental_question":"noteworthy re-analyses or replications, simulation methods.","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"10.1109/TVCG.2024.3468352","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full31","session_room":"Hall E1","session_room_id":"e1","session_title":"VA for AI","session_uid":"efeed64e-a269-420a-a742","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VA for AI"],"time_stamp":"2025-11-05T09:06:00.000Z","title":"A simulation-based approach for quantifying the impact of interactive label correction for machine learning","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/LajPfGQe32k"},{"UID":"f1371364-b3fb-4a6a-9d20-1e7bf6a9b55c","abstract":"One common task in time series analysis is the visual investigation of spectra such as Fourier spectra or wavelet spectra to identify dominating frequencies. In this article, we present the propagation of data uncertainty to the spectra and its visualization. We consider the Fourier and continuous wavelet transformations, which are two common spectral analysis methods. Deriving the propagation for time series that can be modeled as a Gaussian process leads to a combination of weighted non-central chi-squared distributions in the spectrum. Percentile-based visualizations explicitly encode the non-normal uncertainty in the 1D Fourier and 2D wavelet spectrum. We enrich the visualization by including correlations, sensitivity, and signal-to-noise analysis. For visual exploration, we combine the different visualizations into an interactive approach that allows for investigating the uncertain time series in the temporal and spectral domains. Finally, we show the usefulness of our approach by applying it to several real-world data sets and by a qualitative interview study with visualization experts.","accessible_pdf":"No","authors":[{"email":null,"name":"Marina Evers"},{"email":null,"name":"Daniel Weiskopf"}],"award":"","doi":"10.1109/TVCG.2025.3542898","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"f1371364-b3fb-4a6a-9d20-1e7bf6a9b55c","image_caption":"","keywords":["Uncertainty","Data visualization","Time series analysis","Visualization","Continuous wavelet transforms","Correlation","Wavelet analysis","Gaussian processes","Spectral analysis","Wavelet domain"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-08-0779]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full22","session_room":"Hall E2","session_room_id":"e2","session_title":"Multivariate and Time","session_uid":"f1371364-b3fb-4a6a-9d20","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Multivariate and Time"],"time_stamp":"2025-11-05T09:30:00.000Z","title":"Uncertainty-aware Spectral Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-Il9UvlqnNA"},{"UID":"f2b6c739-7eed-4161-a955-230c614bae12","abstract":"Designing and building visual analytics (VA) systems is a complex, iterative process that requires the seamless integration of data processing, analytics capabilities, and visualization techniques. While prior research has extensively examined the social and collaborative aspects of VA system authoring, the practical challenges of developing these systems remain underexplored. As a result, despite the growing number of VA systems, there are only a few structured knowledge bases to guide their design and development. To tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that systematically reviews and categorizes the fundamental building blocks of urban VA systems, a domain particularly rich and representative due to its intricate data and unique problem sets. Applying this methodology to an initial set of 20 systems, we identify and organize their core components into a multi-level structure, forming an initial knowledge base with a structured blueprint for VA system development. To scale this effort, we leverage a large language model to automate the extraction of these components for other 81 papers (completing a corpus of 101 papers), assessing its effectiveness in scaling knowledge base construction. We evaluate our method through interviews with experts and a quantitative analysis of annotation metrics. Our contributions provide a deeper understanding of VA systems' composition and establish a practical foundation to support more structured, reproducible, and efficient system development. VA-Blueprint is available at https://urbantk.org/va-blueprint.","accessible_pdf":null,"authors":[{"affiliation":"University of Illinois Chicago","email":"lferr10@uic.edu","name":"Leonardo Ferreira"},{"affiliation":"University of Illinois Chicago","email":"gmorei3@uic.edu","name":"Gustavo Moreira"},{"affiliation":"University of Illinois Chicago","email":"fabiom@uic.edu","name":"Fabio Miranda"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f2b6c739-7eed-4161-a955-230c614bae12","image_caption":"","keywords":["Visual analytics","large language models","knowledge base","system development","urban visual analytics."],"open_access_supplemental_link":"http://urbantk.org/va-blueprint","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.07497","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"2001","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full4","session_room":"Hall E2","session_room_id":"e2","session_title":"Analytics & Reasoning","session_uid":"f2b6c739-7eed-4161-a955","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Analytics & Reasoning"],"time_stamp":"2025-11-06T15:21:00.000Z","title":"VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/vzBq3yA2yUs"},{"UID":"f4073f80-c0bd-4d83-9c72-3dcfd060ef7f","abstract":"Although data visualization is powerful for revealing patterns and communicating insights, creating effective visualizations requires familiarity with authoring tools and often disrupts the analysis flow. While large language models show promise for automatically converting analysis intent into visualizations, existing methods function as black boxes without transparent reasoning processes, which prevents users from understanding design rationales and refining suboptimal outputs. To bridge this gap, we propose integrating Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization (NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for NL2VIS and develop an automatic pipeline to equip existing datasets with structured reasoning steps. Second, we introduce nvBench-CoT, a specialized dataset capturing detailed step-by-step reasoning from ambiguous natural language descriptions to finalized visualizations, which enables state-of-the-art performance when used for model fine-tuning. Third, we develop DeepVIS, an interactive visual interface that tightly integrates with the CoT reasoning process, allowing users to inspect reasoning steps, identify errors, and make targeted adjustments to improve visualization outcomes. Quantitative benchmark evaluations, two use cases, and a user study collectively demonstrate that our CoT framework effectively enhances NL2VIS quality while providing insightful reasoning steps to users.","accessible_pdf":null,"authors":[{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"zhihaoshuai@hkust-gz.edu.cn","name":"Zhihao Shuai"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"bli303@connect.hkust-gz.edu.cn","name":"Boyan LI"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"syan195@connect.hkust-gz.edu.cn","name":"siyu yan"},{"affiliation":"The Hong Kong University of Science and Technology (Guangzhou)","email":"yuyuluo@hkust-gz.edu.cn","name":"Yuyu Luo"},{"affiliation":"Hong Kong University of Science and Technology (Guangzhou)","email":"weikaiyang@hkust-gz.edu.cn","name":"Weikai Yang"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f4073f80-c0bd-4d83-9c72-3dcfd060ef7f","image_caption":"","keywords":["Data visualization","automatic visualization","large language models"],"open_access_supplemental_link":"https://github.com/Bvivib-shuai/DeepVIS","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/pdf/2508.01700","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1818","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full34","session_room":"Hall M2","session_room_id":"m2","session_title":"Vis & Language","session_uid":"f4073f80-c0bd-4d83-9c72","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Vis & Language"],"time_stamp":"2025-11-06T11:03:00.000Z","title":"DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/JQEgOnSg18o"},{"UID":"f4284776-c09e-4528-8684-cb06c085d1b1","abstract":"Immersive visualization of network data enables users to physically navigate and interact with complex structures, but managing transitions between detailed local (egocentric) views and global (exocentric) overviews remains a major challenge. We present a multifocus probe technique for immersive environments that allows users to instantiate multiple egocentric subgraph views while maintaining persistent links to the global network context. Each probe acts as a portable local focus, enabling fine-grained inspection and editing of distant or occluded regions. Visual and haptic guidance mechanisms ensure context preservation during multi-scale interaction. We demonstrate and discuss the usability of our technique for the editing of network data.","accessible_pdf":null,"authors":[{"affiliation":"Institute for Visual and Analytic Computing","email":"e.zimmermann@uni-rostock.de","name":"Eric Zimmermann"},{"affiliation":"University of Rostock","email":"stefan.bruckner@gmail.com","name":"Stefan Bruckner"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f4284776-c09e-4528-8684-cb06c085d1b1","image_caption":"","keywords":["Virtual Reality","Graph","Focus+Context","Interaction","Editing"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"http://arxiv.org/abs/2507.01140","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1273","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"f4284776-c09e-4528-8684","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T15:30:00.000Z","title":"Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"f53bf658-7b1b-4417-9c0c-0b74e7455702","abstract":"London's famous tube map is an iconic piece of design and perhaps represents the schematic visualization style most well-known to the general public: its octolinearity has become the de facto standar for transit maps around the world. Making a good schematic transit map is challenging and labour-intensive, and has attracted the attention of the optimization community. Much of the literature has focused on mathematically defining an optimal drawing and algorithms to compute one. However, achieving these optimal layouts is computationally challenging, often requiring multiple minutes of runtime. Crucially, what it means for a map to be good is actually highly dependent on factors that evade a general formal definition, like unique landmarks within the network, the context in which a map will be displayed, and the preference of the designer and client. Rather than attempting to make an algorithm that produces a single high-quality and ready-to-use metro map at great cost, we propose it is more fruitful to support rapid layout iteration by a human designer, providing a workflow that enables efficient exploration of a wider range of designs than could be done by hand, and iterating on these designs. To this end we identify steps in the design process of schematic maps that are tedious to do by hand but are algorithmically feasible, and present a framework around a simple linear program that computes network layouts almost instantaneously given a fixed direction for every connection. These connection directions are decided by a designer in a graphical user interface with several interaction methods and a number of quality-of-life features demonstrating the flexibility of the framework; the implementation is available as open source.","accessible_pdf":null,"authors":[{"affiliation":"TU Eindhoven","email":"tvdmaps@gmail.com","name":"Thomas C. van Dijk"},{"affiliation":"TU Eindhoven","email":"s.d.terziadis@tue.nl","name":"Soeren Terziadis"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f53bf658-7b1b-4417-9c0c-0b74e7455702","image_caption":"","keywords":["Geospatial Data","Graph/Network Data","Algorithms","Interaction Design","Software Prototype"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1550","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full20","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Maps & Spatial Vis","session_uid":"f53bf658-7b1b-4417-9c0c","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Maps & Spatial Vis"],"time_stamp":"2025-11-05T09:06:00.000Z","title":"Algorithmically-Assisted Schematic Transit Map Design: A System and Algorithmic Core for Fast Layout Iteration","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/nkeAR0PgfSw"},{"UID":"f663e24c-ffe1-49ff-8c6d-507745448c8a","abstract":"Flow fields are often partitioned into data blocks for massively parallel computation and analysis based on blockwise relationships. However, most of the previous techniques only consider the first-order dependencies among blocks, which is insufficient in describing complex flow patterns. In this work, we present FlowHON, an approach to construct higher-order networks (HONs) from flow fields. FlowHON captures the inherent higher-order dependencies in flow fields as nodes and estimates the transitions among them as edges. We formulate the HON construction as an optimization problem with three linear transformations. The first two layers correspond to the node generation and the third one corresponds to edge estimation. Our formulation allows the node generation and edge estimation to be solved in a unified framework. With FlowHON, the rich set of traditional graph algorithms can be applied without any modification to analyze flow fields, while leveraging the higher-order information to understand the inherent structure and manage flow data for efficiency. We demonstrate the effectiveness of FlowHON using a series of downstream tasks, including estimating the density of particles during tracing, partitioning flow fields for data management, and understanding flow fields using the node-link diagram representation of networks.","accessible_pdf":"Yes","authors":[{"email":null,"name":"Nan Chen"},{"email":null,"name":"Zhihong Li"},{"email":null,"name":"Jun Tao"}],"award":"","doi":"10.1109/TVCG.2025.3550130","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"f663e24c-ffe1-49ff-8c6d-507745448c8a","image_caption":"","keywords":["Data visualization","Visualization","Prefetching","Partitioning algorithms","Layout","Estimation","Vectors","Trajectory","Optimization","Feature extraction"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2312.02243","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2024-09-0811]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full14","session_room":"Hall M1","session_room_id":"m1","session_title":"Flow and Topology","session_uid":"f663e24c-ffe1-49ff-8c6d","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Flow and Topology"],"time_stamp":"2025-11-05T15:33:00.000Z","title":"FlowHON: Representing Flow Fields Using Higher-Order Networks","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ar3ziIYBxhc"},{"UID":"f84ebbf0-98bb-4a65-9640-3161c7449f60","abstract":"We present VolMoVis, a method for dynamic tomographic reconstruction that supports real-time volume generation and volumetric motion visualization from 2D projections. Visualizing the motion of 3D anatomical structures, such as organs and tumors, is critical for computer-aided interventions. However, conventional 4D volumetric reconstruction methods typically produce a limited set of volumes at discrete phases, suffering from low temporal resolution. Moreover, it often requires extensive segmentation of 3D structures or regions for visualizing volumetric data, making it challenging to segment and visualize dynamic volumes in real-time. To address these challenges, VolMoVis framework employs a continuous implicit neural representation that decomposes the dynamic volumetric data into a static reference volume and a continuous deformation field. This decomposition, along with an efficient deformation network, enables our framework to achieve real-time volume generation and volumetric visualization of continuous anatomical motions. We evaluate VolMoVis on both 4D digital phantoms and real patient datasets, demonstrating its effectiveness for accurate anatomical reconstruction and motion tracking. Furthermore, we highlight its capabilities in real-time simultaneous volume generation and tumor segmentation for visualizing dynamic volumes and 4D tumor tracking, showcasing its potential in image-guided radiation therapy.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"dgaofeng@cs.stonybrook.edu","name":"Gaofeng Deng"},{"affiliation":"Stony Brook University","email":"ari@cs.stonybrook.edu","name":"Arie Kaufman"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f84ebbf0-98bb-4a65-9640-3161c7449f60","image_caption":"","keywords":["Neural Representations","4D Reconstruction","Dynamic Volume Visualization","Real-Time Generation","Volume Rendering"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1161","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full32","session_room":"Room 0.94 + 0.95","session_room_id":"0_94_0_95","session_title":"Virtual Session: Volumes & 3D","session_uid":"f84ebbf0-98bb-4a65-9640","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Virtual Session: Volumes & 3D"],"time_stamp":"2025-11-06T10:39:00.000Z","title":"VolMoVis: Real-Time Volume Generation and Motion Visualization with Dynamic Tomographic Reconstruction","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":null},{"UID":"f8717d69-b5a1-497d-9241-6b1931d4c195","abstract":"With the growing availability of urban data and the increasing complexity of societal challenges, visual analytics has become essential for deriving insights into pressing real-world problems. However, analyzing such data is inherently complex and iterative, requiring expertise across multiple domains. The need to manage diverse datasets, distill intricate workflows, and integrate various analytical methods presents a high barrier to entry, especially for researchers and urban experts who lack proficiency in data management, machine learning, and visualization. Advancements in large language models offer a promising solution to lower the barriers to the construction of analytics systems by enabling users to specify intent rather than define precise computational operations. However, this shift from explicit operations to intent-based interaction introduces challenges in ensuring alignment throughout the design and development process. Without proper mechanisms, gaps can emerge between user intent, system behavior, and analytical outcomes. To address these challenges, we propose Urbanite, a framework for human-AI collaboration in urban visual analytics. Urbanite leverages a dataflow-based model that allows users to specify intent at multiple scopes, enabling interactive alignment across the specification, process, and evaluation stages of urban analytics. Based on findings from a survey to uncover challenges, Urbanite incorporates features to facilitate explainability, multi-resolution definition of tasks across dataflows, nodes, and parameters, while supporting the provenance of interactions. We demonstrate Urbanite's effectiveness through usage scenarios created in collaboration with urban experts. Urbanite is available at https://urbantk.org/urbanite.","accessible_pdf":null,"authors":[{"affiliation":"University of Illinois","email":"gmorei3@uic.edu","name":"Gustavo Moreira"},{"affiliation":"University of Illinois Chicago","email":"lferr10@uic.edu","name":"Leonardo Ferreira"},{"affiliation":"University of Illinois","email":"carolvfs@illinois.edu","name":"Carolina Veiga"},{"affiliation":"University of California, Berkeley","email":"maryamh@mit.edu","name":"Maryam Hosseini"},{"affiliation":"University of Illinois Chicago","email":"fabiom@uic.edu","name":"Fabio Miranda"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f8717d69-b5a1-497d-9241-6b1931d4c195","image_caption":"","keywords":["Urban analytics","urban data","dataflow","large language models","visualization framework","visualization system."],"open_access_supplemental_link":"http://urbantk.org/urbanite","open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.07390","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1938","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full29","session_room":"Hall M1","session_room_id":"m1","session_title":"Transportation, Buildings, and Urban Vis","session_uid":"f8717d69-b5a1-497d-9241","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Transportation, Buildings, and Urban Vis"],"time_stamp":"2025-11-06T13:24:00.000Z","title":"Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/hoMn73KnXCg"},{"UID":"f8d762ae-1228-45b3-a314-a7a550e6b725","abstract":"Annotations are central to effective data communication, yet most visualization tools treat them as secondary constructs---manually defined, difficult to reuse, and loosely coupled to the underlying visualization grammar. We propose a declarative extension to Wilkinson's Grammar of Graphics that reifies annotations as first-class design elements, enabling structured specification of annotation targets, types, and positioning strategies. To demonstrate the utility of our approach, we develop a prototype extension called Vega-Lite Annotation. Through comparison with eight existing tools, we show that our approach enhances expressiveness, reduces authoring effort, and enables portable, semantically integrated annotation workflows.","accessible_pdf":null,"authors":[{"affiliation":"University of Utah","email":"dilshadur@sci.utah.edu","name":"Md Dilshadur Rahman"},{"affiliation":"University of Utah","email":"rahatzamancse@gmail.com","name":"Md Rahat-uz- Zaman"},{"affiliation":"University of Utah","email":"mcnutt.andrew@gmail.com","name":"Andrew McNutt"},{"affiliation":"University of Utah","email":"paul.rosen@utah.edu","name":"Paul Rosen"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f8d762ae-1228-45b3-a314-a7a550e6b725","image_caption":"","keywords":["annotation","visualization grammar"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1199","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short3","session_room":"Hall E2","session_room_id":"e2","session_title":"Workflows & Infrastructure","session_uid":"f8d762ae-1228-45b3-a314","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Workflows & Infrastructure"],"time_stamp":"2025-11-06T09:15:00.000Z","title":"AnnoGram: An Annotative Grammar of Graphics Extension","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/-r69u3gOTsk"},{"UID":"f999c7ff-3a3e-4d54-a591-9d720a8d8244","abstract":"This paper introduces a timeline authoring tool that integrates Augmented Reality and tablet-based spatial interaction, enabling users to create large-scale non-linear timelines by leveraging spatial cognition and embodied interaction. In this system, the AR headset provides a large, immersive space for visualizing and interacting with a room-sized 3D timeline, while the tablet allows for precise sketching, annotation, and sculpting of timeline structures such as curves and branches. We conducted a design workshop to explore the user experience with this tool, which supports the creation of diverse expressive timelines for two datasets.","accessible_pdf":null,"authors":[{"affiliation":"University of Alabama in Huntsville","email":"ptv0002@uah.edu","name":"Veronica Vu"},{"affiliation":"UAH","email":"yr0011@uah.edu","name":"Yogesh Rai"},{"affiliation":"University of Alabama in Huntsville","email":"hc0021@uah.edu","name":"Haeyong Chung"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f999c7ff-3a3e-4d54-a591-9d720a8d8244","image_caption":"","keywords":["Timeline","authoring","mobile","immersive analytics."],"open_access_supplemental_link":null,"open_access_supplemental_question":"This work introduces a novel hybrid AR system that supports the authoring of large-scale, expressive timeline visualizations through tablet-based spatial interaction and immersive AR space.","paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1243","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short1","session_room":"Hall M2","session_room_id":"m2","session_title":"Immersive Visualization and Extended Reality","session_uid":"f999c7ff-3a3e-4d54-a591","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Immersive Visualization and Extended Reality"],"time_stamp":"2025-11-06T15:39:00.000Z","title":"Walking Through Time: A Hybrid Immersive System for Spatial and Expressive Timeline Authoring","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/02WpqQypVig"},{"UID":"f9fb5a2a-c9bd-48f5-b5be-8a75e1b77b02","abstract":"Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.","accessible_pdf":"Accessible","authors":[{"affiliation":"Harvard University","email":"catherineyeh@g.harvard.edu","name":"Catherine Yeh"},{"affiliation":"Harvard University","email":"tarakmenon@gmail.com","name":"Tara Menon"},{"affiliation":"Harvard University","email":"aryarobinsingh@gmail.com","name":"Robin Singh Arya"},{"affiliation":"Harvard University","email":"hairong.he03@gmail.com","name":"Helen He"},{"affiliation":"Harvard University","email":"weigel@fas.harvard.edu","name":"Moira Weigel"},{"affiliation":"Harvard University","email":"viegas@google.com","name":"Fernanda Viegas"},{"affiliation":"Harvard","email":"wattenberg@gmail.com","name":"Martin Wattenberg"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"f9fb5a2a-c9bd-48f5-b5be-8a75e1b77b02","image_caption":"","keywords":["Narrative visualization","interactive literary analysis","large language models"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.06772","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1516","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full15","session_room":"Hall E2","session_room_id":"e2","session_title":"From Data to Meaning","session_uid":"f9fb5a2a-c9bd-48f5-b5be","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["From Data to Meaning"],"time_stamp":"2025-11-06T10:27:00.000Z","title":"Story Ribbons: Reimagining Storyline Visualizations with Large Language Models","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/v-iY-PKyXHs"},{"UID":"fb22d5c6-8a2f-4d98-80ec-1fbbfed18538","abstract":"Visualizing and analyzing 3D unsteady flow fields is a very challenging task.\nWe approach this problem by leveraging the mathematical foundations of 3D observer fields to explore and analyze 3D flows in reference frames that are more suitable to visual analysis than the input reference frame.\nWe design novel interactive tools for determining, filtering, and combining reference frames for observer-aware 3D unsteady flow visualization. We represent the space of reference frame motions in a 3D spatial domain via a 6D parameter space, in which every observer is a time-dependent curve. Our framework supports operations in this 6D observer space by separately focusing on two 3D subspaces, for 3D translations, and 3D rotations, respectively. We show that this approach facilitates a variety of interactions with 3D flow fields.\nBuilding on the interactive selection of observers, we furthermore introduce novel techniques such as observer-aware streamline- and pathline-filtering as well as observer-aware isosurface animations of scalar fluid properties for the enhanced visualization and analysis of 3D unsteady flows.\nWe discuss the theoretical underpinnings as well as practical implementation considerations of our approach, and demonstrate the benefits of its 6+1D observer-based methodology on several 3D unsteady flow datasets.","accessible_pdf":"Accessible","authors":[{"affiliation":"King Abdullah University of Science and Technology (KAUST)","email":"xingdi.zhang@kaust.edu.sa","name":"Xingdi Zhang"},{"affiliation":"King Abdullah University of Science and Technology","email":"amani.ageeli@kaust.edu.sa","name":"Amani Ageeli"},{"affiliation":"Thomas Theussl","email":"tom.theussl@gmail.com","name":"Thomas Theu\u00dfl"},{"affiliation":"KAUST","email":"markus.hadwiger@kaust.edu.sa","name":"Markus Hadwiger"},{"affiliation":"King Abdullah University of Science and Technology","email":"peter.rautek@gmail.com","name":"Peter Rautek"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"fb22d5c6-8a2f-4d98-80ec-1fbbfed18538","image_caption":"","keywords":["Flow visualization","unsteady flow","reference frame optimization","interactive visualization","coherent structures"],"open_access_supplemental_link":"https://github.com/Cindy-xdZhang/PyflowVis","open_access_supplemental_question":"This paper is accompanied by comprehensive supplemental materials and a well-documented open-source implementation.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://vccvisualization.org/research/observerspaces/","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1817","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full14","session_room":"Hall M1","session_room_id":"m1","session_title":"Flow and Topology","session_uid":"fb22d5c6-8a2f-4d98-80ec","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Flow and Topology"],"time_stamp":"2025-11-05T15:09:00.000Z","title":"Exploring 3D Unsteady Flow using 6D Observer Space Interactions","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/ar3ziIYBxhc"},{"UID":"fb8fb932-9800-4cfe-9ad5-4d7504df2c00","abstract":"We explore the effects of data and design considerations through the example case of part-to-whole data relationships. Standard part-to-whole representations like pie charts and stacked bar charts make the relationships of parts to the whole explicit. Value estimation in these charts benefits from two perceptual mechanisms: anchoring, where the value is close to a reference value with an easily recognized shape, and alignment where the beginning or end of the shape is aligned with a marker. In an online study, we explore how data and design factors such as value, position, and encoding together impact these effects in making estimations in part-to-whole charts. The results show how salient values and alignment to positions on a scale affect task performance. This demonstrates the need for informed visualization design based around how data properties and design factors affect perceptual mechanisms.","accessible_pdf":null,"authors":[{"affiliation":"University of Wisconsin - Madison","email":"cbailey9@wisc.edu","name":"Connor Bailey"},{"affiliation":"University of Wisconsin - Madison","email":"gleicher@cs.wisc.edu","name":"Michael Gleicher"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"fb8fb932-9800-4cfe-9ad5-4d7504df2c00","image_caption":"","keywords":["part-to-whole","estimation","graphical perception","anchoring","alignment","rounding","perceptual mechanisms."],"open_access_supplemental_link":"https://osf.io/e36au, https://github.com/uwgraphics/PartToWhole","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1307","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"fb8fb932-9800-4cfe-9ad5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T11:18:00.000Z","title":"Anchoring and Alignment: Data Factors in Part-to-Whole Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"},{"UID":"fbac208d-6896-420b-8276-ab913bb0e617","abstract":"This artwork presents an interdisciplinary interaction installation that visualizes collective online mourning behavior in China. By focusing on commemorative content posted on Sina Weibo following the deaths of seven prominent Chinese authors, the artwork employs data scraping, natural language processing, and 3D modeling to transform fragmented textual expressions into immersive digital monuments. Through the analysis of word frequencies, topic models, and user engagement metrics, the system constructs a semantic-visual landscape that reflects both authorial legacies and collective memory. This research contributes to the fields of digital humanities, visualization design, and digital memorial architecture by proposing a novel approach for preserving and reactivating collective memory in the digital age.","accessible_pdf":null,"authors":[{"affiliation":"Harbin Institute of Technology","email":"lingyupeng6@163.com","name":"Lingyu Peng"},{"affiliation":"Harbin institute of technology (Shenzhen)","email":"2634641504@qq.com","name":"Chang Ge"},{"affiliation":"Harbin Institute of Technology, Shenzhen","email":"582546102@qq.com","name":"Liying Long"},{"affiliation":"Future Design School","email":"li1179327296@163.com","name":"xin Li"},{"affiliation":"Harbin Institute of Technology","email":"1140084087@qq.com","name":"Xiao Hu"},{"affiliation":"Harbin Institute of Technology","email":"867103556@qq.com","name":"Pengda Lu"},{"affiliation":"Harbin Institute of Technology","email":"liqingchuan@hit.edu.cn","name":"Qingchuan Li"},{"affiliation":"Harbin Institute of Technology","email":"wu.jiangyue@outlook.com","name":"Jiangyue Wu"}],"award":"","doi":null,"event_id":"a-visap","event_title":"VIS Arts Program","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"fbac208d-6896-420b-8276-ab913bb0e617","image_caption":"","keywords":["Online mourning","Collective memory","Artistic Data Visualization","Text visualization","Interaction installation"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"associated","paper_type_color":"#2672B9","paper_type_name":"Associated Event","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1049","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"visap2","session_room":"Room 1.14","session_room_id":"1_14","session_title":"VISAP Papers","session_uid":"fbac208d-6896-420b-8276","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["VISAP Papers"],"time_stamp":"2025-11-06T08:54:00.000Z","title":"Tides of Memory: Digital Echoes of Netizen Remembrance","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/VhkjwheU9Bc"},{"UID":"fcafb37e-70f2-4d68-b9a4-e92902d9d2bd","abstract":"This paper evaluates the visualization literacy of modern Large Language Models (LLMs) and introduces a novel prompting technique called Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet, GPT-4.5-preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment Test (VLAT) using standard prompts and our structured approach. The Charts-of-Thought method guides LLMs through a systematic data extraction, verification, and analysis process before answering visualization questions. Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method, far exceeding the human baseline of 28.82. This approach improved performance across all models, with score increases of 21.8% for GPT-4.5, 9.4% for Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The performance gains were consistent across original and modified VLAT charts, with Claude correctly answering 100% of questions for several chart types that previously challenged LLMs. Our study reveals that modern multimodal LLMs can surpass human performance on visualization literacy tasks when given the proper analytical framework. These findings establish a new benchmark for LLM visualization literacy and demonstrate the importance of structured prompting strategies for complex visual interpretation tasks. Beyond improving LLM visualization literacy, Charts-of-Thought could also enhance the accessibility of visualizations, potentially benefiting individuals with visual impairments or lower visualization literacy.","accessible_pdf":null,"authors":[{"affiliation":"Stony Brook University","email":"amitkumar.das@stonybrook.edu","name":"Amit Kumar Das"},{"affiliation":"East West University","email":"md.tarun005@gmail.com","name":"Mohammad Tarun"},{"affiliation":"Stony Brook University","email":"mueller@cs.sunysb.edu","name":"Klaus Mueller"}],"award":"","doi":null,"event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"fcafb37e-70f2-4d68-b9a4-e92902d9d2bd","image_caption":"","keywords":["Visualization Literacy","Large Language Models","Charts-of-Thoughts","Data Extraction"],"open_access_supplemental_link":"https://github.com/vhcailab/Charts-of-Thought","open_access_supplemental_question":"We provide comprehensive supplementary materials including all source code, experimental data, prompts, and detailed results to ensure full reproducibility. All materials are publicly accessible through both the submission system and a dedicated GitHub repository (https://github.com/vhcailab/Charts-of-Thought), enabling researchers to replicate our Charts-of-Thought prompting methodology and build upon our structured approach to enhancing LLM visualization literacy.","paper_type":"full","paper_type_color":"#1C3160","paper_type_name":"VIS Full Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2508.04842","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1997","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full35","session_room":"Room 0.11 + 0.12","session_room_id":"0_11_0_12","session_title":"Visualization Literacy","session_uid":"fcafb37e-70f2-4d68-b9a4","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization Literacy"],"time_stamp":"2025-11-07T09:06:00.000Z","title":"Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/R3RWFseiZmo"},{"UID":"fe3d1eaa-d502-47c2-9939-61ad180dc260","abstract":"Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a novel simplified vector format that encodes chart elements such as mark type, position, and size. The effectiveness of SimVec is demonstrated by using MLLMs to reconstruct chart information from SimVec formats. Then, we build a new visualization dataset, SimVecVis, to enhance the performance of MLLMs in visualization understanding, which consists of three key dimensions: bitmap images of charts, their SimVec representations, and corresponding data-centric question-answering (QA) pairs with explanatory chain-of-thought (CoT) descriptions. We finetune state-of-the-art MLLMs (e.g., MiniCPM and Qwen-VL), using SimVecVis with different dataset dimensions. The experimental results show that it leads to substantial performance improvements of MLLMs with good spatial perception capabilities (e.g., MiniCPM) in data-centric QA tasks. Our dataset and source code are available at: https://github.com/VIDA-Lab/SimVecVis.","accessible_pdf":null,"authors":[{"affiliation":"Nanyang Technological University","email":"can.liu.1996@gmail.com","name":"Can Liu"},{"affiliation":"UNIVERSITY OF ST ANDREWS","email":"dachunlin00@gmail.com","name":"Chunlin Da"},{"affiliation":"Nanjing University","email":"xxlong@nju.edu.cn","name":"Xiaoxiao Long"},{"affiliation":"Tsinghua University","email":"yuxiao@fondant.design","name":"Yuxiao Yang"},{"affiliation":"Huawei Technologies Co., Ltd","email":"yuzhang94@outlook.com","name":"Yu Zhang"},{"affiliation":"Nanyang Technological University","email":"yong-wang@ntu.edu.sg","name":"Yong WANG"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"fe3d1eaa-d502-47c2-9939-61ad180dc260","image_caption":"","keywords":["Visualization LLM","Multimodal LLM","Chart QA"],"open_access_supplemental_link":"https://github.com/VIDA-Lab/SimVecVis","open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2506.21319","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1151","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short8","session_room":"Hall E1","session_room_id":"e1","session_title":"Visualization with/for/in AI","session_uid":"fe3d1eaa-d502-47c2-9939","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Visualization with/for/in AI"],"time_stamp":"2025-11-05T10:24:00.000Z","title":"SimVecVis: A Dataset for Enhancing MLLMs in Visualization Understanding","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/rPjFS7xuL5w"},{"UID":"fea404a7-6131-46b3-afa5-258461f48c21","abstract":"Grounded theory (GT) is a research methodology that entails a systematic workflow for theory generation grounded on emergent data. In this article, we juxtapose GT workflows with typical workflows in visualization and visual analytics (VIS), unveiling the characteristics shared by these workflows. We explore the research landscape of VIS to study where GT is applied to generate VIS theories, explicitly as well as implicitly. We discuss \u201cwhy\u201d GT can potentially play a significant role in VIS. We outline a \u201chow\u201d methodology for conducting GT research in VIS, which addresses the need for theoretical advancement in VIS while benefiting from other methods and techniques in VIS. We illustrate this \u201chow\u201d methodology with a use case of adopting GT approaches in studying visualization guidelines.","accessible_pdf":"No","authors":[{"email":null,"name":"Alexandra Diehl"},{"email":null,"name":"Alfie Abdul-Rahman"},{"email":null,"name":"Benjamin Bach"},{"email":null,"name":"Mennatallah El-Assady"},{"email":null,"name":"Matthias Kraus"},{"email":null,"name":"Robert Laramee"},{"email":null,"name":"Daniel A. Keim"},{"email":null,"name":"Min Chen"}],"award":"","doi":"10.1109/TVCG.2024.3452985","event_id":"v-full","event_title":"VIS Full Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":false,"id":"fea404a7-6131-46b3-afa5-258461f48c21","image_caption":"","keywords":["Visual analytics","Guidelines","Data visualization","Encoding","Social sciences","Cognition","Pipelines"],"open_access_supplemental_link":null,"open_access_supplemental_question":"Novel methodology","paper_type":"invited","paper_type_color":"#1C3160","paper_type_name":"None","pdf_url":null,"preprint_link":"https://arxiv.org/abs/2203.01777","prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"[TVCG 2023-08-0480]","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"full21","session_room":"Room 1.14","session_room_id":"1_14","session_title":"Models, Methods, and Typologies","session_uid":"fea404a7-6131-46b3-afa5","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Models, Methods, and Typologies"],"time_stamp":"2025-11-07T09:18:00.000Z","title":"An Analysis of the Interplay and Mutual Benefits of Grounded Theory and Visualization","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/moykjX8QeZI"},{"UID":"ff31e8af-e918-47c4-a7f9-123a2ac6e98c","abstract":"This work introduces AGri (Adaptive Thumbnails for Grid-based Visualizations), a method for dynamically adjusting thumbnails of spatiotemporal data\u2014such as videos\u2014to varying screen footprints in grid-based layouts. AGri aims to maximize thumbnail expressiveness, which quantifies how well similarity relationships among data members (e.g., video frames) are preserved. Thumbnails are generated via cropping, with crop windows optimized based on cumulative salience images. By modeling the trade-off between expressiveness and footprint size, AGri defines a curve\u2014the AGri curve\u2014representing Pareto-optimal visual representations. This curve enables dynamic selection of thumbnails suited to different grid sizes and resolutions. The approach is demonstrated on two datasets: a spatiotemporal ensemble from scientific experiments and an animated short film.","accessible_pdf":null,"authors":[{"affiliation":"University of Groningen","email":"s.d.frey@rug.nl","name":"Steffen Frey"}],"award":"","doi":null,"event_id":"v-short","event_title":"VIS Short Papers","external_paper_link":"","fno":null,"has_fno":false,"has_image":false,"has_pdf":true,"id":"ff31e8af-e918-47c4-a7f9-123a2ac6e98c","image_caption":"","keywords":["Grid-based Visualization","Thumbnails","Spatiotemporal Data","Video Visualization"],"open_access_supplemental_link":null,"open_access_supplemental_question":null,"paper_type":"short","paper_type_color":"#FDBB30","paper_type_name":"VIS Short Paper","pdf_url":null,"preprint_link":null,"prerecorded_video_id":null,"prerecorded_video_link":null,"program_paper_id":"1357","session_bunny_ff_link":null,"session_bunny_ff_subtitles":null,"session_bunny_prerecorded_link":null,"session_bunny_prerecorded_subtitles":null,"session_id":"short5","session_room":"Hall M2","session_room_id":"m2","session_title":"Charts, Diagrams & Plots","session_uid":"ff31e8af-e918-47c4-a7f9","session_youtube_ff_id":null,"session_youtube_ff_link":null,"session_youtube_prerecorded_id":null,"session_youtube_prerecorded_link":null,"sessions":["Charts, Diagrams & Plots"],"time_stamp":"2025-11-05T10:51:00.000Z","title":"AGri: Adaptive Thumbnails For Grid-based Visualizations","youtube_ff_id":null,"youtube_ff_url":null,"youtube_url":"https://youtu.be/FOAj5-NjQho"}]
