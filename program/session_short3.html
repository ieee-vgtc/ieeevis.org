<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Short Papers: Workflows & Infrastructure"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Short Papers: Workflows & Infrastructure"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Short Papers: Workflows &amp; Infrastructure</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Workflows &amp; Infrastructure</li></ol></nav><h1 class="session-title">VIS Short Papers: Workflows &amp; Infrastructure</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-short.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-short.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Jaemin Jo </h3><h3 class="session-room mt-4"> Room: Hall E2 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T08:30:00+00:00 &ndash; 2025-11-06T09:45:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T08:30:00+00:00 &ndash; 2025-11-06T09:45:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/short3.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945279275831377" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1068&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;An Evaluation of Temporal and Categorical Uncertainty on Timelines: A Case Study in Human Activity Recall Visualizations&#39;, &#39;contributors&#39;: [&#39;Veronika Potter&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T08:39:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Veronika Potter&#39;, &#39;email&#39;: &#39;potter.v@northeastern.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}, {&#39;name&#39;: &#39;Ha Le&#39;, &#39;email&#39;: &#39;le.ha1@northeastern.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}, {&#39;name&#39;: &#39;Uzma Haque Syeda&#39;, &#39;email&#39;: &#39;syeda.u@northeastern.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}, {&#39;name&#39;: &#39;Stephen Intille&#39;, &#39;email&#39;: &#39;s.intille@northeastern.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}, {&#39;name&#39;: &#39;Michelle Borkin&#39;, &#39;email&#39;: &#39;m.borkin@neu.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}], &#39;abstract&#39;: &#34;Encoding uncertainty in timelines can provide more precise and informative visualizations (e.g., visual representations of unsure times or locations in event planning timelines). To evaluate the effectiveness of different temporal and categorical uncertainty representations on timelines, we conducted a mixed-methods user study with 81 participants on uncertainty in activity recall timelines (ARTs). We find that participants&#39; accuracy is better when temporal uncertainty is encoded using transparency instead of dashing, and that a participant&#39;s visual encoding preference does not always align with their performance (e.g., they performed better with a less-preferred visual encoding technique). Additionally, qualitative findings show that existing biases of an individual alter their interpretation of ARTs. A copy of our study materials is available at https://osf.io/98p6m/.&#34;, &#39;uid&#39;: &#39;135b9aea-b3cc-4bf7-880f-117393bf1f70&#39;, &#39;keywords&#39;: [&#39;Timelines&#39;, &#39;Uncertainty Visualization&#39;, &#39;Evaluation Study.&#39;], &#39;preprint_link&#39;: &#39;https://osf.io/98p6m/&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/98p6m/&#39;} <h3 class="session-list-title"><a href="paper_135b9aea-b3cc-4bf7-880f-117393bf1f70.html"> An Evaluation of Temporal and Categorical Uncertainty on Timelines: A Case Study in Human Activity Recall Visualizations <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Veronika Potter, Ha Le, Uzma Haque Syeda, Stephen Intille, Michelle Borkin </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Veronika Potter </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:30:00.000Z &ndash; 2025-11-06T08:39:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1016&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues&#39;, &#39;contributors&#39;: [&#39;Oliver Huang&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:39:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:39:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T08:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Oliver Huang&#39;, &#39;email&#39;: &#39;oliver@dgp.toronto.edu&#39;, &#39;affiliation&#39;: &#39;University of Toronto&#39;}, {&#39;name&#39;: &#39;Carolina Nobre&#39;, &#39;email&#39;: &#39;cnobre@cs.toronto.edu&#39;, &#39;affiliation&#39;: &#39;University of Toronto&#39;}], &#39;abstract&#39;: &#39;Data visualization tasks often require multi-step reasoning, and the interpretive strategies experts use-such as decomposing complex goals into smaller subtasks and selectively attending to key chart regions-are rarely made explicit. ViStruct is an automated pipeline that simulates these expert behaviours by breaking high-level visual questions into structured analytic steps and highlighting semantically relevant chart areas. Leveraging large language and vision-language models, ViStruct identifies chart components, maps subtasks to spatial regions, and presents visual attention cues to externalize expert-like reasoning flows. While not designed for direct novice instruction, ViStruct provides a replicable model of expert interpretation that can inform the development of future visual literacy tools. We evaluate the system on 45 tasks across 12 chart types and validate its outputs with trained visualization users, confirming its ability to produce interpretable and expert-aligned reasoning sequences.&#39;, &#39;uid&#39;: &#39;abf4110d-55e9-4824-875f-5b8a60547704&#39;, &#39;keywords&#39;: [&#39;Data Visualization&#39;, &#39;Task Decomposition&#39;, &#39;Large Language Models(LLMs)&#39;, &#39;Guidance System&#39;, &#39;Computer Vision&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2506.21762&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;Our system and code are publicly available as an open-source interactive platform at https://vi-struct.vercel.app, including annotated examples, chart inputs, and model outputs. We have also open-sourced the expert review evaluation data used in our study to support transparency and facilitate future comparative work.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/hivelabuoft/ViStruct&#39;} <h3 class="session-list-title"><a href="paper_abf4110d-55e9-4824-875f-5b8a60547704.html"> ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Oliver Huang, Carolina Nobre </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Oliver Huang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:39:00.000Z &ndash; 2025-11-06T08:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1052&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;How Do Observable Users Decompose D3 Code? A Qualitative Study&#39;, &#39;contributors&#39;: [&#39;Melissa Lin&#39;, &#39;Heer Patel&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T08:57:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Melissa Lin&#39;, &#39;email&#39;: &#39;lin.family.folders@gmail.com&#39;, &#39;affiliation&#39;: &#39;Carnegie Mellon University&#39;}, {&#39;name&#39;: &#39;Heer Patel&#39;, &#39;email&#39;: &#39;heerpate@cs.washington.edu&#39;, &#39;affiliation&#39;: &#39;University of Washington&#39;}, {&#39;name&#39;: &#39;Medina Lamkin&#39;, &#39;email&#39;: &#39;mlamkin@cs.washington.edu&#39;, &#39;affiliation&#39;: &#39;University of Washington&#39;}, {&#39;name&#39;: &#39;Hannah Bako&#39;, &#39;email&#39;: &#39;hbako@virginia.edu&#39;, &#39;affiliation&#39;: &#39;University of Maryland&#39;}, {&#39;name&#39;: &#39;Leilani Battle&#39;, &#39;email&#39;: &#39;leibatt@cs.washington.edu&#39;, &#39;affiliation&#39;: &#39;University of Washington&#39;}], &#39;abstract&#39;: &#39;Many toolkit developers seek to streamline the visualization programming process through structured support such as prescribed templates and example galleries. However, few projects examine how users organize their own visualization programs and how their coding choices may deviate from the intents of toolkit developers, impacting visualization prototyping and design. Further, is it possible to infer users’ reasoning indirectly through their code, even when users copy code from other sources? We explore this question through a qualitative analysis of 715 D3 programs on Observable. We identify three levels of program organization based on how users decompose their code into smaller blocks: Program-, Chart-, and Component-Level code decomposition, with a strong preference for Component-Level reasoning. In a series of interviews, we corroborate that these levels reflect how Observable users reason about visualization programs. We compare common user-made components with those theorized in the Grammar of Graphics to assess overlap in user and toolkit developer reasoning. We find that, while the Grammar of Graphics covers basic visualizations well, it falls short in describing complex visualization types, especially those with animation, interaction, and parameterization components. Our findings highlight how user practices differ from formal grammars and reinforce ongoing efforts to rethink visualization toolkit support, including augmenting learning tools and AI assistants to better reflect real-world coding strategies.&#39;, &#39;uid&#39;: &#39;a4b21717-3ef3-4fdf-b25a-30b41b9f0ef9&#39;, &#39;keywords&#39;: [&#39;Visualization toolkits&#39;, &#39;Code reuse.&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2405.14341&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;We provided substantial supplemental of our transparent analysis practices and data. This allows for future reproducibility of this work.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/sudb8/?view_only=cc72bdc685804e478852a96297328eb8&#39;} <h3 class="session-list-title"><a href="paper_a4b21717-3ef3-4fdf-b25a-30b41b9f0ef9.html"> How Do Observable Users Decompose D3 Code? A Qualitative Study <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Melissa Lin, Heer Patel, Medina Lamkin, Hannah Bako, Leilani Battle </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Melissa Lin, Heer Patel </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:48:00.000Z &ndash; 2025-11-06T08:57:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1295&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;Publish-Time Optimizations for Web-Based Visualizations&#39;, &#39;contributors&#39;: [&#39;Jeffrey Heer&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T08:57:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T08:57:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:06:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Ron Pechuk&#39;, &#39;email&#39;: &#39;rpechuk@uw.edu&#39;, &#39;affiliation&#39;: &#39;University of Washington&#39;}, {&#39;name&#39;: &#39;Jeffrey Heer&#39;, &#39;email&#39;: &#39;jheer@uw.edu&#39;, &#39;affiliation&#39;: &#39;University of Washington&#39;}], &#39;abstract&#39;: &#39;Fast loading and responsive interaction lead to more effective web-based visualizations. While run-time optimizations such as caching and data tiling improve interaction latency, these approaches leave initial load performance unoptimized. In this work, we investigate _publish-time optimizations_ that shift computational work ahead of user sessions to accelerate both loading and interaction. We organize the space of publish-time optimizations into categories of data preparation, pre-computation of data assets for optimization, and pre-rendering; and then reason about tradeoffs in terms of time-to-render (TTR), time-to-activation (TTA), and storage cost (SC). To assess their effectiveness, we implement publish-time optimizations for the open-source Mosaic architecture and evaluate their impact across varied visualizations and dataset sizes. On average, publish-time strategies reduced rendering latency by 83.7% and activation latency by 33.3%, demonstrating their value for improving the performance of web-based visualizations.&#39;, &#39;uid&#39;: &#39;32ebace7-cf30-462d-b6a4-40a3e39c6226&#39;, &#39;keywords&#39;: [&#39;scalable visualization&#39;, &#39;web visualization&#39;, &#39;visualization optimization&#39;, &#39;visualization systems&#39;, &#39;user interfaces&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/uwdata/mosaic-publish&#39;} <h3 class="session-list-title"><a href="paper_32ebace7-cf30-462d-b6a4-40a3e39c6226.html"> Publish-Time Optimizations for Web-Based Visualizations <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Ron Pechuk, Jeffrey Heer </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jeffrey Heer </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T08:57:00.000Z &ndash; 2025-11-06T09:06:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1218&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;Capturing Visualization Design Rationale&#39;, &#39;contributors&#39;: [&#39;Maeve Hutchinson&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:06:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:06:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:15:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Maeve Hutchinson&#39;, &#39;email&#39;: &#39;maeve.hutchinson@city.ac.uk&#39;, &#39;affiliation&#39;: &#39;City, University of London&#39;}, {&#39;name&#39;: &#39;Radu Jianu&#39;, &#39;email&#39;: &#39;radu.jianu@city.ac.uk&#39;, &#39;affiliation&#39;: &#39;City, University of London&#39;}, {&#39;name&#39;: &#39;Aidan Slingsby&#39;, &#39;email&#39;: &#39;a.slingsby@city.ac.uk&#39;, &#39;affiliation&#39;: &#39;City, University of London&#39;}, {&#39;name&#39;: &#39;Jo Wood&#39;, &#39;email&#39;: &#39;j.d.wood@city.ac.uk&#39;, &#39;affiliation&#39;: &#39;City, University of London&#39;}, {&#39;name&#39;: &#39;Pranava Madhyastha&#39;, &#39;email&#39;: &#39;pranava.madhyastha@city.ac.uk&#39;, &#39;affiliation&#39;: &#39;City, University of London&#39;}], &#39;abstract&#39;: &#39;Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.&#39;, &#39;uid&#39;: &#39;6d474cda-8152-40b9-ba2f-7a28f9bbe0bd&#39;, &#39;keywords&#39;: [&#39;Design&#39;, &#39;Literate Visualization&#39;, &#39;Natural Language&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2506.16571&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/maevehutch/DesignQAR; https://maevehutch.github.io/DesignQAR/&#39;} <h3 class="session-list-title"><a href="paper_6d474cda-8152-40b9-ba2f-7a28f9bbe0bd.html"> Capturing Visualization Design Rationale <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Maeve Hutchinson </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:06:00.000Z &ndash; 2025-11-06T09:15:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1199&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;AnnoGram: An Annotative Grammar of Graphics Extension&#39;, &#39;contributors&#39;: [&#39;Md Rahat-uz- Zaman&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:15:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:15:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Md Dilshadur Rahman&#39;, &#39;email&#39;: &#39;dilshadur@sci.utah.edu&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Md Rahat-uz- Zaman&#39;, &#39;email&#39;: &#39;rahatzamancse@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Andrew McNutt&#39;, &#39;email&#39;: &#39;mcnutt.andrew@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Paul Rosen&#39;, &#39;email&#39;: &#39;paul.rosen@utah.edu&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}], &#39;abstract&#39;: &#34;Annotations are central to effective data communication, yet most visualization tools treat them as secondary constructs---manually defined, difficult to reuse, and loosely coupled to the underlying visualization grammar. We propose a declarative extension to Wilkinson&#39;s Grammar of Graphics that reifies annotations as first-class design elements, enabling structured specification of annotation targets, types, and positioning strategies. To demonstrate the utility of our approach, we develop a prototype extension called Vega-Lite Annotation. Through comparison with eight existing tools, we show that our approach enhances expressiveness, reduces authoring effort, and enables portable, semantically integrated annotation workflows.&#34;, &#39;uid&#39;: &#39;f8d762ae-1228-45b3-a314-a7a550e6b725&#39;, &#39;keywords&#39;: [&#39;annotation&#39;, &#39;visualization grammar&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_f8d762ae-1228-45b3-a314-a7a550e6b725.html"> AnnoGram: An Annotative Grammar of Graphics Extension <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Md Dilshadur Rahman, Md Rahat-uz- Zaman, Andrew McNutt, Paul Rosen </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Md Rahat-uz- Zaman </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:15:00.000Z &ndash; 2025-11-06T09:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1333&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;AnnoLens: Exploration and Annotation through Lens-Based Guidance&#39;, &#39;contributors&#39;: [&#39;Franziska Becker&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:33:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Franziska Becker&#39;, &#39;email&#39;: &#39;franziska.becker@vis.uni-stuttgart.de&#39;, &#39;affiliation&#39;: &#39;University of Stuttgart&#39;}, {&#39;name&#39;: &#39;Steffen Koch&#39;, &#39;email&#39;: &#39;steffen.koch@vis.uni-stuttgart.de&#39;, &#39;affiliation&#39;: &#39;University of Stuttgart&#39;}, {&#39;name&#39;: &#39;Tanja Blascheck&#39;, &#39;email&#39;: &#39;research@blascheck.eu&#39;, &#39;affiliation&#39;: &#39;University of Stuttgart&#39;}], &#39;abstract&#39;: &#39;Annotation is often a time-consuming but fruitful activity in data analysis contexts. The manual labor required to create useful annotations is a barrier that keeps users from documenting their analysis, especially intermediate results. To address the needs of exploration and annotation alike, we propose integrating annotation with lens-based interactions, combining both with guidance. We investigate the exploration-annotation requirement space, identifying challenges and extracting five design requirements for annotation in exploration contexts. Based on this investigation, we designed ANNOLENS—a concrete instantiation of such a system that lets users explore and annotate dimensionality-reduced multivariate data. It employs a dual-lens approach for contrastive exploration, using guidance to steer users toward interesting data subsets and attributes. Annotation is directly integrated into the lenses, letting users quickly annotate hunches and discoveries. Automated merging and linking serve to simplify annotation management and reduce disruptions. In a pilot study, we conducted a preliminary evaluation of our approach, which indicated that users find it easy to annotate data and were able to incorporate their knowledge and unique perspective into the process. A free copy of this paper and all supplemental materials are available at https://osf.io/zpu6c/.&#39;, &#39;uid&#39;: &#39;2b485865-8584-417a-bd55-5a961dcff5ef&#39;, &#39;keywords&#39;: [&#39;Annotation&#39;, &#39;exploration&#39;, &#39;guidance&#39;, &#39;visual analytics.&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/zpu6c/&#39;} <h3 class="session-list-title"><a href="paper_2b485865-8584-417a-bd55-5a961dcff5ef.html"> AnnoLens: Exploration and Annotation through Lens-Based Guidance <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Franziska Becker, Steffen Koch, Tanja Blascheck </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Franziska Becker </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:24:00.000Z &ndash; 2025-11-06T09:33:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-short-1160&#39;, &#39;session_id&#39;: &#39;short3&#39;, &#39;title&#39;: &#39;Safire: Similarity Framework for Visualization Retrieval&#39;, &#39;contributors&#39;: [&#39;Huyen N. Nguyen&#39;], &#39;paper_type&#39;: &#39;Short&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T09:33:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T09:33:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T09:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Huyen N. Nguyen&#39;, &#39;email&#39;: &#39;huyen_nguyen@hms.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}, {&#39;name&#39;: &#39;Nils Gehlenborg&#39;, &#39;email&#39;: &#39;nils@hms.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}], &#39;abstract&#39;: &#39;Effective visualization retrieval necessitates a clear definition of similarity. Despite the growing body of work in specialized visualization retrieval systems, a systematic approach to understanding visualization similarity remains absent. We introduce the Similarity Framework for Visualization Retrieval (Safire), a conceptual model that frames visualization similarity along two dimensions: comparison criteria and representation modalities. Comparison criteria identify the aspects that make visualizations similar, which we divide into primary facets (data, visual encoding, interaction, style, metadata) and derived properties (data-centric and human-centric measures). Safire connects what to compare with how comparisons are executed through representation modalities. We categorize existing representation approaches into four groups based on their levels of information content and visualization determinism: raster image, vector image, specification, and natural language description, together guiding what is computable and comparable. We analyze several visualization retrieval systems using Safire to demonstrate its practical value in clarifying similarity considerations. Our findings reveal how particular criteria and modalities align across different use cases. Notably, the choice of representation modality is not only an implementation detail but also an important decision that shapes retrieval capabilities and limitations. Based on our analysis, we provide recommendations and discuss broader implications for multimodal learning, AI applications, and visualization reproducibility.&#39;, &#39;uid&#39;: &#39;ba183a14-a911-4177-8f18-c35ee2220c67&#39;, &#39;keywords&#39;: [&#39;Visualization retrieval&#39;, &#39;similarity framework&#39;, &#39;visualization similarity&#39;, &#39;representation modality&#39;, &#39;comparison&#39;], &#39;preprint_link&#39;: &#39;https://osf.io/preprints/osf/p47z5&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;To promote transparency and reproducibility, our work introduces Safire, a systematic framework that makes explicit the comparison criteria and representation modalities underlying visualization retrieval systems, helping researchers clearly articulate their similarity definitions and approaches. By analyzing existing systems through this lens, we provide actionable insights and recommendations to enhance the reproducibility and extensibility of future retrieval methods.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_ba183a14-a911-4177-8f18-c35ee2220c67.html"> Safire: Similarity Framework for Visualization Retrieval <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Huyen N. Nguyen, Nils Gehlenborg </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Huyen N. Nguyen </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T09:33:00.000Z &ndash; 2025-11-06T09:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-short.html">VIS Short Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T08:30:00+00:00'
    endTime = '2025-11-06T09:45:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "e2-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>