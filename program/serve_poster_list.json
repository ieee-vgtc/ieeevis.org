{"v-vis-posters-1030":{"abstract":"Guided data visualization systems are highly useful for domain experts to highlight important trends in their large-scale and complex datasets. However, more work is needed to understand the impact of guidance on interpreting data visualizations as well as on the resulting use of visualizations when communicating insights. We conducted two user studies with domain experts and found that experts benefit from a guided coarse-to-fine structure when using data visualization systems, as this is the same structure in which they communicate findings.","author_affiliations":["Yale University, New Haven, United States|Yale, New Haven, United States|Yale University, New Haven, United States&Yale University, New Haven, United States"],"authors":["Sherry Qiu","HOLLY RUSHMEIER","Kim RM Blenman"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"sherry.qiu@yale.edu","presenting_author_name":"Sherry Qiu","title":"Towards Understanding the Impact of Guidance in Data Visualization Systems for Domain Experts","uid":"v-vis-posters-1030"},"v-vis-posters-1032":{"abstract":"Gender-based violence (GBV) is a critical human rights issue where data visualizations play an important role in educating the public about its scale and impact, and in shaping the decisions of policymakers. However, this area often suffers from missing and inconsistent data due to underreporting, lack of infrastructural support, and varying definitions of GBV across societal and cultural contexts, posing challenges for accurate visualizations. This study proposes an interdisciplinary framework for evaluating the reliability of GBV visualizations, focusing on the intersection between data quality and visual representation. The framework was applied to a representative sample of 15 countries from the World Health Organization Global Database on the Prevalence Violence against Women (VAW), the current largest visualization project on GBV. A summary dataset was produced based on the evaluation results and used to create analysis and \"meta-visualizations\" that reveal shortcomings of the visualization design and underlying data collection. The study reveals numerous limitations of the VAW project, including misleading color schemes, insufficient representation of data context and quality, and lack of diverse voices in data collection. The study calls for more culturally responsive data collection and nuanced visualization approaches to accurately portray and address GBV. The next step involves applying the framework to the remaining countries in the database and developing guidelines for more effective GBV data collection and visualization, thereby contributing to addressing GBV as a multidimensional, situated issue.","author_affiliations":["Brown University, Providence, United States|Brown University , Providence, RI, United States"],"authors":["Yifan Zhang","Helis Sikk"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"yifan_zhang4@brown.edu","presenting_author_name":"Yifan Zhang","title":"Mapping Inconsistencies: Applying an Interdisciplinary Framework to Evaluate Gender-based Violence Data Collection and Visualization","uid":"v-vis-posters-1032"},"v-vis-posters-1034":{"abstract":"In group art therapy, visualizing emotions through artistic creation is a challenge. In this work, we present an artificial intelligence (AI) visualization system named Metamood, which focuses on helping people automatically generate visual expressions of emotions in group art therapy.To create the exploration method, we conducted the following procedure:1) Using the Russell emotion model and Long Short-Term Memory (LSTM) for emotion classification.2) Creating the relationship between emotions and virtual environment design.3) Proposing individual and Explicit Shared Emotion visualization methods and deploying them in both Virtual Reality(VR) and Mixed Reality (MR) environments.","author_affiliations":["Beihang University, Beiiing, China&Beihang University, Beiiing, China|Academy of Arts and Design, Beijing, China&Academy of Arts and Design, Beijing, China|Beihang University, Beijing, China&Beihang University, Beijing, China|Beihang University, Beijing, China&Beihang University, Beijing, China"],"authors":["Fengyi Yan","Siyu Luo","Shuo Yan","Xukun Shen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"756573064@qq.com","presenting_author_name":"Fengyi Yan","title":"MetaMood: An AI-based Shared Emotion Visualisation in Immersive Healing Spaces","uid":"v-vis-posters-1034"},"v-vis-posters-1037":{"abstract":"A bespoke illustration is often used to grab attention and increase engagement in data communication. However, if the illustration is driven by data and needs to update as that data changes it now requires proficiency with both programming and illustration tools. The resultant gulf of execution has been identified in previous work and addressed with custom vector editing tools and integrations between existing vector editors and libraries like D3. We propose a technique for annotating SVG files and data files to enable dynamic illustrations using only existing no-code tools. We also examine the data mappings needed by such a system and the terms that illustrators use to describe those mappings.","author_affiliations":["SAS, Cary, United States|SAS Institute, Cary, United States|SAS Institute, Cary, United States"],"authors":["Jordan Riley Benson","Karl Prewo","Rajiv Ramarajan"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"jordan.riley.benson@gmail.com","presenting_author_name":"Jordan Riley Benson","title":"Dynamic Vector Graphics: Enabling Data-Driven Illustrations","uid":"v-vis-posters-1037"},"v-vis-posters-1038":{"abstract":"Analyzing hourly precipitation is crucial for identifying patterns, trends, and anomalies in rainfall, which is essential for designing urban drainage systems, flood forecasting, hydrological modeling, and climate studies. Although conducting an analysis by identifying statistical characteristics of storm events with hourly precipitation data is critical, no effective system is available to provide a precise understanding of storm events by evaluating historical data. This study presents an interactive, web-based statistical rainfall analysis system that compiles all available hourly precipitation data across all U.S. states. It is designed as a web-based visual analytics system, utilizing multiple visualizations with supporting multiple user interaction techniques to facilitate comprehensive and interactive data analysis.","author_affiliations":["Univ. of the District of Columbia, Washington, United States|University of the District of Columbia, Washington, United States|University of the District of Columbia, Washington, United States|Bowie State University, Bowie, United States"],"authors":["Dong Hyun Jeong","Pradeep Behera","Brian Higgs","Soo-Yeon Ji"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"djeong@udc.edu","presenting_author_name":"Dong Hyun Jeong","title":"Designing an Interactive Web-based Rainfall Analysis System","uid":"v-vis-posters-1038"},"v-vis-posters-1039":{"abstract":"In the current field of quantum computing, data visualization stands out as a potential game-changer for the upcoming quantum revolution. The relevance and importance of data visualization are highlighted in various scientific domains; there is an existing in-text data visualization within quantum computing. In this work, we propose an approach to address this void, bridging classical and quantum methodologies for effective text data visualization and presenting a promising path for future developments in emerging fields such as quantum machine learning and quantum natural language processing.","author_affiliations":["Florida Institute of Technology, Melbourne, United States|Florida Institute of Technology, Melbourne, United States"],"authors":["Abu Kaisar Mohammad Masum","Naveed Mahmud"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"abu.cse@diu.edu.bd","presenting_author_name":"Abu Kaisar Mohammad Masum","title":"QuanText: Text Data Visualization in Quantum Computing","uid":"v-vis-posters-1039"},"v-vis-posters-1042":{"abstract":"The proliferation of mobile and wearable sensing research has amplified the need for in-the-wild data collection to capture and analyze human behaviors. However, data quality can be compromised by factors such as participants turning off devices, failing to respond to surveys, or not wearing sensors. We developed a visualization dashboard to monitor missing data in mobile and wearable data collection campaigns so that researchers can find and handle the problems. The tool utilizes a simple quality metric of item count, along with statistical quality control mechanisms, to help researchers quickly identify and prevent significant missing data issues. The dashboard's feasibility was validated through a real-world field study, demonstrating its utility in highlighting missing data and facilitating researcher intervention.","author_affiliations":["KAIST, Daejeon, Korea, Republic of|KAIST, Daejeon, Korea, Republic of"],"authors":["Yugyeong Jung","Uichin Lee"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"yugyeong.jung@kaist.ac.kr","presenting_author_name":"Yugyeong Jung","title":"Visual Analytics System for Monitoring Mobile and Wearable Sensing Data Collection Campaigns","uid":"v-vis-posters-1042"},"v-vis-posters-1043":{"abstract":"In personal and unstructured multi-criteria decision making (MCDM) contexts such as job seeking, qualitative factors like work culture and team dynamics can be as important as quantitative factors like salary and commute distance. However, most MCDM visualization tools, such as LineUp and ValueCharts, focus on quantitative data, often overlooking qualitative criteria. This gap appears to stem from a lack of studies on real-world decision making tasks. To address this, we conducted in-depth interviews with job seekers, emphasizing the integration and prioritization of qualitative data. After investigating the role of qualitative data in decision making, we introduced and evaluated a tool that extends LineUp\u201a\u00c4\u00f4s features to support qualitative criteria. Our insights underscore the vital role of qualitative data in job seeking, illustrating how visualization design can better accommodate the nuanced preferences inherent in these decision making processes.","author_affiliations":["Utrecht University, Utrecht, Netherlands|-, Utrecht, Netherlands|Utrecht University, Utrecht, Netherlands"],"authors":["Ba\u015fak Oral","Robert V\u00f5eras","Evanthia Dimara"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"e.oral@uu.nl","presenting_author_name":"Ba\u015fak Oral","title":"Iterative Quantification of Categorical Criteria for Enhanced Job Seeking","uid":"v-vis-posters-1043"},"v-vis-posters-1044":{"abstract":"We present our initial work on integrating a conversational agent using a large-language model (LLM) in OpenSpace, to provide conversational-based navigation for astrophysics visualization software. We focus on applications of visualization for education and outreach, where the versatility and intuitiveness of conversational agents can be leveraged to provide engaging and meaningful learning experiences. Visualization benefits from the development of LLMs by leveraging its capability to understand requests in natural language, allowing users to express complex tasks efficiently. Natural Language Interfaces can be combined with more traditional visualization interaction techniques, streamlining real-time interaction and facilitating free data exploration. We thus instructed a voice-controlled GPT-4o LLM to send commands to an OpenSpace instance, effectively providing the LLM with the the ability to steer the visualization software as a museum facilitator would for educational shows. We present our implementation and discuss future possibilities.","author_affiliations":["LiU Link\u00f6ping Universitet, Norrk\u00f6ping, Sweden|Link\u00f6ping University, Norrk\u00f6ping, Sweden|Link\u00f6ping University, Norrk\u00f6ping, Sweden|Inria, Saclay, France|Link\u00f6ping University, Norrk\u00f6ping, Sweden|Link\u00f6ping University, Norrk\u00f6ping, Sweden"],"authors":["Mathis Brossier","Alexander Bock","Konrad J Sch\u00f6nborn","Tobias Isenberg","Anders Ynnerman","Lonni Besan\u00e7on"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"mathis.brossier@liu.se","presenting_author_name":"Mathis Brossier","title":"In space, no one (but AI) can hear you scream","uid":"v-vis-posters-1044"},"v-vis-posters-1045":{"abstract":"Visualizing both temporal and spatial aspects of trajectories on a 2D plane presents challenges due to the need to sacrifice one dimension in a single view. In this work, we try to address these challenges by proposing a 2D multi-view approach that enhances user interaction and comprehension compared to traditional 3D methods like the space-time cube. Our approach emphasizes spatial visualization while enabling interactive selection of regions of interest (ROI) for detailed temporal analysis. Preliminary tests demonstrate the feasibility of our method for large-scale spatio-temporal data, with future work focusing on refining temporal interaction flexibility and direction selection within the spatial view.","author_affiliations":["University of Konstanz, Konstanz, Germany|University of Konstanz, Konstanz, Germany|University of Konstanz, Konstanz, Germany|University of Konstanz, Konstanz, Germany|Renmin University of China, Beijing, China|University of Konstanz, Konstanz, Germany"],"authors":["Yumeng Xue","Patrick Paetzold","Bin Chen","Rebecca Kehlbeck","Yunhai Wang","Oliver Deussen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"yumeng.xue@uni-konstanz.de","presenting_author_name":"Yumeng Xue","title":"Exploring Large-scale Trajectory Data through 2D Time-space View","uid":"v-vis-posters-1045"},"v-vis-posters-1048":{"abstract":"Currently, designing and building accessible data navigation using a tool such as Data Navigator is difficult due to the complexity of knowledge required to specify interaction possibilities using code. Exploring and expressing ideal experiences for end-users becomes time consuming. Our solution, Skeleton, provides a visual user interface canvas (with affordances similar to popular tools such as Figma or tldraw) and automatically generates code based on specified visual design constraints. Our approach enables creators to visually design data navigation structures without having to worry about code while easily adjusting their designs as they see fit. We hypothesize this interface will provide a flexible design experience that facilitates more active collaboration, faster iterations, and clearer communication of prototype ideas through demonstration.","author_affiliations":["Carnegie Mellon University, Pittsburgh, United States|Carnegie Mellon University, Pittsburgh, United States|Carnegie Mellon University, Pittsburgh, United States"],"authors":["Chieri J Nnadozie","Frank Elavsky","Dominik Moritz"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"cnnadozi@andrew.cmu.edu","presenting_author_name":"Chieri J Nnadozie","title":"Skeleton: Facilitating collaborative design and development scaffolding of accessible data navigation experiences","uid":"v-vis-posters-1048"},"v-vis-posters-1050":{"abstract":"Uncertainty visualization has recently raised significant attention within visualization communities, as it could allow for more informed decision-making. While various uncertainty visualization methods have been proposed, their evaluation is challenging and often does not capture the practical effect on decision-making. This study introduces a novel game-based evaluation approach to assess the effectiveness of uncertainty visualization on decision-making. In this paper, we describe a game to benchmark various uncertainty visualization techniques developed for brain tumor surgery. The results of our study with domain experts show that the simulation enhances the perception and comprehension of uncertainty. Participants' decisions, free from harmful consequences for patients, allow them to explore and understand the outcomes of different choices. Our findings indicate that participants performed better and scored higher with uncertainty visualization than without.","author_affiliations":["University of Massachusetts Boston, Boston, United States|Harvard Medical School, Boston, United States|Brigham and Women's Hospital, Boston, United States|Inria, Strasbourg, France|Brigham and Women's Hospital, Boston, United States|Isomics, Inc., Cambridge, United States|Brigham and Women's Hospital, Boston, United States|Brigham and Women's Hospital, Boston, United States|University of Massachusetts Boston, Boston, United States|Brigham and Women's Hospital, Boston, United States"],"authors":["Mahsa Geshvadi","Reuben Dorent","Colin Galvin","Nazim Haouchine","Tina Kapur","Steve Pieper PhD","William Wells","Alexandra J. Golby","Daniel Haehn","Sarah Frisken"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"mahsa.geshvadi001@umb.edu","presenting_author_name":"Mahsa Geshvadi","title":"Game-Based Evaluation of Uncertainty Visualization","uid":"v-vis-posters-1050"},"v-vis-posters-1053":{"abstract":"The rise of single-cell RNA sequencing (scRNA-seq) technologies has increased the need for visual analytical tools. Current visualization tools often cater to specific analysis stages rather than the entire pipeline, and many analyses rely on computational notebooks requiring significant programming skills. To address these challenges, we developed scFlowVis, a web-based visualization application designed in collaboration with bioinformatics and biological experts. scFlowVis supports the entire analysis pipeline for scRNA-seq data, providing visualization and interactive assistance throughout the process without programming. It dynamically links panel functionalities, allowing users to explore data attributes, compare parameters, and document workflows visually. The tool can also recommend appropriate visualizations based on data characteristics. Our experiences highlight the challenges and lessons in developing visualization tools for similar research contexts. scFlowVis has been released on https://github.com/EavanXing0416/scFlowVis.","author_affiliations":["King's College London, London, United Kingdom|Kings College London, London, United Kingdom|King","s College London, London, United Kingdom|Kings College London, London, United Kingdom|King's College London, London, United Kingdom"],"authors":["Yiwen Xing","stanley Odezi owomero","Sophia Tsoka","Rita Borgo","Alfie Abdul-Rahman"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"yiwen.xing@kcl.ac.uk","presenting_author_name":"Yiwen Xing","title":"scFlowVis: Streamlining scRNA-seq Analysis through Visual Design","uid":"v-vis-posters-1053"},"v-vis-posters-1055":{"abstract":"The growing popularity of interactive time series exploration platforms has made visualizing data of public interest more accessible to general audiences. At the same time, the democratized access to professional-looking explorers with preloaded data enables the creation of convincing visualizations with carefully cherry-picked items. Prior research shows that people use data explorers to create and share charts that support their potentially biased or misleading views on public health or economic policy and that such charts have, for example, contributed to the spread of COVID-19 misinformation. Interventions against misinformation have focused on post hoc approaches such as fact-checking or removing misleading content, which are known to be challenging to execute. In this work, we explore whether we can use visualization design to impede cherry-picking---one of the most common methods employed by deceptive charts created on data exploration platforms. We describe a design space of guardrails---interventions against cherry-picking in time series explorers. Using our design space, we create a prototype data explorer with four types of guardrails and conduct two crowd-sourced experiments to evaluate them. Based on our findings, we propose recommendations for developing effective guardrails for visualizations.","author_affiliations":["University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States"],"authors":["Maxim Lisnic","Zach Cutler","Marina Kogan","Alexander Lex"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"maxim.lisnic@utah.edu","presenting_author_name":"Maxim Lisnic","title":"Visualization Guardrails: Designing Interventions Against Cherry-Picking in Interactive Data Explorers","uid":"v-vis-posters-1055"},"v-vis-posters-1056":{"abstract":"We propose a system that supports contextually aware, controllable, and interactive exploration of academic publications and scholars. By enabling bidirectional interaction between question-answering components and Scholets, the 2D projections of scholarly works' embeddings, our system enables users to textually and visually interact with large amounts of publications. We report the system design and demonstrate its utility through an exploratory study with graduate researchers.","author_affiliations":["University of Waterloo, Waterloo, Canada|University of Waterloo, Waterloo, Canada|University of Waterloo, Waterloo, Canada|University of Waterloo, Waterloo, Canada|University of Waterloo, Waterloo, Canada"],"authors":["Ryan Yen","Yelizaveta Brus","Leyi Yan","Jimmy Lin","Jian Zhao"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"r4yen@uwaterloo.ca","presenting_author_name":"Ryan Yen","title":"Scholarly Exploration via Conversations with Scholars-Papers Embedding","uid":"v-vis-posters-1056"},"v-vis-posters-1059":{"abstract":"This work introduces two interaction techniques for examining multi-attribute spatial data for visual analytics applications: Layer Toggling and Visibility-Preserving Lenses. With the rise of open city data and services, gathering information at the city scale, like transportation, modeling, and predicting city events such as crime and traffic, has become feasible. Visually assessing the quality of these models requires correlating features, predictions, and ground truth both globally and locally, which is challenging with multi-attribute data due to occlusion. To address this, we introduce Layer Toggling for instantaneously changing layer visibility using a physical button box, allowing comparison of spatially coherent views using retinal persistence. Visibility-preserving lenses dynamically adjust to the density of revealed features, facilitating the exploration of spatial (2D) and quantitative data (1D), such as temporal attributes. We validate our approach on a use case visualizing urban data from S\u221a\u00a3o Paulo city across multiple data layers. Our methods support user exploration and expert analysis tasks, especially in validating and interpreting prediction algorithm outcomes at both global and local scales.","author_affiliations":["Universidade de Sao Paulo, S\u00e3o Paulo, Brazil|Inria, Saclay, France&Universit\u00e9 Paris-Saclay, CNRS, Orsay, France|University of Sao Paulo, Sao Carlos, Brazil"],"authors":["Karelia Alexandra Vilca Salinas","Jean-Daniel Fekete","Luis Gustavo Nonato"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"karelia@usp.br","presenting_author_name":"Karelia Alexandra Vilca Salinas","title":"Navigating Multi-Attribute Spatial Data Through Layer Toggling and Visibility-Preserving Lenses","uid":"v-vis-posters-1059"},"v-vis-posters-1061":{"abstract":"This study presents a taxonomy that can analyze the design methods and components of XR based dashboards according to categories. The dashboard cases utilized in the analysis are design outputs that assist effective interaction for XR content and XR data visualization tool. For the study, after preliminary data investigation and composition of dashboard design categories with domain experts, we constructed a taxonomy by creating detailed sub-elements. Using the constructed taxonomy, we analyzed a total of 30 dashboard cases and derived dashboard design patterns. In the future, this study plans to create an exploration system that can examine the characteristics of dashboard cases and review the design process by utilizing the constructed taxonomy.","author_affiliations":["Sogang University, Seoul, Korea, Republic of|Ajou University, Suwon, Korea, Republic of|Sogang University, Seoul, Korea, Republic of"],"authors":["Hyoji Ha","Hyerim Joung","Sanghun Park"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"hjha0508@u.sogang.ac.kr","presenting_author_name":"Hyoji Ha","title":"A Taxonomy for Analyzing Dashboard Design in XR based Content and Data Visualization Tool","uid":"v-vis-posters-1061"},"v-vis-posters-1062":{"abstract":"Voronoi treemaps are a well-established tool for simultaneously depicting data points and their hierarchical relationships. In addition to the hierarchical structure, information that indicates neighborhood and similarities between data points frequently exists. Examples include shared attributes like borders between countries or contextualized semantic information such as embedding vectors derived from large language models. In this work, we introduce a novel adaptation of the traditional Voronoi treemap algorithm that leverages similarity information to optimize the proximities or neighborhoods of Voronoi cells during initialization and optimization to more accurately reflect their similarities. We demonstrate the practicality of our approach through multiple real-world examples drawn from the domains of infographics and linguistics.","author_affiliations":["University of Konstanz, Konstanz, Germany|University of Konstanz, Konstanz, Germany|University of Konstanz, Konstanz, Germany|University of Konstanz, Konstanz, Germany|Renmin University of China, Beijing, China|University of Konstanz, Konstanz, Germany"],"authors":["Rebecca Kehlbeck","Patrick Paetzold","Yumeng Xue","Bin Chen","Yunhai Wang","Oliver Deussen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"patrick.paetzold@uni-konstanz.de","presenting_author_name":"Patrick Paetzold","title":"Neighborhood-Preserving Voronoi Treemaps","uid":"v-vis-posters-1062"},"v-vis-posters-1063":{"abstract":"Spatiotemporal data analysis is a critical yet complex task. Methods that simplify the exploration of such data are essential for various purposes, including monitoring and hypothesis generation. In a recent study, we utilized a combination of clustering and dimensionality reduction techniques to spatially visualize patterns in time-series data. We evaluated several such methods to determine their effectiveness based on various validation metrics. Our findings revealed that methods based on k-means clustering generally outperform self-organizing maps on real-world datasets. Additionally, we introduced EpiVECS, an open-source web application that facilitates cluster embedding and exploration of results through interactive visualization. EpiVECS is accessible at https://episphere.github.io/epivecs.","author_affiliations":["Queen's University, Belfast, United Kingdom&NIH, Rockville, United States|Queen's University Belfast , Belfast , United Kingdom|National Institutes of Health, Rockville, United States"],"authors":["Lee Mason","Bl\u00e1naid Hicks","Jonas S Almeida"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"masonlk@nih.gov","presenting_author_name":"Lee Mason","title":"EpiVECS: Exploring Spatiotemporal Data Using Low-Dimensional Cluster Representations","uid":"v-vis-posters-1063"},"v-vis-posters-1066":{"abstract":"We explore the potential for glanceable real-time conversation timelines in the Augmented Reality (AR) space, with a focus on lightweight and non-intrusive design. In recent years, AR has become an increasingly popular option for conversational support, leading to many contributions in the field. However, there has been less focus on developing lightweight, non-intrusive, on-demand conversation visualizations in AR. It is important that visualizations don't distract from the conversation partner or the conversation itself. Furthermore, the information displayed may not always be relevant to the user at every moment. Therefore, we explore methodologies to dissect and visualize conversation timelines in real time, providing glanceable summarizations and breakdowns of conversations which are easily summoned and dismissed. In our prototyping, we use a combination of Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to recognize and classify changes in topic. To display the visualization, we use the XReal sunglasses as a lightweight unobtrusive headset which could reasonably be worn in everyday life. We identify a unique set of visualization challenges and opportunities related to minimalist conversation visualizations. Following these explorations, we discuss our findings, and provide our initial reflections on various visualization techniques.","author_affiliations":["University of Calgary, Calgary, Canada|University of Calgary, Calgary, Canada"],"authors":["Shanna Li Ching Hollingworth","Wesley Willett"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"shanna.hollingwor1@ucalgary.ca","presenting_author_name":"Shanna Li Ching Hollingworth","title":"Towards Glanceable On-Demand AR Conversation Visualization","uid":"v-vis-posters-1066"},"v-vis-posters-1067":{"abstract":"Annotations are crucial in visualizations, communicating insights and directing attention to key visual elements. Understanding and practicing professional annotation techniques can significantly enhance visualization clarity and effectiveness. Our study analyzes 72 professionally designed static charts with annotations from major US news outlets, using a qualitative approach to identify annotation types, assess their frequency, explore annotation combinations, categorize text quantity, and examine the relationship between chart captions and annotations. The analysis reveals professionals' common strategies: alignment with captions, targeted highlighting, descriptive text, strategic use of multiple annotations, and emphasis on article-related numbers. These findings offer guidance for improving annotation practices, tools, and methodologies, enhancing data comprehension and communication in visualizations.","author_affiliations":["University of Utah, Salt Lake City, United States&University of Utah, Salt Lake City, United States|University of Oklahoma, Norman, United States&University of Oklahoma, Norman, United States|University of Utah, Salt Lake City, United States&University of Utah, Salt Lake City, United States"],"authors":["Md Dilshadur Rahman","Ghulam Jilani Quadri","Paul Rosen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"dilshadur@sci.utah.edu","presenting_author_name":"Md Dilshadur Rahman","title":"How Do Professionals Use Annotations in Visualizations?","uid":"v-vis-posters-1067"},"v-vis-posters-1068":{"abstract":"Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and smooth transitions across abstraction levels of math operations and model structures. It runs a live GPT-2 model locally in the user\u201a\u00c4\u00f4s browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public\u201a\u00c4\u00f4s access to AI education. Within the first ten days of release, it drew over 60,000 users. Our open-sourced tool is available at https://poloclub.github.io/transformer-explainer/. A video demo is available at https://youtu.be/ECR4oAwocjs.","author_affiliations":["Georgia Institute of Technology, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Georgia Tech, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Georgia Tech, Atlanta, United States|Georgia Tech, Atlanta, United States|IBM Research AI, Cambridge, United States|Georgia Tech, Atlanta, United States"],"authors":["Aeree Cho","Grace C. Kim","Alexander Karpekov","Alec Helbling","Zijie J. Wang","Seongmin Lee","Benjamin Hoover","Duen Horng (Polo) Chau"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"aeree@gatech.edu","presenting_author_name":"Aeree Cho","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","uid":"v-vis-posters-1068"},"v-vis-posters-1069":{"abstract":"Data visualizations are commonly employed to convey relationships between variables from complex datasets in exploratory data analysis. Recent advancements in Large Language Models (LLMs) have shown surprising performance in assisting data analysis and visualization. In this poster, we investigate the capabilities of LLMs for reasoning about causality between concept pairs in visualized data using line charts, bar charts, and scatterplots. By using LLMs to replicate two human-subject empirical studies about causality judgments, we how their inferences about causality between concept pairs compare to those of humans, both with and without accompanying visualizations showing varying association levels. Our findings indicate that LLMs' causality inferences are more likely to align with human results without visualizations at very high or very low causal ratings, but LLMs are more influenced by low visualized associations and relatively unaffected by high visualized associations.","author_affiliations":["University of North Carolina-Chapel Hill, Chapel Hill, United States|UNC-Chapel Hill, Chapel Hill, United States|University of North Carolina, Chapel Hill, United States"],"authors":["Arran Zeyu Wang","David Borland","David Gotz"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"zeyuwang@cs.unc.edu","presenting_author_name":"Arran Zeyu Wang","title":"Leveraging LLMs to Infer Causality from Visualized Data: Alignments and Deviations from Human Judgments","uid":"v-vis-posters-1069"},"v-vis-posters-1070":{"abstract":"Collage techniques are commonly used in visualization to organize a collection of geometric shapes, facilitating the representation of visual features holistically, as seen in word clouds or circular packing diagrams. Typically, packing methods rely on object-space optimization techniques, which often necessitate customizing the optimization process to suit the complexity of geometric primitives and the specific application requirements. In this paper, we introduce a versatile image-space collage technique designed to pack geometric elements into a given shape. Leveraging a differential renderer and image-space losses, our optimization process is highly efficient and can easily accommodate various loss functions. We demonstrate the diverse visual expressiveness of our approach across various visualization applications. The project page is https://szuviz.github.io/pixel-space-collage-technique/.","author_affiliations":["Shenzhen University, Shenzhen, China&Shenzhen University, Shenzhen, China|Tel Aviv University, Tel Aviv, Israel|Shenzhen University, Shenzhen, China"],"authors":["Zhenyu Wang","Daniel CohenOr","Min Lu"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"lumin.vis@gmail.com","presenting_author_name":"Min Lu","title":"A Versatile Collage Visualization Technique","uid":"v-vis-posters-1070"},"v-vis-posters-1071":{"abstract":"Visualization facilitates visual comprehension of salient patterns in complex datasets. However, high-level visual comprehension varies significantly based on individuals' backgrounds, for example, education and professional experience, indicating the need to understand the hierarchical nature of visual comprehension. This work explores how visualization comprehension can be hierarchical based on the individual's background. We interviewed ten participants, showing them eight stimuli scatterplots to investigate variation among participants' high-level comprehension. Participants described each of the tested graphs using natural language. The descriptions were coded using axial coding to identify their alignments with the designer's intentions. The results revealed that high-level comprehension could be of four levels: basic reading of the graph, conceptual understanding, common graph knowledge, and statistics and patterns. Our findings show that by understanding individual's differences in comprehension levels, visualization designers can enhance the accessibility and effectiveness of their visualizations, ensuring that they communicate information more effectively to people with diverse backgrounds and expertise levels.","author_affiliations":["University of Oklahoma, Norman, United States|University of North Carolina-Chapel Hill, Chapel Hill, United States|University of North Carolina-Chapel Hill, Chapel Hill, United States|University of Oklahoma, Norman, United States"],"authors":["Faraz Naeinian","Arran Zeyu Wang","Danielle Albers Szafir","Ghulam Jilani Quadri"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"quadri@ou.edu","presenting_author_name":"Ghulam Jilani Quadri","title":"Exploring the Hierarchical Nature of Visual Comprehension Through the Lens of Individual Differences","uid":"v-vis-posters-1071"},"v-vis-posters-1072":{"abstract":"Planetarium dome show films combine cinematic scientific visualizations with animations and recorded videos to provide immersive educational experiences worldwide. However, quantifying their global reach presents significant challenges due to the lack of standardized viewership tracking mechanisms across diverse planetarium venues. We present an analysis of the global impact of these shows in planetariums, presenting data regarding four documentary films from a single visualization lab. Specifically, we designed and administered a viewership survey of four long-running planetarium shows that contained cinematic scientific visualizations. Reported survey data shows that between 1.2 - 2.6 million people have viewed these four films across the 68 responding planetariums. When we include estimates and extrapolate for the 315 planetariums that licensed these shows, we arrive at an estimate of 16.5 - 24.1 million people having seen these films.","author_affiliations":["Scientific Computing and Imaging Institute, Salt Lake City, United States&National Center for Supercomputing Applications, Urbana, United States|University of Illinois at Urbana-Champaign, Urbana, United States|University of Illinois at Urbana-Champaign, Urbana, United States|National Center for Supercomputing Applications, Urbana, United States|University of Illinois at Urbana-Champaign, Urbana, United States|National Center for Supercomputing Applications, Urbana, United States|The University of Utah, Salt Lake City, United States"],"authors":["Kalina Borkiewicz","Eric Jensen","Yiwen Miao","Stuart Levy","J.P. Naiman","Jeffrey D Carpenter","Katherine E. Isaacs"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"kalina@sci.utah.edu","presenting_author_name":"Kalina Borkiewicz","title":"Audience Reach of Scientific Data Visualizations in Planetarium-Screened Films","uid":"v-vis-posters-1072"},"v-vis-posters-1073":{"abstract":"While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM\u201a\u00c4\u00f4s text generation. Our library offers a new way to quickly attribute an LLM\u201a\u00c4\u00f4s text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/LLM-Attribution.","author_affiliations":["Georgia Tech, Atlanta, United States|Georgia Tech, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Georgia Tech, Atlanta, United States|Google, Atlanta, United States"],"authors":["Seongmin Lee","Zijie J. Wang","Aishwarya Chakravarthy","Alec Helbling","ShengYun Peng","Mansi Phute","Duen Horng (Polo) Chau","Minsuk Kahng"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"seongmin@gatech.edu","presenting_author_name":"Seongmin Lee","title":"LLM Attributor: Interactive Visual Attribution for LLM Generation","uid":"v-vis-posters-1073"},"v-vis-posters-1074":{"abstract":"Dimensionality reduction is a widely adopted tool in Natural Language Processing (NLP). Techniques such as Uniform Manifold Approximation and Projection (UMAP) transform high-dimensional embeddings of text data into a lower-dimensional space for visualization. Two-dimensional plots of these embeddings aid in developing insights into model performance. To make sense of these plots, users need to inspect the underlying text represented by the points which can be time-consuming and cognitively intensive. To address this challenge, we developed a novel approach for summarizing and analyzing data behind user selections in text embedding plots. Our interactive approach involves allowing the user to make selections on the text embedding and then utilizing a large-language model (LLM) for: getting a quick overview of the selection, identifying instances of miss-classification, understanding text data within a mixed-class selection, and suggesting additional labels that better fit the underlying text. We implemented our approach in a prototype application, Text-Embedding Selection Sidekick (TESS), and present our initial results.","author_affiliations":["University of Cincinnati, Cincinnati, United States|University of Cincinnati, Cincinnati, United States|University of Cincinnati, Cincinnati, United States"],"authors":["Allen Detmer","Raj K Bhatnagar","Jillian Aurisano"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"allen.detmer@gmail.com","presenting_author_name":"Allen Detmer","title":"LLM Assisted Analysis of Text-Embedding Visualizations","uid":"v-vis-posters-1074"},"v-vis-posters-1075":{"abstract":"The think-aloud protocol has long been recognized as a useful tool for conducting user studies. Crowd-sourced studies likely would also benefit from think-aloud, but the increased technical overhead, as well as the uncertainty of how participants may react to being asked to speak without a researcher present, have restricted think-aloud to in-person studies. To address this problem, we introduce CrowdAloud, a user study platform for creating and analyzing crowd-sourced think-aloud studies. Utilizing audio and task-level provenance data, CrowdAloud provides the necessary conditions for think-aloud studies to be run as well as a qualitative analysis conducted on the results, without the technical overhead involved with an ad-hoc solution. To evaluate the comparability of in-person and crowd-sourced studies, we conduct two identical think aloud studies, one in-person and one online with crowd-sourced participants. Based on our results, we discuss implications of crowd-sourced think-aloud studies.","author_affiliations":["University of Utah, Salt Lake City, United States|Worcester Polytechnic Institute, Worcester, United States|University of Toronto, Toronto, Canada|University of Utah, Salt Lake City, United States"],"authors":["Zach Cutler","Lane Harrison","Carolina Nobre","Alexander Lex"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"zcutler@sci.utah.edu","presenting_author_name":"Zach Cutler","title":"CrowdAloud: A Platform for Crowd-Sourced Think-Aloud Studies","uid":"v-vis-posters-1075"},"v-vis-posters-1076":{"abstract":"Data visualizations are typically not accessible to blind and low-vision users. The most widely used remedy for making data visualizations accessible is text descriptions. Yet, manually creating useful text descriptions is often omitted by visualization authors, either because of a lack of awareness or a perceived burden. Automatically generated text descriptions are a potential partial remedy. However, with current methods it is unfeasible to create text descriptions for complex scientific charts. In this poster, we describe our methods for generating text descriptions for one complex scientific visualization: the UpSet plot. UpSet is a widely used technique for the visualization and analysis of sets and their intersections. At the same time, UpSet is arguably unfamiliar to novices and used mostly in scientific contexts. Generating text descriptions for UpSet plots is challenging because the patterns observed in UpSet plots have not been studied. We first analyze patterns present in dozens of published UpSet plots. We then introduce software that generates text descriptions for UpSet plots based on the patterns present in the chart. Finally, we introduce a web service that generates text descriptions based on a specification of an UpSet plot, and demonstrate its use in both an interactive web-based implementation and a static Python implementation of UpSet.","author_affiliations":["University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States|Scientific Computing and Imaging Institute, Salt Lake City, United States|University College London, London, United Kingdom|University of Utah, Salt Lake City, United States"],"authors":["Ishrat Jahan Eliza","Jake Wagoner","Jack Wilburn","Nate Lanza","Daniel Hajas","Alexander Lex"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"ishratjahan.eliza@utah.edu","presenting_author_name":"Ishrat Jahan Eliza","title":"Enhancing Accessibility of UpSet Plots with Text Descriptions","uid":"v-vis-posters-1076"},"v-vis-posters-1077":{"abstract":"Reeb graphs are a topological analysis tool useful for abstracting data across various fields, e.g., medical imaging and geography. The Topology ToolKit (TTK) is a popular library for topological data analysis and visualization, including Reeb graphs. However, TTK\u201a\u00c4\u00f4s visualization method struggles to accurately represent Reeb graph arcs. This paper presents a new algorithm that improves Reeb graph visualizations by ensuring arcs stay within model boundaries, follow the shortest path between critical points, and align with the gradient of the elevation function. Qualitative and quantitative eval- uations demonstrate significant improvements.","author_affiliations":["University of Utah, Salt Lake City, United States&University of Utah, Salt Lake City, United States|Oak Ridge National Laboratory, Oak Ridge, United States&Oak Ridge National Laboratory, Oak Ridge, United States|University of Utah, Salt Lake City, United States&University of Utah, Salt Lake City, United States"],"authors":["Sefat E Rahman","Tushar M. Athawale","Paul Rosen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"sefat.rahman@utah.edu","presenting_author_name":"Sefat E Rahman","title":"GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 3D Reeb Graphs","uid":"v-vis-posters-1077"},"v-vis-posters-1078":{"abstract":"Ecosystems are important for maintaining the planet's ecological balance, supporting biodiversity, and providing resources. The Global Ecosystem Dynamics Investigation (GEDI) mission uses space lidar to measure the 3D structure of global vegetation. The observations from GEDI, in the form of waveforms, hold the potential to reveal the spatial patterns and variation of ecosystem structures. However, it is difficult to understand this data due to its multivariate nature and random spatial sampling. Here, we use machine learning to develop a binning-based technique to visualize the spatial variation of large quantities of GEDI waveforms. This method can highlight the transition of different ecosystems through sharp changes in waveform distribution. In addition, it can reveal the internal heterogeneity of defined ecoregions. We expect this method to be helpful in studies of global ecosystems and generalize to other remote sensing data.","author_affiliations":["Brown University, Providence, United States|Brown University, Providence, United States|Brown University, Providence, United States|Brown University, Providence, United States|Brown University, Providence, United States"],"authors":["Ziang Liu","James Tompkin","Matthew Harrison","James R. Kellner","David H. Laidlaw"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"ziang_liu@brown.edu","presenting_author_name":"Ziang Liu","title":"Exploring Global Ecosystem Variation through GEDI waveforms","uid":"v-vis-posters-1078"},"v-vis-posters-1079":{"abstract":"This poster showcases AdaptLIL, a real-time adaptive link-indented list ontology mapping visualization that uses eye gaze as the primary input source. Through a multimodal combination of real-time systems, deep learning, and web development applications, this system uniquely curtails graphical overlays (adaptations) to pairwise mappings of link-indented list ontology visualizations for individual users based solely on their eye gaze.","author_affiliations":["California State University, Long Beach, Long Beach, United States|California State University, Long Beach, Long Beach, United States"],"authors":["Nicholas Chow","Bo Fu"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"nickrjchow@gmail.com","presenting_author_name":"Nicholas Chow","title":"AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping","uid":"v-vis-posters-1079"},"v-vis-posters-1082":{"abstract":"Since visual assets are powerful tools for facilitating comprehension, educational resources, such as slides, videos, and textbooks, employ charts and visual metaphors to convey complex concepts. However, highly visual media like comics remain underused in higher education. To address this gap, we introduce Comixplain, a toolkit designed to enhance the learning of academic subjects through comics, specifically applied in visualization education. Through surveys, workshops, and focus groups, we identified key requirements that informed the creation of all assets. Two comics were subsequently integrated into two data visualization courses and evaluated with students and lecturers.","author_affiliations":["St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria|St. Poelten University of Applied Sciences, St. Poelten, Austria|Institute of CreativeMedia/Technologies, St. P\u00f6lten, Austria&Austrian Computer Society, Vienna, Austria|St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria|St. Poelten University of Applied Sciences, St. Poelten, Austria|St. Poelten University of Applied Sciences, St. Poelten, Austria"],"authors":["Magdalena Boucher","Christina Stoiber","Alena Boucher","Hsiang-Yun Wu","Wolfgang Aigner","Victor Adriel de Jesus Oliveira"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"magdalena.boucher@fhstp.ac.at","presenting_author_name":"Magdalena Boucher","title":"Comixplain: Comics on Visualization Foundations in Higher Education","uid":"v-vis-posters-1082"},"v-vis-posters-1083":{"abstract":"Creativity in visualisation design is essential for designers and data scientists who need to present data in innovative ways. It is often achieved through sketching or drafting low-fidelity prototypes. However, judging this innovation is often difficult. A creative visualisation test would offer a structured approach to enhancing visual thinking and design skills, which are vital across many fields. Such a test can facilitate objective evaluation, skill identification, benchmarking, fostering innovation, and improving learning outcomes. In developing such a test, we propose focusing on four criteria: Quantity, Correctness, Novelty, and Feasibility. These criteria integrate into a test that is easy to administer. We name it the Rowen Test of Creativity in Visualisation Design; We introduce the test, scoring system and results from using eight visualisation experts.","author_affiliations":["Bangor University, Bangor, United Kingdom|Bangor University, Bangor, United Kingdom"],"authors":["Aron E. Owen","Jonathan C Roberts"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"aron.e.owen@bangor.ac.uk","presenting_author_name":"Aron E. Owen","title":"Towards Metrics for Evaluating Creativity in Visualisation Design","uid":"v-vis-posters-1083"},"v-vis-posters-1084":{"abstract":"We present UniDistriVis, an innovative in-browser visualization tool designed to aid users in understanding and analyzing the probability density function (PDF) and cumulative distribution function (CDF) of discrete and continuous random variables and querying the shape, formula, and parameter meaning of all univariate distributions. While many existing tools such as Matlab and Python can plot an univariate distribution figure, UniDistriVis can realize it by dragging sliders without coding, which can be applicable to a variety of education and data analysis scenarios. Meanwhile, UniDistriVis is capable of anchoring parameters with a fixed random variable, visually demonstrating the impact of these parameter changes within a single graph. Furthermore, Unidistrivis uses its rich univariate relationship formulas to fit the user's own data to find the most suitable data distribution type. UniDistriVis is open-sourced at https://github.com/PinkR1ver/univariate-distribution-relationships and runs in all modern web browsers. A demo of the tool in action is available at: https://unidistrivis.streamlit.app/","author_affiliations":["Zhejiang University, Hangzhou, China|Shanghai Jiao Tong University, Shanghai, China|Fudan University, Shanghai, China"],"authors":["Yichong Wang","Tan Zhou","Yanhao Zhu"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"pinkr1veroops@gmail.com","presenting_author_name":"Yichong Wang","title":"UniDistriVis: Univariate Distribution All in One","uid":"v-vis-posters-1084"},"v-vis-posters-1087":{"abstract":"AI prevails in financial fraud detection and decision making. Yet, due to concerns about biased automated decision making or profiling, regulations mandate that final decisions are made by humans. Financial fraud investigators face the challenge of manually synthesizing vast amounts of unstructured information, including AI alerts, transaction histories, social media insights, and governmental laws. Current Visual Analytics (VA) systems primarily support isolated aspects of this process, such as explaining binary AI alerts and visualizing transaction patterns, thus adding yet another layer of information to the overall complexity. In this work, we propose a framework where the VA system supports decision makers throughout all stages of financial fraud investigation, including data collection, information synthesis, and human criteria iteration. We illustrate how VA can claim a central role in AI-aided decision making, ensuring that human judgment remains in control while minimizing potential biases and labor-intensive tasks.","author_affiliations":["Utrecht University, Utrecht, Netherlands|Utrecht University, Utrecht, Netherlands"],"authors":["Angelos Chatzimparmpas","Evanthia Dimara"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"a.chatzimparmpas@uu.nl","presenting_author_name":"Angelos Chatzimparmpas","title":"Aiding Humans in Financial Fraud Decision Making: Toward an XAI-Visualization Framework","uid":"v-vis-posters-1087"},"v-vis-posters-1088":{"abstract":"Efficiently communicating information from categorical datasets to users is a crucial goal for data visualization. Recently, Large Language Models (LLMs) have been increasingly used to assist users in performing data analysis. In this poster, we investigated the capabilities of LLMs in interpreting categorical information in visualizations. We conducted two studies to analyze how effectively LLMs can estimate class means in multiclass scatterplots using both color and shape encodings. We found that LLMs struggled to comprehend categorical information with higher numbers of categories, especially compared to robust human performance found in past work. We argue that reasoning about data visualizations with categorical visual encodings remains a challenging task for current language models, highlighting the need for further research in this area.","author_affiliations":["University of North Carolina-Chapel Hill, Chapel Hill, United States|University of North Carolina at Chapel Hill, Chapel Hill, United States|University of North Carolina-Chapel Hill, Chapel Hill, United States"],"authors":["Arran Zeyu Wang","Matt-Heun Hong","Danielle Albers Szafir"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"zeyuwang@cs.unc.edu","presenting_author_name":"Arran Zeyu Wang","title":"Examining the Capabilities of LLMs in Interpreting Categorical Encodings from Data Visualizations","uid":"v-vis-posters-1088"},"v-vis-posters-1089":{"abstract":"Accuracy is a vital factor that affects trust in recommender systems and influences decision-making. Providing additional information, such as visualizations alongside accuracy, offers context that pure accuracy metrics lack. However, the role of visualizations like scatterplots, in influencing trust and decisions in recommender systems is under-explored. To bridge this gap, we conducted a human-subject experiment to investigate the impact of scatterplots on decisions made using recommender systems. Our study focuses on high-level decisions, such as selecting a recommender system. Our results indicate scatterplots accompanied with accuracies influences decisions and trust on recommender systems. Our findings represent the initial steps towards comprehending the role of scatterplots in recommender system decision making, highlighting the need to study visualizations in this aspect.","author_affiliations":["University of South Florida , Tampa, United States&University of South Florida , Tampa, United States|University of Utah, Salt Lake City, United States&University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States&University of Utah, Slat Lake City, United States"],"authors":["Bhavana Doppalapudi","Md Dilshadur Rahman","Paul Rosen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"bdoppalapudi@usf.edu","presenting_author_name":"Bhavana Doppalapudi","title":"Seeing is Believing: The Role Recommender System Accuracy Plays on Trust in Scatterplots","uid":"v-vis-posters-1089"},"v-vis-posters-1090":{"abstract":"With the widespread use of convolutional layers in various neural networks for vision tasks, understanding their complex structures has become crucial for their optimization. Using our developed open-source visualization tool, ChannelExplorer, this study employs well-known visualization techniques like scatterplots, heatmaps, and similarity matrices to examine activation patterns in model layers. These visualizations help users create class hierarchy in any classification dataset, identify weak activation channels in the model for pruning, and find confusion in the model to classify certain images. We demonstrate this by showing better class labeling of a popular dataset, ImageNet. Additionally, we demonstrate the model speed improvement opportunity by finding weak filters in a popular model, InceptionV3. Finally, we evaluated the effectiveness of the tool with expert users.","author_affiliations":["University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States|University of Utah, Salt Lake City, United States"],"authors":["Md Rahat-uz- Zaman","Bei Wang","Paul Rosen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"rahatzamancse@gmail.com","presenting_author_name":"Md Rahat-uz- Zaman","title":"ChannelExplorer: Visual Analytics at Activation Channel\u201a\u00c4\u00f4s Granularity","uid":"v-vis-posters-1090"},"v-vis-posters-1091":{"abstract":"Data visualization researchers publish in various conferences and journals. This paper will focus on a few of the main venues, VIS, EuroVis, and CHI. In addition to the published manuscript, researchers often create other artifacts related to the work, such as videos, blog posts, and deployed versions of the tools created. Some of these artifacts can be found through search engines, but others are not readily accessible. Additionally, performing a keyword search across visualization publication venues is not currently possible. This project aims to collect publications along with their auxiliary artifacts across venues into a single repository with a user-friendly interface.","author_affiliations":["University of Utah, Salt Lake City, United States"],"authors":["Devin Lange"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"devin@sci.utah.edu","presenting_author_name":"Devin Lange","title":"Vispubs.com: A Visualization Publications Repository","uid":"v-vis-posters-1091"},"v-vis-posters-1092":{"abstract":"The need for innovative ideas in data visualisation drives us to explore new creative approaches. Combining two or more creative words, particularly those that contradict each other, can positively impact the creative process, sparking novel ideas and designs. As we move towards AI-driven design, an open question arises: do these design contradictions work positively with AI tools? Currently, the answer is no. AI systems, like large language models (LLMs), rely on algorithms that engender similarity, whereas creativity often requires divergence and novelty. This poster initiates a conversation on how to drive AI systems to be more creative and generate new ideas. This research invites us to reconsider traditional design methods and explore new approaches in an AI-driven world. Can we apply the same techniques used in traditional design, like the double diamond model, or do we need new methods for design engineering? How can we quickly design visualisations and craft new ideas with generative AI? This paper seeks to start this critical conversation and offers practical insights into the potential of AI in driving creativity in data visualisation.","author_affiliations":["Bangor University, Bangor, United Kingdom|Bangor University, Bangor, United Kingdom"],"authors":["Aron E. Owen","Jonathan C Roberts"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"aron.e.owen@bangor.ac.uk","presenting_author_name":"Aron E. Owen","title":"Design Contradictions: Help or Hindrance?","uid":"v-vis-posters-1092"},"v-vis-posters-1093":{"abstract":"Data-art exhibitions offer a unique and real-world setting to foster creative visualisation skills among students. They serve as a real-world platform for students to display their work, bridging the gap between classroom learning and professional practice. Students must develop a technical solution, grasp the context, and produce work that is appropriate for public presentation. This scenario helps to encourage innovative thinking, engagement with the topic, and helps to enhance technical proficiency. We present our implementation of a data-art exhibition within a computing curriculum, for third-year degree-level students. Students create art-based visualisations from selected datasets and present their work in a public exhibition. We have used this initiative over the course of two academic years with different cohorts, and reflect on its impact on student learning and creativity.","author_affiliations":["Bangor University, Bangor, United Kingdom"],"authors":["Jonathan C Roberts"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"j.c.roberts@bangor.ac.uk","presenting_author_name":"Jonathan C Roberts","title":"Fostering Creative Visualisation Skills Through Data-Art Exhibitions","uid":"v-vis-posters-1093"},"v-vis-posters-1094":{"abstract":"Analyzing large sets of rankings brings many unique challenges, with existing methods often obscuring key insights and overlooking outlier perspectives. This work explores these challenges through a visualization system, FairSpace, designed to effectively aggregate and visualize consensus patterns within large-scale ranking data. Addressing limitations of existing methods, FairSpace incorporates techniques for handling large numbers of individual rankings, revealing and highlighting similarities and differences across and within rankings, uncovering levels of fairness at multiple levels of aggregation, and identifying potential biases or outliers. Through interactive visualizations and data exploration tools, the system empowers users to understand the reasons behind the fair consensus, fostering transparent and informed decision-making.","author_affiliations":["Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States"],"authors":["Hilson Shrestha","Kathleen Cachel","Mallak Alkhathlan","Elke A. Rundensteiner","Lane Harrison"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"hilsonshrestha@gmail.com","presenting_author_name":"Hilson Shrestha","title":"Exploring Fairness across Many Rankings","uid":"v-vis-posters-1094"},"v-vis-posters-1095":{"abstract":"As datasets increase in size and complexity, query latency and visualization rendering times can increase prohibitively. Analysts often pragmatically resort to performing exploratory analysis over a sampled subset of the data. However, a sampled subset may not accurately reflect key patterns, may miss important values, and may distort trends which are important for accurately completing common visual analytics tasks. Our work provides insights on the tradeoffs of various sampling methods. We conduct two experiments on five large-scale datasets using five sampling methods (two diverse, two probablistic, one visualization-based). The first experiment investigates the subjective ability of these five sampling methods to capture key features of the data. In our second experiment, we empirically measure user performance accuracy in common, low-level analytic tasks using visualizations sampled via the five methods. We evaluate the performance of these methods via three metrics: the accuracy of (1) human response at reading the sampled data values, (2) human perception in using samples to make inferences about the original data, and (3) human perception in using samples to correctly estimate the ground truth.","author_affiliations":["University of South Florida, Tampa, United States|University of Massachusetts Amherst, Amherst, United States|Adobe, Bangalore, India|Adobe Research, Bengaluru, India|Adobe Research, San Jose, United States|University of Massachusetts Amherst, Amherst, United States|Georgia Tech, Atlanta, United States"],"authors":["Hamza Elhamdadi","Maliha Tashfia Islam","Subrata Mitra","Iftikhar Ahamath Burhanuddin","Tong Yu","Alexandra Meliou","Cindy Xiong Bearfield"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"helhamdadi@umass.edu","presenting_author_name":"Hamza Elhamdadi","title":"Contrasting Diverse, Probabilistic, and Visualization-Based Data Selection Methods for Visual Analytics","uid":"v-vis-posters-1095"},"v-vis-posters-1096":{"abstract":"Charts often hold crucial details not explicitly mentioned in the text of the document. While existing chart question-answering (CQA) systems can generate real-time text and visualizations based on data, they often fail to capture the contextual relationship between text and charts along with the lack of dynamic feedback mechanisms for iterative refinement to create tailored report. In this paper, we introduce and evaluate a prototype workflow for a CQA tool through the use of LLMs to query text and chart data, transform existing charts, and create new visualizations to support tailored report generation. CQA is designed to facilitate the interactive querying of the text and chart data within a document, chart transformation, and generation of new tailored report containing text and new visualizations that are directly derived from the original source documents. An initial testing indicated that, with the prototype, users can derive new visualizations and query for detailed insights and explanations. Testing also identified areas such as function invocation errors and subsequent LLM hallucinations, highlighting areas for improving robustness and reliability.","author_affiliations":["Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States"],"authors":["Bijesh Shrestha","Roee Shraga","Lane Harrison"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"bshrestha@wpi.edu","presenting_author_name":"Bijesh Shrestha","title":"Exploring AI-Driven Interactive Chart Transformation and Visualization Creation","uid":"v-vis-posters-1096"},"v-vis-posters-1097":{"abstract":"Geospatial visualizations such as choropleth maps suffer from many well-documented biases. One recently developed technique, surprise maps, uses Bayesian weighting to reveal map areas that deviate from an expected model. While useful, such techniques can require significant time and expertise on the part of the user for adjusting parameter settings, often requiring code-based approaches. In this paper, we explore the use of coordinated multiple views address these challenges. We describe SurpriseSync, which 1) directly visualizes surprise models and parameters using coordinated multiple views, 2) enables model tuning and encoding adjustment via direct manipulation and interaction with visualizations, and 3) supports the creation and comparison of linked surprise models at multiple scales. We discuss how SurpriseSync aims to empower users to make discoveries in spatial data that are difficult or impossible to identify with choropleth maps or static Surprise Maps alone.","author_affiliations":["Worcester Polytechnic Institute, Worcester, United States|Worcester Polytechnic Institute, Worcester, United States|University of Colorado Boulder, Boulder, United States|Worcester Polytechnic Institute, Worcester, United States"],"authors":["Akim Ndlovu","Hilson Shrestha","Evan Peck","Lane Harrison"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"andlovu@wpi.edu","presenting_author_name":"Akim Ndlovu","title":"SurpriseSync: Visual Exploration for De-biased Choropleth Maps","uid":"v-vis-posters-1097"},"v-vis-posters-1098":{"abstract":"We present two challenges associated with our current work on a visual lexicon that expresses data absences and ambiguities. First, most visualization approaches fail to express the potential ambiguity and incompleteness in the data they represent. This can pose a fundamental sense-making and communication challenge in contexts (like community organizing), where official data sources and local knowledge may have little overlap or even disagree. Second, indicating expectations, ambiguity, or contradiction between community and administrative data is likely to increase visualization complexity. This increased complexity poses challenges for accessibility and engagement. We outline our work to create a visual lexicon and address the interactions between these challenges.","author_affiliations":["University of Calgary, Calgary, Canada|University of Calgary, Calgary, Canada"],"authors":["Karly Ross","Wesley Willett"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"karly.ross@ucalgary.ca","presenting_author_name":"Karly Ross","title":"I Do Not (Completely) Trust Your Data: Towards Visualization Lexicons for Ambiguous and Incomplete Data","uid":"v-vis-posters-1098"},"v-vis-posters-1099":{"abstract":"Overly complex visualizations have the potential to overwhelm audiences and occlude key takeaways during visualization experiences. While a long line of research in the visualization community has sought to identify 'best design practices' to make visualizations more interpretable, the community has yet to articulate how the presence of specific design features contributes to whether a visualization is deemed overwhelming or overly complex. As an initial exploration into this gap, we augmented the MASSVIS dataset, one of the most comprehensive static visualization datasets to-date, with visual complexity ratings and also novel metadata capturing design features related to text, color, underlying data, and chart types. We examined distributions of visual design features across visualizations in the MASSVIS dataset and compared their effects on complexity. Intuitively, we find that higher quantities of visual elements are associated with higher ratings of complexity. Moreover, we determine that visualizations with certain designs (e.g., diagrams) are associated with higher ratings of complexity as opposed to visualizations without, while other design features are associated with lower ratings (e.g., captions).","author_affiliations":["Georgia Institute of Technology, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Northwestern University, Evanston, United States|University of Massachusetts, Amherst , Amherst, United States|Georgia Tech, Atlanta, United States"],"authors":["Kylie R. Lin","Sean Sheng-tse Ru","David N. Rapp","Hui Guan","Cindy Xiong Bearfield"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"klin368@gatech.edu","presenting_author_name":"Kylie R. Lin","title":"What Makes a Visualization Visually Complex? Exploring Design Features Related to Visual Complexity","uid":"v-vis-posters-1099"},"v-vis-posters-1100":{"abstract":"Line charts can surface many relevant features in time series data, from trends to periodicity to peaks and valleys. However, not every potentially important feature in the data may correspond to a visual feature which readers can detect, attend to, or value. In this work, we perform a mixed-methods study, where participants engage in a visual stenography task in which they re-draw line charts, to solicit information about the visual features that participants believe to be important in line charts and how faithfully and accurately they recreate them. We identified three predominant strategies, whose use correlated with the noise present in the stimuli: the replicators attempted to retain all major features of the line chart; the trend keepers faithfully retained trends but no other features; and the overwhelmed only represented the noise. Further, we found that participants tended to faithfully retain trends and peaks and valleys when these features were present, while periodicity and noise were represented in more qualitative or gestural ways: semantically rather than accurately. These results suggest a need to consider more flexible and human-centric ways of presenting, summarizing, pre-processing, or clustering time series data.","author_affiliations":["University of Utah, Salt Lake City, United States|Northeastern University, Portland, United States|University of Oklahoma, Norman, United States|University of Utah, Salt Lake City, United States"],"authors":["Rifat Ara Proma","Michael Correll","Ghulam Jilani Quadri","Paul Rosen"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"u1450373@umail.utah.edu","presenting_author_name":"Rifat Ara Proma","title":"Visual Stenography: Feature Recreation and Preservation in Sketches of Line Charts","uid":"v-vis-posters-1100"},"v-vis-posters-1101":{"abstract":"We present a novel transformation method that converts extensive Earth science datasets into various 3D representations for narrative data visualizations. Our method comprises two main components: First, we classify Earth science data by analyzing the spatial and temporal structures of the datasets. This involves determining if the intervals within these dimensions are regular or irregular and integrating domain knowledge about the physical climate phenomena depicted in the data. Second, the transformation functions aim to preserve the accuracy and integrity of the original data representation, conveying both qualitative and quantitative attributes. We applied this method to Earth science datasets and developed a narrative visualization that helps people understand a wildfire scenario.","author_affiliations":["The University of Alabama in Huntsville, Huntsville, United States|NASA, Huntsville, United States|University of Alabama in Huntsville, Huntsville, United States"],"authors":["Connor Bleisch","Manil Maskey","Haeyong Chung"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"hc0021@uah.edu","presenting_author_name":"Haeyong Chung","title":"Generalized Transformation of Earth Science Datasets for 3D Narrative Visualization","uid":"v-vis-posters-1101"},"v-vis-posters-1103":{"abstract":"We outline a geovisual analytic method that is suited for large multiplex spatial networks (i.e., networks with different edge types). We use a county-to-county network of migrants, commuters, travelers, Facebook friends, and Twitter ties in the contiguous U.S. totaling over 1 billion weighted flows. Our goal is to visualize strong and weak connections in the U.S. To do so, we apply a network community detection algorithm to create communities (i.e., subgraphs) from each of the five input data sets. Then, we use an adjacency matrix of counties to visualize the number of times each pair of adjacent counties is assigned to the same subgraph. The resulting map shows tight-knit regions and large divides in the country and provides an alternative to mapping large origin-destination flows without losing key information.","author_affiliations":["Georgia Tech, Atlanta, United States|University of Iowa, Iowa City, United States Minor Outlying Islands|University of California, Los Angeles, Los Angeles, United States"],"authors":["Clio Andris","Caglar Koylu","Mason A Porter"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"clio@gatech.edu","presenting_author_name":"Clio Andris","title":"Visualizing Large Multiplex Geographic Network Data using a Regionalization Approach","uid":"v-vis-posters-1103"},"v-vis-posters-1104":{"abstract":"Understanding and inferring causal relationships between variables is a fundamental task in visualization and visual analysis. However, it can be challenging to verify inferences of causal relationships from traditional observational data because they often lack a ground truth causal model, complicating the evaluation of visual causal inference tools. To address this challenge, we introduce CausalSynth, an interactive web application designed to generate synthetic datasets from user-defined causal relationships. CausalSynth enables users to define acyclic causal graphs via a user-friendly graphical interface, establish interrelationships between variables, and produce datasets that reflect these desired causal interactions. The application also includes built-in tools for visualizing the generated datasets, facilitating deeper insights into the user-defined causal structure and aiding the validation of the generated data. By providing a user-friendly interface for synthetic data generation and visualization based on ground truth causal models, CausalSynth helps support more meaningful evaluations of visual causal inference technologies.","author_affiliations":["University of North Carolina - Chapel Hill, Chapel Hill, United States|University of North Carolina-Chapel Hill, Chapel Hill, United States|UNC-Chapel Hill, Chapel Hill, United States|University of North Carolina, Chapel Hill, United States"],"authors":["Zhehao Wang","Arran Zeyu Wang","David Borland","David Gotz"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"zhehaow24@gmail.com","presenting_author_name":"Zhehao Wang","title":"CausalSynth: An Interactive Web Application for Synthetic Dataset Generation and Visualization with User-Defined Causal Relationships","uid":"v-vis-posters-1104"},"v-vis-posters-1105":{"abstract":"We introduce a novel crowdsourcing method for identifying important areas in graphical images through punch-hole labeling. Traditional methods, such as gaze trackers and mouse-based annotations, which generate continuous data, can be impractical in crowdsourcing scenarios. They require many participants, and the outcome data can be noisy. In contrast, our method first segments the graphical image with a grid and drops a portion of the patches (punch holes). Then, we iteratively ask the labeler to validate each annotation with holes, narrowing down the annotation only having the most important area. This approach aims to reduce annotation noise in crowdsourcing by standardizing the annotations while enhancing labeling efficiency and reliability. Preliminary findings from fundamental charts demonstrate that punch-hole labeling can effectively pinpoint critical regions. This also highlights its potential for broader application in visualization research, particularly in studying large-scale users\u201a\u00c4\u00f4 graphical perception. Our future work aims to enhance the algorithm to achieve faster labeling speed and prove its utility through large-scale experiments.","author_affiliations":["Seoul National University, Seoul, Korea, Republic of|Seoul National University, Seoul, Korea, Republic of|Seoul National University, Seoul, Korea, Republic of|Seoul National University, Seoul, Korea, Republic of|Seoul National University, Seoul, Korea, Republic of|Georgia Tech, Atlanta, United States|Seoul National University, Seoul, Korea, Republic of"],"authors":["Minsuk Chang","Soohyun Lee","Aeri Cho","Hyeon Jeon","Seokhyeon Park","Cindy Xiong Bearfield","Jinwook Seo"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"jangsus1@snu.ac.kr","presenting_author_name":"Minsuk Chang","title":"Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation","uid":"v-vis-posters-1105"},"v-vis-posters-1106":{"abstract":"Investigating program optimizations requires navigation of the thousands of binary code lines and comparing them to their generating source code. Existing tools either focus on the understanding of assembly instructions order or the structure of the control flow. They usually focus on data from a single source file, which is insufficient to analyze large complex binaries. We have designed a new layout to balance the instruction order control flow of the binary code, visualize the loop structure, and handle large complex programs. This new layout is part of the tool DisViz, which correlates source code and assembly instructions. It is also designed for large and more complex multi-source binaries, making the navigation process easier. Our poster presents the system design and layout of DisViz and demonstrates its advantage for large-scale binary program analysis.","author_affiliations":["University of Utah, Salt Lake City, United States|Lawrence Livermore National Laboratory, Livermore, United States|The University of Utah, Salt Lake City, United States"],"authors":["Shadmaan Hye","Matthew Legendre","Katherine E. Isaacs"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"praptishadmaan@gmail.com","presenting_author_name":"Shadmaan Hye","title":"Balancing Code Order and Loop Structure in a Control Flow Layout","uid":"v-vis-posters-1106"},"v-vis-posters-1107":{"abstract":"Visualization research aims to contribute to the development and use of machine learning by developing novel visual encodings that target needs identified through interviews with expert users. While interviews with experts can help identify specific needs, they may not fully recognize more fundamental interest from broader but less expert communities. More importantly, targeted design studies have limited reach, typically only being deployed to a single target user group. We suggest that there is potential for broader impact in visualization research by reaching out to broader audiences, identifying gaps, and advocating for existing visualization research artifacts. In this work, we analyze publicly available data from machine learning tutorials and public software packages to understand how visualization is being used to explain and interpret machine learning concepts to beginners and learners. We present guidelines for visualization researchers to incorporate their research artifacts into tutorials and open source software packages to improve their impact.","author_affiliations":["Brandeis University, Waltham, United States|Brandeis University, Waltham, United States|Brandeis University, Waltham, United States"],"authors":["Ge Gao","Yuxuan Xiong","Dylan Cashman"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"dylancashman@brandeis.edu","presenting_author_name":"Dylan Cashman","title":"Meet Them Where They Are: An Analysis of Visualization Use in Machine Learning Tutorials and Software Libraries","uid":"v-vis-posters-1107"},"v-vis-posters-1108":{"abstract":"New consumer devices with refreshable tactile displays promise to allow visually impaired people to analyze data through tactile representations of graphical visualizations. To understand whether results based on visual perception translate to tactile perception, we present a study replicating the formative study by Cleveland and McGill (1984) on graphical perception to tactile representations suitable for visually impaired users. To assess how tactile graphics can convey complex graphical information, we investigate the effectiveness of tactile data visualizations compared to reported results on visual graphical primitives, examining the accuracy and inference times of visually impaired versus sighted users. We find that visually impaired users interpret simpler tactile formats such as bar charts with significantly greater accuracy and speed than more complex formats like bubble charts.","author_affiliations":["Brandeis University, Waltham, United States|Worcester Polytechnic Institute, Worcester, United States|Boston College, Chestnut Hill, United States|Brandeis University, Waltham, United States"],"authors":["Areen Khalaila","Lane Harrison","Nam Wook Kim","Dylan Cashman"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"areenkh@brandeis.edu","presenting_author_name":"Areen Khalaila","title":"A replication of visual perception studies with tactile representations of data for visually impaired users","uid":"v-vis-posters-1108"},"v-vis-posters-1110":{"abstract":"Some charts are shown with a hidden non-zero vertical axis. Prior measures of this distortion, such as GDI or RGD, calculate the distortion of an individual bar. This measure is hard to interpret, and can vary significantly based on which bar is selected. This poster proposes a new approach to calculate the missing axis. The measure is tested on a large sample of distorted charts from yearly reports issued by public companies. The measure is less sensitive to bar order, and is less skewed than prior measures.","author_affiliations":["West Virginia University, Morgantown, United States"],"authors":["Nathan Garrett"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"profgarrett@gmail.com","presenting_author_name":"Nathan Garrett","title":"Assessing Chart Distortion with the Missing Axis Measure","uid":"v-vis-posters-1110"},"v-vis-posters-1111":{"abstract":"When studying human perceptions of visualizations, it is important to capture the extent to which a visualization is deemed complex or difficult to parse. To do this, researchers can measure a visualization\u201a\u00c4\u00f4s visual complexity. However, the extent to which ratings of complexity vary depending on the design of a visualization has not yet been studied. We offer an initial exploration into this space by investigating how the chart types used in static visualizations are associated with variations in human ratings of visual complexity. We utilized the MASSVIS dataset of static visualizations, labeling each visualization by the chart types it contains and eliciting visual complexity ratings for each visualization. We find that the spread of visual complexity ratings for a given visualization looks different across chart types \u201a\u00c4\u00ec for example, Grid & Matrix, Line, and Bar charts were most represented in visualizations with less varied ratings, and Text and Circle charts were most represented in visualizations with more varied ratings. These findings suggest that some chart types may elicit a variety of reactions from different audiences when it comes to perceiving visual complexity while others may elicit more consistent perceptions.","author_affiliations":["Georgia Institute of Technology, Atlanta, United States|Georgia Institute of Technology, Atlanta, United States|Northwestern University, Evanston, United States|University of Massachusetts, Amherst , Amherst, United States|Georgia Tech, Atlanta, United States"],"authors":["Sean Sheng-tse Ru","Kylie R. Lin","David N. Rapp","Hui Guan","Cindy Xiong Bearfield"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"sru3@gatech.edu","presenting_author_name":"Sean Sheng-tse Ru","title":"Charting Complexity: How Chart Types Relate to Visual Complexity","uid":"v-vis-posters-1111"},"v-vis-posters-1112":{"abstract":"Designing visualizations is very often an iterative process that involves exploration of different visual encodings. While there have been many studies of the design process in different contexts, the lower-level details of code-heavy visualization workflows have been harder to capture. Using exploratory notebooks and higher-level frameworks that facilitate rapid iteration, users can quickly test ideas and examine results. We use publicly-available notebook version histories to study how users work in these environments, observing both how they build new visualizations from existing templates or previous work, and how they refine and enhance visualizations over time. In addition, we study differences between the Observable Plot and Vega Lite visualization frameworks, and compare those code-oriented frameworks with the Observable Chart Cell wizard. Our analysis provides insights into how users build off of existing work including templates, and provides some clues about the use of different visualization properties and encodings, including those that may require more exploration.","author_affiliations":["Northern Illinois University, DeKalb, United States|Northern Illinois University , Dekalb , United States|University of Massachusetts Dartmouth, Dartmouth, United States|Northern Illinois University, DeKalb, United States"],"authors":["Colin Brown","Hamed Alhoori","Maoyuan Sun","David Koop"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"colinjbrown@niu.edu","presenting_author_name":"Colin Brown","title":"Extracting Visualization Workflows from Versioned Notebooks","uid":"v-vis-posters-1112"},"v-vis-posters-1113":{"abstract":"In this poster, we present a work-in-progress application that integrates image processing, composite visual representation, and coordinated interactions into a visualization pipeline to support motion analysis for camouflage detection in captured videos. The application uses the MAE and SSIM image metrics to calculate differences in frames that are visually encoded as foreground and background pixels and/or marks in the composite view. We applied this approach to videos featuring camouflaged wildlife to assess its potential for camouflage object detection (COD), as a step towards developing effective visual motion analysis for camouflage detection. The source code and application are available on GitHub at https://github.com/enjelika/TemporalMotionExtractionAnalysis.","author_affiliations":["University of Oklahoma, Norman, United States|University of Oklahoma, Norman, United States|University of Oklahoma, Norman, United States"],"authors":["Debra L Hogue","David Shane Elliott","Chris Weaver"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"debra.hogue1981@gmail.com","presenting_author_name":"Debra L Hogue","title":"Visual Analysis of Motion for Camouflaged Object Detection","uid":"v-vis-posters-1113"},"v-vis-posters-1116":{"abstract":"The FAIR data principles advocate for making scientific and research datasets findable. Yet, the sheer volume and diversity of these datasets present significant challenges. Despite advancements in data search technologies, techniques for representing search results are still traditional and inadequate, often returning extraneous results. To address these issues, we developed a graph-based visual search application designed to enhance data search for Earth System Scientists. This application utilizes various chart widgets and a knowledge graph at the backend, connecting two disparate data repositories.","author_affiliations":["Institute for Software Technologies, German Aerospace Center (DLR), Braunschweig, Germany|German Aerospace Center (DLR), Cologne, Germany|German Aerospace Center (DLR), Cologne, Germany|German Aerospace Center (DLR), Braunschweig, Germany&University of Bremen, Bremen, Germany"],"authors":["Pawandeep Kaur Betz","Tobias Hecking","Andreas Schreiber","Andreas Gerndt"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"pawandeep.kaur-betz@dlr.de","presenting_author_name":"Pawandeep Kaur Betz","title":"Knowledge Graph Based Visual Search Application","uid":"v-vis-posters-1116"},"v-vis-posters-1118":{"abstract":"We present VIRUS, a visualization system to keep records of questionable papers being investigated and their impact on several scholarly measures. This effort, based on an identified set of potentially problematic papers, is meant to help researchers explore what interactive visualization systems may be needed in the future. As the number of retracted papers continues to soar, we believe that such systems will be essential for sleuthing efforts. We thus describe a prototype that we aim to iteratively evaluate with academic sleuths for improvement. All materials (poster, code, follow-up) for this submission are available at https://osf.io/83v2a/.","author_affiliations":["None, Essaouira, Morocco|Link\u00f6ping University, Norrk\u00f6ping, Sweden"],"authors":["Fabrice FRANK","Lonni Besan\u00e7on"],"discord_channel":"","event":"VIS Posters","event_prefix":"v-vis-posters","has_image":false,"has_poster_pdf":false,"has_summary_pdf":false,"presenting_author_email":"lonni.besancon@gmail.com","presenting_author_name":"Lonni Besan\u00e7on","title":"VIRUS: Visualization of Irregular Research Under Scrutiny","uid":"v-vis-posters-1118"}}
