<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Perception"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Perception"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Perception</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Perception</li></ol></nav><h1 class="session-title">VIS Full Papers: Perception</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Lace Padilla </h3><h3 class="session-room mt-4"> Room: Room 1.14 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T13:00:00+00:00 &ndash; 2025-11-06T14:15:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T13:00:00+00:00 &ndash; 2025-11-06T14:15:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full25.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945285051519147" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1260&#39;, &#39;session_id&#39;: &#39;full25&#39;, &#39;title&#39;: &#39;Eye of the Beholder: Towards Measuring Visualization Complexity&#39;, &#39;contributors&#39;: [&#39;Johannes Ellemose&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Johannes Ellemose&#39;, &#39;email&#39;: &#39;johannes@ellemose.eu&#39;, &#39;affiliation&#39;: &#39;Aarhus University&#39;}, {&#39;name&#39;: &#39;Niklas Elmqvist&#39;, &#39;email&#39;: &#39;elm@cs.au.dk&#39;, &#39;affiliation&#39;: &#39;Aarhus University&#39;}], &#39;abstract&#39;: &#39;Constructing expressive and legible visualizations is a key activity for visualization designers.While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/.&#39;, &#39;uid&#39;: &#39;a610d354-3cae-4df3-ac69-db5a5a7572a6&#39;, &#39;keywords&#39;: [&#39;Visualization complexity&#39;, &#39;visualization literacy&#39;, &#39;perception&#39;, &#39;crowdsourcing&#39;, &#39;LLMs.&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/w85a4/&#39;} <h3 class="session-list-title"><a href="paper_a610d354-3cae-4df3-ac69-db5a5a7572a6.html"> Eye of the Beholder: Towards Measuring Visualization Complexity <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Johannes Ellemose, Niklas Elmqvist </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Johannes Ellemose </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:00:00.000Z &ndash; 2025-11-06T13:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1497&#39;, &#39;session_id&#39;: &#39;full25&#39;, &#39;title&#39;: &#39;Beyond Log Scales: Toward Cognitively Informed Bar Charts for Orders of Magnitude Values&#39;, &#39;contributors&#39;: [&#39;Katerina Batziakoudi&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:12:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:12:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Katerina Batziakoudi&#39;, &#39;email&#39;: &#39;kbatziakoudi@gmail.com&#39;, &#39;affiliation&#39;: &#39;Berger-Levrault&#39;}, {&#39;name&#39;: &#39;Stéphanie Rey&#39;, &#39;email&#39;: &#39;reystef@gmail.com&#39;, &#39;affiliation&#39;: &#39;Berger-Levrault&#39;}, {&#39;name&#39;: &#39;Jean-Daniel Fekete&#39;, &#39;email&#39;: &#39;jean-daniel.fekete@inria.fr&#39;, &#39;affiliation&#39;: &#39;Université Paris-Saclay, CNRS, Inria, LISN&#39;}], &#39;abstract&#39;: &#39;In this work, we challenge the dominant use of logarithmic scales to communicate values spanning multiple orders of magnitude—Orders of Magnitude Values (OMVs)—to the general public. Focusing on bar charts, we incorporate cognitive insights into visualization design to better align with how humans perceive OMVs. Studies in cognitive psychology suggest that, for large numerical ranges such as millions and billions, people do not think logarithmically. Instead, they perceive numbers in a piecewise linear manner, grouping values into scale words (e.g., millions) and applying linear reasoning within each group. We build upon a recently introduced piecewise linear scale, EplusM, and validate its use in bar charts, which we refer to as EplusM bar charts. We also introduce two novel variants of the EplusM bar chart informed by findings in numerical perception: Bricks, which builds on the concepts of round numbers and subitizing, and Multi-Magnitude, which leverages categorical perception of large numbers. In a crowdsourced experiment, we evaluate four bar chart designs: 1) Log, 2) EplusM, 3) Bricks, and 4) Multi-Magnitude, across value retrieval and quantitative comparison tasks. Our results show that EplusM bar charts are significantly preferred over logarithmic designs, increase user confidence, and reduce perceived mental demand, while maintaining task performance. These findings suggest that EplusM bar charts can serve as effective alternatives to logarithmic ones when visualizing OMVs for general audiences.&#39;, &#39;uid&#39;: &#39;80f2d2e8-877c-469a-8ce2-bb3e5bea1f93&#39;, &#39;keywords&#39;: [&#39;orders of magnitude&#39;, &#39;comparisons&#39;, &#39;exponent&#39;, &#39;mantissa&#39;, &#39;logarithmic scale&#39;, &#39;bar charts&#39;], &#39;preprint_link&#39;: &#39;https://hal.science/hal-05171203&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/ hybvp/?view_only=5cd17943e9ba46deb66a8f7f4eeeb4da&#39;} <h3 class="session-list-title"><a href="paper_80f2d2e8-877c-469a-8ce2-bb3e5bea1f93.html"> Beyond Log Scales: Toward Cognitively Informed Bar Charts for Orders of Magnitude Values <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Katerina Batziakoudi, Stéphanie Rey, Jean-Daniel Fekete </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Katerina Batziakoudi </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:12:00.000Z &ndash; 2025-11-06T13:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1566&#39;, &#39;session_id&#39;: &#39;full25&#39;, &#39;title&#39;: &#39;Characterizing Visualization Perception with Psychological Phenomena: Uncovering the Role of Subitizing in Data Visualization&#39;, &#39;contributors&#39;: [&#39;Arran Zeyu Wang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:36:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Arran Zeyu Wang&#39;, &#39;email&#39;: &#39;zeyuwang@cs.unc.edu&#39;, &#39;affiliation&#39;: &#39;University of North Carolina-Chapel Hill&#39;}, {&#39;name&#39;: &#39;Ghulam Jilani Quadri&#39;, &#39;email&#39;: &#39;quadri@ou.edu&#39;, &#39;affiliation&#39;: &#39;University of Oklahoma&#39;}, {&#39;name&#39;: &#39;Mengyuan Zhu&#39;, &#39;email&#39;: &#39;gisellez@ad.unc.edu&#39;, &#39;affiliation&#39;: &#39;The University of North Carolina at Chapel Hill&#39;}, {&#39;name&#39;: &#39;Chin Tseng&#39;, &#39;email&#39;: &#39;chint@cs.unc.edu&#39;, &#39;affiliation&#39;: &#39;University of North Carolina-Chapel Hill&#39;}, {&#39;name&#39;: &#39;Danielle Szafir&#39;, &#39;email&#39;: &#39;danielle.szafir@cs.unc.edu&#39;, &#39;affiliation&#39;: &#39;University of North Carolina-Chapel Hill&#39;}], &#39;abstract&#39;: &#39;Understanding how people perceive visualizations is crucial for designing effective visual data representations; however, many heuristic design guidelines are derived from specific tasks or visualization types, without considering the constraints or conditions under which those guidelines hold. In this work, we aimed to assess existing design heuristics for categorical visualization using well-established psychological knowledge. Specifically, we examine the impact of the subitizing phenomenon in cognitive psychology—people’s ability to automatically recognize a small set of objects instantly without counting—in data visualizations. We conducted three experiments with multi-class scatterplots—between 2 and 15 classes with varying design choices—across three different tasks—class estimation, correlation comparison, and clustering judgments—to understand how performance changes as the number of classes (and therefore set size) increases. Our results indicate if the category number is smaller than six, people tend to perform well at all tasks, providing empirical evidence of subitizing in visualization. When category numbers increased, performance fell, with the magnitude of the performance change depending on task and encoding. Our study bridges the gap between heuristic guidelines and empirical evidence by applying well-established psychological theories, suggesting future opportunities for using psychological theories and constructs to characterize visualization perception.&#39;, &#39;uid&#39;: &#39;8aa31c36-374d-4155-b9c3-fd20d8deeec2&#39;, &#39;keywords&#39;: [&#39;Visualization Perception&#39;, &#39;Psychology&#39;, &#39;Subitizing&#39;, &#39;Fechner’s Law&#39;, &#39;Dual-System Theory&#39;, &#39;Categorical Data&#39;, &#39;Color&#39;, &#39;Shape&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/y3z2b/?view_only=7f6569187b344fadbd11cc09a6e63d24&#39;} <h3 class="session-list-title"><a href="paper_8aa31c36-374d-4155-b9c3-fd20d8deeec2.html"> Characterizing Visualization Perception with Psychological Phenomena: Uncovering the Role of Subitizing in Data Visualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Arran Zeyu Wang, Ghulam Jilani Quadri, Mengyuan Zhu, Chin Tseng, Danielle Szafir </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Arran Zeyu Wang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:24:00.000Z &ndash; 2025-11-06T13:36:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-09-0882&#39;, &#39;session_id&#39;: &#39;full25&#39;, &#39;title&#39;: &#39;Color-Name Aware Optimization to Enhance the Perception of Transparent Overlapped Charts&#39;, &#39;contributors&#39;: [&#39;Kecheng Lu&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:36:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:36:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T13:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Kecheng Lu&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Lihang Zhu&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yunhai Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Qiong Zeng&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Weitao Song&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Khairi Reda&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;Transparency is commonly utilized in visualizations to overlay color-coded histograms or sets, thereby facilitating the visual comparison of categorical data. However, these charts often suffer from significant overlap between objects, resulting in substantial color interactions. Existing color blending models struggle in these scenarios, frequently leading to ambiguous color mappings and the introduction of false colors. To address these challenges, we propose an automated approach for generating optimal color encodings to enhance the perception of translucent charts. Our method harnesses color nameability to maximize the association between composite colors and their respective class labels. We introduce a color-name aware (CNA) optimization framework that generates maximally coherent color assignments and transparency settings while ensuring perceptual discriminability for all segments in the visualization. We demonstrate the effectiveness of our technique through crowdsourced experiments with composite histograms, showing how our technique can significantly outperform both standard and visualization-specific color blending models. Furthermore, we illustrate how our approach can be generalized to other visualizations, including parallel coordinates and Venn diagrams. We provide an open-source implementation of our technique as a web-based tool.&#39;, &#39;uid&#39;: &#39;349234b8-5d8e-40a9-87db-36db8f367bb3&#39;, &#39;keywords&#39;: [&#39;Image color analysis&#39;, &#39;Optimization&#39;, &#39;Histograms&#39;, &#39;Data visualization&#39;, &#39;Visualization&#39;, &#39;Standards&#39;, &#39;Semantics&#39;, &#39;Rendering (computer graphics)&#39;, &#39;Data models&#39;, &#39;Color&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2412.16242&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3520219&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;By reducing false color artifacts and improving the whole-from-parts perception, the method improves user accuracy in tasks like identifying distributions, estimating categories, or reading overlaps.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_349234b8-5d8e-40a9-87db-36db8f367bb3.html"> Color-Name Aware Optimization to Enhance the Perception of Transparent Overlapped Charts <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Kecheng Lu, Lihang Zhu, Yunhai Wang, Qiong Zeng, Weitao Song, Khairi Reda </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Kecheng Lu </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:36:00.000Z &ndash; 2025-11-06T13:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2023-05-0258&#39;, &#39;session_id&#39;: &#39;full25&#39;, &#39;title&#39;: &#39;Generalization of CNNs on Relational Reasoning With Bar Charts&#39;, &#39;contributors&#39;: [&#39;Yong Wang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T13:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T13:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T14:00:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Zhenxing Cui&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Lu Chen&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yunhai Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Daniel Haehn&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yong Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Hanspeter Pfister&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;This article presents a systematic study of the generalization of convolutional neural networks (CNNs) and humans on relational reasoning tasks with bar charts. We first revisit previous experiments on graphical perception and update the benchmark performance of CNNs. We then test the generalization performance of CNNs on a classic relational reasoning task: estimating bar length ratios in a bar chart, by progressively perturbing the standard visualizations. We further conduct a user study to compare the performance of CNNs and humans. Our results show that CNNs outperform humans only when the training and test data have the same visual encodings. Otherwise, they may perform worse. We also find that CNNs are sensitive to perturbations in various visual encodings, regardless of their relevance to the target bars. Yet, humans are mainly influenced by bar lengths. Our study suggests that robust relational reasoning with visualizations is challenging for CNNs. Improving CNNs’ generalization performance may require training them to better recognize task-related visual properties.&#39;, &#39;uid&#39;: &#39;2c73a979-fd41-4019-98ea-12a5581a53f1&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;Bars&#39;, &#39;Visualization&#39;, &#39;Cognition&#39;, &#39;Standards&#39;, &#39;Encoding&#39;, &#39;Training&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2503.00086&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3463800&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;This work is notable for systematically evaluating convolutional neural networks’ ability to perform relational reasoning on bar charts, a fundamental but underexplored task in visual understanding. It introduces carefully designed synthetic datasets and benchmarks that rigorously test model generalization beyond training distributions, providing valuable insights into current model limitations and directions for improvement.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_2c73a979-fd41-4019-98ea-12a5581a53f1.html"> Generalization of CNNs on Relational Reasoning With Bar Charts <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Zhenxing Cui, Lu Chen, Yunhai Wang, Daniel Haehn, Yong Wang, Hanspeter Pfister </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yong Wang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T13:48:00.000Z &ndash; 2025-11-06T14:00:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-08-0683&#39;, &#39;session_id&#39;: &#39;full25&#39;, &#39;title&#39;: &#39;Visualization-Driven Illumination for Density Plots&#39;, &#39;contributors&#39;: [&#39;Xin Chen&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T14:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T14:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T14:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Xin Chen&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yunhai Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Huaiwei Bao&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Kecheng Lu&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Jaemin Jo&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Chi-Wing Fu&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Jean-Daniel Fekete&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#34;We present a novel visualization-driven illumination model for density plots, a new technique to enhance density plots by effectively revealing the detailed structures in high- and medium-density regions and outliers in low-density regions, while avoiding artifacts in the density field&#39;s colors. When visualizing large and dense discrete point samples, scatterplots and dot density maps often suffer from overplotting, and density plots are commonly employed to provide aggregated views while revealing underlying structures. Yet, in such density plots, existing illumination models may produce color distortion and hide details in low-density regions, making it challenging to look up density values, compare them, and find outliers. The key novelty in this work includes (i) a visualization-driven illumination model that inherently supports density-plot-specific analysis tasks and (ii) a new image composition technique to reduce the interference between the image shading and the color-encoded density values. To demonstrate the effectiveness of our technique, we conducted a quantitative study, an empirical evaluation of our technique in a controlled study, and two case studies, exploring twelve datasets with up to two million data point samples.&#34;, &#39;uid&#39;: &#39;67fc7baa-0f81-46bb-b2ca-85e82da220be&#39;, &#39;keywords&#39;: [&#39;Image color analysis&#39;, &#39;Lighting&#39;, &#39;Data visualization&#39;, &#39;Visualization&#39;, &#39;Distortion&#39;, &#39;Kernel&#39;, &#39;Estimation&#39;, &#39;Computational modeling&#39;, &#39;Analytical models&#39;, &#39;Shape&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.17265&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3495695&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_67fc7baa-0f81-46bb-b2ca-85e82da220be.html"> Visualization-Driven Illumination for Density Plots <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Xin Chen, Yunhai Wang, Huaiwei Bao, Kecheng Lu, Jaemin Jo, Chi-Wing Fu, Jean-Daniel Fekete </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Xin Chen </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T14:00:00.000Z &ndash; 2025-11-06T14:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T13:00:00+00:00'
    endTime = '2025-11-06T14:15:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "1_14-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>