<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2024/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/1.12/auth0-spa-js.production.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2024/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2024/js/data/persistor.js"></script><script src="/static/2024/js/data/api.js"></script><link rel="shortcut icon" href="/static/2024/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2024/css/Zilla.css" rel="stylesheet"><link href="/static/2024/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2024/css/main.css"><link rel="stylesheet" href="/static/2024/css/fa_solid.css"><link rel="stylesheet" href="/static/2024/css/lazy_load.css"><link rel="stylesheet" href="/static/2024/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2024 - Paper: Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts"><meta name="twitter:description" content="Machine Learning models for chart-grounded Q&A (CQA) often treat charts as images, but performing CQA on pixel values has proven challenging. We thus investigate a resource overlooked by current ML-based approaches: the declarative documents describing how charts should visually encode data (i.e., chart specifications). In this work, we use chart specifications to enhance language models (LMs) for chart-reading tasks, such that the resulting system can robustly understand language for CQA. Through a case study with 359 bar charts, we test novel fine tuning schemes on both GPT-3 and T5 using a new dataset curated for two CQA tasks: question-answering and visual explanation generation. Our text-only approaches strongly outperform vision-based GPT-4 on explanation generation (99% vs. 63% accuracy), and show promising results for question-answering (57-67% accuracy). Through in-depth experiments, we also show that our text-only approaches are mostly robust to natural language variation."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2024/paper_images/v-short-1276_Image.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2024/paper_images/v-short-1276_Image.png"><meta name="description" property="og:description" content="Machine Learning models for chart-grounded Q&A (CQA) often treat charts as images, but performing CQA on pixel values has proven challenging. We thus investigate a resource overlooked by current ML-based approaches: the declarative documents describing how charts should visually encode data (i.e., chart specifications). In this work, we use chart specifications to enhance language models (LMs) for chart-reading tasks, such that the resulting system can robustly understand language for CQA. Through a case study with 359 bar charts, we test novel fine tuning schemes on both GPT-3 and T5 using a new dataset curated for two CQA tasks: question-answering and visual explanation generation. Our text-only approaches strongly outperform vision-based GPT-4 on explanation generation (99% vs. 63% accuracy), and show promising results for question-answering (57-67% accuracy). Through in-depth experiments, we also show that our text-only approaches are mostly robust to natural language variation."><meta name="title" property="og:title" content="Virtual IEEE VIS 2024 - Paper: Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts"><meta property="og:type" content="website"><title>IEEE VIS 2024 Content: Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-12"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="event_v-short.html">VIS Short Papers</a></li><li class="breadcrumb-item"><a href="session_short7.html">Short Papers: Text and Multimedia</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts</li></ol></nav><h1 class="paper-title">Representing Charts as Text for Language Models: An In-Depth Study of Question Answering for Bar Charts</h1><div class="checkbox-bookmark fas" style="font-size: 24pt;position: absolute; top:10px; right:20px;" data-tippy-content="(un-)bookmark this paper"> &#xf02e; </div><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span> Victor S. Bursztyn - Adobe Research, San Jose, United States </h4><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span><a href="mailto:jhoffs@adobe.com">Jane Hoffswell</a> - Adobe Research, Seattle, United States </h4><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span> Shunan Guo - Adobe Research, San Jose, United States </h4><h4 class="paper-authors pb-2 mt-2"><span class="fas mr-1">&#xf183;</span> Eunyee Koh - Adobe Research, San Jose, United States </h4><h5><span class="fas" title="The authors made this paper screen-reader accessible in the IEEE Digital Library.">&#xf29a;</span> Screen-reader Accessible PDF </h5><h5 class="paper-link pb-2"><a href="https://github.com/vbursztyn/charts-as-text-for-chartqa"><span class="fas mr-1" title="This paper has additional material, like demos or experimental data, available online.">&#xf0c6;</span> Download Supplemental Material </a></h5><h3 class="session-room mt-4"><span class="fas mr-1">&#xf108;</span><a href="room_bayshore6.html"> Room: Bayshore VI </a></h3><h5 class="paper-presentation pb-2"><span class="format-date">2024-10-17T14:42:00Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2024-10-17T14:42:00Z</span><span class="current-time tztooltiptext"></span></span></h5></div></div><div class="row my-3"><div class="col-md-8"><figure class="figure"><img class="figure-img img-fluid" src="https://ieeevis.b-cdn.net/vis_2024/paper_images/v-short-1276_Image.png" alt="Exemplar figure, described by caption below" aria-describedby="figure-caption"><figcaption class="figure-caption" id="figure-caption">We explore two main tasks related to chart-grounded Q&amp;A: question answering (QA) and visual explanation generation (VEG). QA leverages templated domain facts (DF) from the chart&#39;s CSV file, whereas VEG relies on visual context (VC) from its JSON file. In the first fine-tuning step, the charts&#39; underlying text files are injected into the language models (LMs). We then fine-tune the QA and VEG steps on 90% of the charts, with 10% held out for testing during our evaluation in ยง4. To understand the robustness of our LMs to natural language variation, we also perform a question paraphrasing task to rephrase our template-generated questions more naturally.</figcaption></figure></div></div><div class="row my-3"><div class="col-md-8"></div></div><div class="row my-3"><div class="col-md-8"><h5 class="paper-details-heading">Fast forward</h5><iframe width="730" height="410" src="https://www.youtube-nocookie.com/embed/m9owYC9e3PU?rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div><div class="row my-3"><div class="col-md-8"><h5 class="paper-details-heading">Keywords</h5><p>Machine Learning Techniques; Charts, Diagrams, and Plots; Datasets; Computational Benchmark Studies</p><h5 class="paper-details-heading">Abstract</h5><p>Machine Learning models for chart-grounded Q&amp;A (CQA) often treat charts as images, but performing CQA on pixel values has proven challenging. We thus investigate a resource overlooked by current ML-based approaches: the declarative documents describing how charts should visually encode data (i.e., chart specifications). In this work, we use chart specifications to enhance language models (LMs) for chart-reading tasks, such that the resulting system can robustly understand language for CQA. Through a case study with 359 bar charts, we test novel fine tuning schemes on both GPT-3 and T5 using a new dataset curated for two CQA tasks: question-answering and visual explanation generation. Our text-only approaches strongly outperform vision-based GPT-4 on explanation generation (99% vs. 63% accuracy), and show promising results for question-answering (57-67% accuracy). Through in-depth experiments, we also show that our text-only approaches are mostly robust to natural language variation.</p></div></div><script lang="js">
      const paperID = "v-short-1276"
      $(document).ready(() => {
        tippy('[data-tippy-content]');

        const allBookmarks =
          d3.selectAll('.checkbox-bookmark')
            .on("click", function () {
              const newValue = !d3.select(this).classed('selected');
              API.markSet(API.storeIDs.bookmarked, paperID, newValue);
              d3.select(this).classed('selected', newValue);
            })
        API.markGet(API.storeIDs.bookmarked, paperID).then(is_bookmarked => {
          is_bookmarked = !!is_bookmarked;
          allBookmarks.classed('selected', is_bookmarked);
        })
        API.markSet(API.storeIDs.visited, paperID, true);

      })

    </script><script src="/static/2024/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" โ ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script></body></html>