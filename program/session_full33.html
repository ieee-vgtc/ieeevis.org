<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Virtual Session: Encoding & Comprehension"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Virtual Session: Encoding & Comprehension"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Virtual Session: Encoding &amp; Comprehension</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Virtual Session: Encoding &amp; Comprehension</li></ol></nav><h1 class="session-title">VIS Full Papers: Virtual Session: Encoding &amp; Comprehension</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Alex Kale </h3><h3 class="session-room mt-4"> Room: Room 0.94 + 0.95 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-05T08:30:00+00:00 &ndash; 2025-11-05T09:45:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-05T08:30:00+00:00 &ndash; 2025-11-05T09:45:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full33.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1495&#39;, &#39;session_id&#39;: &#39;full33&#39;, &#39;title&#39;: &#39;Evaluating judgements of spatial correlation in visual displays of scalar field distributions&#39;, &#39;contributors&#39;: [&#39;Yayan Zhao&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T08:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T08:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T08:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yayan Zhao&#39;, &#39;email&#39;: &#39;yayan.zhao@vanderbilt.edu&#39;, &#39;affiliation&#39;: &#39;Vanderbilt University&#39;}, {&#39;name&#39;: &#39;Matthew Berger&#39;, &#39;email&#39;: &#39;matthew.berger@vanderbilt.edu&#39;, &#39;affiliation&#39;: &#39;Vanderbilt University&#39;}], &#39;abstract&#39;: &#39;In this work we study the identification of spatial correlation in distributions of 2D scalar fields, presented across different forms of visual displays. We study simple visual displays that directly show color-mapped scalar fields, namely those drawn from a distribution, and whether humans can identify strongly correlated spatial regions in these displays. In this setting, the recognition of correlation requires making judgments on a set of fields, rather than just one field. Thus, in our experimental design we compare two basic visualization designs: animation-based displays against juxtaposed views of scalar fields, along different choices of color scales. Moreover, we investigate the impacts of the distribution itself, controlling for the level of spatial correlation and discriminability in spatial\nscales. Our study’s results illustrate the impacts of these distribution characteristics, while also highlighting how different visual displays impact the types of judgments made in assessing spatial correlation. Supplemental material is available at https://osf.io/zn4qy/&#39;, &#39;uid&#39;: &#39;14e8969b-dbaf-41b3-8330-a0decc6b53b0&#39;, &#39;keywords&#39;: [&#39;Evaluation&#39;, &#39;spatial correlation&#39;, &#39;color maps&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.17997&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/zn4qy&#39;} <h3 class="session-list-title"><a href="paper_14e8969b-dbaf-41b3-8330-a0decc6b53b0.html"> Evaluating judgements of spatial correlation in visual displays of scalar field distributions <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yayan Zhao, Matthew Berger </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Yayan Zhao </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T08:30:00.000Z &ndash; 2025-11-05T08:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1514&#39;, &#39;session_id&#39;: &#39;full33&#39;, &#39;title&#39;: &#39;Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals&#39;, &#39;contributors&#39;: [&#39;Tingying He&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T08:42:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T08:42:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T08:54:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Tingying He&#39;, &#39;email&#39;: &#39;hetingying.hty@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Maggie McCracken&#39;, &#39;email&#39;: &#39;maggie.mccracken@psych.utah.edu&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Daniel Hajas&#39;, &#39;email&#39;: &#39;d.hajas@ucl.ac.uk&#39;, &#39;affiliation&#39;: &#39;University College London&#39;}, {&#39;name&#39;: &#39;Sarah Creem-Regehr&#39;, &#39;email&#39;: &#39;sarah.creem@psych.utah.edu&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Alexander Lex&#39;, &#39;email&#39;: &#39;alexander.lex@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}], &#39;abstract&#39;: &#34;We investigate whether tactile charts support comprehension and learning of complex visualizations for blind and low-vision (BLV) individuals and contribute four tactile chart designs and an interview study. Visualizations are powerful tools for conveying data, yet BLV individuals typically can rely only on assistive technologies—primarily alternative texts—to access this information. Prior research shows the importance of mental models of chart types for interpreting these descriptions, yet BLV individuals have no means to build such a mental model based on images of visualizations. Tactile charts show promise to fill this gap in supporting the process of building mental models. Yet studies on tactile data representations mostly focus on simple chart types, and it is unclear whether they are also appropriate for more complex charts as would be found in scientific publications. Working with two BLV researchers, we designed 3D-printed tactile template charts with exploration instructions for four advanced chart types: UpSet plots, violin plots, clustered heatmaps, and faceted line charts. We then conducted an interview study with 12 BLV participants comparing whether using our tactile templates improves mental models and understanding of charts and whether this understanding translates to novel datasets experienced through alt texts. Thematic analysis shows that tactile models support chart type understanding and are the preferred learning method by BLV individuals. We also report participants&#39; opinions on tactile chart design and their role in BLV education.&#34;, &#39;uid&#39;: &#39;055a075f-b15e-43ec-becb-cc09ad944d2c&#39;, &#39;keywords&#39;: [&#39;Accessibility&#39;, &#39;tactile representations&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.21462&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;We provide an extensive, well-documented 40-page appendix that includes materials and detailed descriptions of the full design and user study processes.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/9dwgq/&#39;} <h3 class="session-list-title"><a href="paper_055a075f-b15e-43ec-becb-cc09ad944d2c.html"> Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Tingying He, Maggie McCracken, Daniel Hajas, Sarah Creem-Regehr, Alexander Lex </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Tingying He </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T08:42:00.000Z &ndash; 2025-11-05T08:54:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1614&#39;, &#39;session_id&#39;: &#39;full33&#39;, &#39;title&#39;: &#39;Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable&#39;, &#39;contributors&#39;: [&#39;Tingying He&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T08:54:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T08:54:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:06:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Tingying He&#39;, &#39;email&#39;: &#39;hetingying.hty@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Utah&#39;}, {&#39;name&#39;: &#39;Jason Dykes&#39;, &#39;email&#39;: &#39;j.dykes@city.ac.uk&#39;, &#39;affiliation&#39;: &#39;City, University of London&#39;}, {&#39;name&#39;: &#39;Petra Isenberg&#39;, &#39;email&#39;: &#39;petra.isenberg@inria.fr&#39;, &#39;affiliation&#39;: &#39;Université Paris-Saclay, CNRS, Inria, LISN&#39;}, {&#39;name&#39;: &#39;Tobias Isenberg&#39;, &#39;email&#39;: &#39;tobias.isenberg@gmail.com&#39;, &#39;affiliation&#39;: &#39;Université Paris-Saclay, CNRS, Inria, LISN&#39;}], &#39;abstract&#39;: &#39;We present a new comprehensive theory for explaining, exploring, and using pattern as a visual variable in visualization. Although patterns have long been used for data encoding and continue to be valuable today, their conceptual foundations are precarious: the concepts and terminology used across the research literature and in practice are inconsistent, making it challenging to use patterns effectively and to conduct research to inform their use. To address this problem, we conduct a comprehensive cross-disciplinary literature review that clarifies ambiguities around the use of pattern and texture. As a result, we offer a new consistent treatment of pattern as a composite visual variable composed of structured groups of graphic primitives that can serve as marks for encoding data individually and collectively. This new and widely applicable formulation opens a sizable design space for the visual variable pattern, which we formalize as a new system comprising three sets of variables: the spatial arrangement of primitives, the appearance relationships among primitives, and the retinal visual variables that characterize individual primitives. We show how our pattern system relates to existing visualization theory and highlight opportunities for visualization design. We further explore patterns based on complex spatial arrangements, demonstrating explanatory power and connecting our conceptualization to broader theory on maps and cartography. An author version and additional materials are available on OSF: osf.io/z7ae2.&#39;, &#39;uid&#39;: &#39;625a86de-1529-4724-8869-f081c7776f88&#39;, &#39;keywords&#39;: [&#39;Pattern&#39;, &#39;texture&#39;, &#39;visual variables&#39;, &#39;retinal variables&#39;, &#39;data visualization&#39;, &#39;Jacques Bertin&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2508.02639&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;We provide a well-documented appendix that includes detailed discussions and examples to support our theory. All images we created are correctly licensed under CC BY 4.0, and the paper has been uploaded to OSF and arXiv as well as will be shared on HAL; there is no code that relates to the paper, so a replicability stamp application was not an option.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/z7ae2/&#39;} <h3 class="session-list-title"><a href="paper_625a86de-1529-4724-8869-f081c7776f88.html"> Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Tingying He, Jason Dykes, Petra Isenberg, Tobias Isenberg </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Tingying He </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T08:54:00.000Z &ndash; 2025-11-05T09:06:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1627&#39;, &#39;session_id&#39;: &#39;full33&#39;, &#39;title&#39;: &#39;Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions&#39;, &#39;contributors&#39;: [&#39;Danyang Fan&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T09:06:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T09:06:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:18:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Danyang Fan&#39;, &#39;email&#39;: &#39;danfan17@stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#39;Walker Smith&#39;, &#39;email&#39;: &#39;walksmit@stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#39;Takako Fujioka&#39;, &#39;email&#39;: &#39;takako@ccrma.stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#39;Chris Chafe&#39;, &#39;email&#39;: &#39;cc@ccrma.stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#34;Sile O&#39;Modhrain&#34;, &#39;email&#39;: &#39;sileo@umich.edu&#39;, &#39;affiliation&#39;: &#39;University of Michigan&#39;}, {&#39;name&#39;: &#39;Diana Deutsch&#39;, &#39;email&#39;: &#39;othello5@stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}, {&#39;name&#39;: &#39;Sean Follmer&#39;, &#39;email&#39;: &#39;sfollmer@stanford.edu&#39;, &#39;affiliation&#39;: &#39;Stanford University&#39;}], &#39;abstract&#39;: &#34;Sonification offers a non-visual way to understand data, with pitch-based encodings being the most common. Yet, how well people perceive slope and acceleration—key features of data trends—remains poorly understood. Drawing on people&#39;s natural abilities to perceive tempo, we introduce a novel sampling method for pitch-based sonification to enhance the perception of slope and acceleration in univariate functions. While traditional sonification methods often sample data at uniform x-spacing, yielding notes played at a fixed tempo with variable pitch intervals (Variable Pitch Interval), our approach samples at uniform y-spacing, producing notes with consistent pitch intervals but variable tempo (Variable Tempo). We conducted psychoacoustic experiments to understand slope and acceleration perception across three sampling methods: Variable Pitch Interval, Variable Tempo, and a Continuous (no sampling) baseline. In slope comparison tasks, Variable Tempo was more accurate than the other methods when modulated by the magnitude ratio between slopes. For acceleration perception, just-noticeable differences under Variable Tempo were over 13 times finer than with other methods. Participants also commonly reported higher confidence, lower mental effort, and a stronger preference for Variable Tempo compared to other methods. This work contributes models of slope and acceleration perception across pitch-based sonification techniques, introduces Variable Tempo as a novel and preferred sampling method, and provides promising initial evidence that leveraging timing can lead to more sensitive, accurate, and precise interpretation of derivative-based data features.&#34;, &#39;uid&#39;: &#39;9e3dfe2c-bbe4-4431-ae8b-1ee64a399c8b&#39;, &#39;keywords&#39;: [&#39;Visualization&#39;, &#39;Sonification&#39;, &#39;Empirical Studies&#39;, &#39;Auditory Perception&#39;], &#39;preprint_link&#39;: &#39;https://shape.stanford.edu/research/TempoSonification/Fan25PerceivingSlopeAndAccelerationVariableTempoSamplingSonification.pdf&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/a4cth/?view_only=784965f9e9f64a3ea720cf53ff241481&#39;} <h3 class="session-list-title"><a href="paper_9e3dfe2c-bbe4-4431-ae8b-1ee64a399c8b.html"> Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Danyang Fan, Walker Smith, Takako Fujioka, Chris Chafe, Sile O&#39;Modhrain, Diana Deutsch, Sean Follmer </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Danyang Fan </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T09:06:00.000Z &ndash; 2025-11-05T09:18:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1930&#39;, &#39;session_id&#39;: &#39;full33&#39;, &#39;title&#39;: &#39;TiVy: Time Series Visual Summary for Scalable Visualization&#39;, &#39;contributors&#39;: [&#39;Gromit Yeuk-Yin Chan&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T09:18:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T09:18:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:30:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Gromit Yeuk-Yin Chan&#39;, &#39;email&#39;: &#39;ychan@adobe.com&#39;, &#39;affiliation&#39;: &#39;Adobe Research&#39;}, {&#39;name&#39;: &#39;Luis Gustavo Nonato&#39;, &#39;email&#39;: &#39;gnonato@icmc.usp.br&#39;, &#39;affiliation&#39;: &#39;University of Sao Paulo&#39;}, {&#39;name&#39;: &#39;Themis Palpanas&#39;, &#39;email&#39;: &#39;themis@mi.parisdescartes.fr&#39;, &#39;affiliation&#39;: &#39;Paris Descartes University&#39;}, {&#39;name&#39;: &#39;Claudio Silva&#39;, &#39;email&#39;: &#39;csilva@nyu.edu&#39;, &#39;affiliation&#39;: &#39;New York University&#39;}, {&#39;name&#39;: &#39;Juliana Freire&#39;, &#39;email&#39;: &#39;juliana.freire@nyu.edu&#39;, &#39;affiliation&#39;: &#39;New York University&#39;}], &#39;abstract&#39;: &#39;Visualizing multiple time series presents fundamental tradeoffs between scalability and visual clarity. Time series capture the behavior of many large-scale real-world processes, from stock market trends to urban activities. Users often gain insights by visualizing them as line charts, juxtaposing or superposing multiple time series to compare them and identify trends and patterns. However, existing representations struggle with scalability: when covering long time spans, leading to visual clutter from too many small multiples or overlapping lines. We propose TiVy, a new algorithm that summarizes time series using sequential patterns. It transforms the series into a set of symbolic sequences based on subsequence visual similarity using Dynamic Time Warping (DTW), then constructs a disjoint grouping of similar subsequences based on the frequent sequential patterns. The grouping result, a visual summary of time series, provides uncluttered superposition with fewer small multiples. Unlike common clustering techniques, TiVy extracts similar subsequences (of varying lengths) aligned in time. We also present an interactive time series visualization that renders large-scale time series in real-time. Our experimental evaluation shows that our algorithm (1) extracts clear and accurate patterns when visualizing time series data, (2) achieves a significant speed-up (1000X) compared to a straightforward DTW clustering. We also demonstrate the efficiency of our approach to explore hidden structures in massive time series data in two usage scenarios.&#39;, &#39;uid&#39;: &#39;8347663a-7e88-43e2-a557-e7b2179fd4e5&#39;, &#39;keywords&#39;: [&#39;Time Series Visualization&#39;, &#39;Sub-sequence Clustering&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/pdf/2507.18972&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/GromitC/TiVy&#39;} <h3 class="session-list-title"><a href="paper_8347663a-7e88-43e2-a557-e7b2179fd4e5.html"> TiVy: Time Series Visual Summary for Scalable Visualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Gromit Yeuk-Yin Chan, Luis Gustavo Nonato, Themis Palpanas, Claudio Silva, Juliana Freire </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Gromit Yeuk-Yin Chan </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T09:18:00.000Z &ndash; 2025-11-05T09:30:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-2070&#39;, &#39;session_id&#39;: &#39;full33&#39;, &#39;title&#39;: &#39;A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime&#39;, &#39;contributors&#39;: [&#39;Shuning Jiang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T09:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T09:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Shuning Jiang&#39;, &#39;email&#39;: &#39;jiang.2126@osu.edu&#39;, &#39;affiliation&#39;: &#39;The Ohio State University&#39;}, {&#39;name&#39;: &#39;Wei-Lun Chao&#39;, &#39;email&#39;: &#39;weilunchao760414@gmail.com&#39;, &#39;affiliation&#39;: &#39;The Ohio State University&#39;}, {&#39;name&#39;: &#39;Daniel Haehn&#39;, &#39;email&#39;: &#39;daniel.haehn@umb.edu&#39;, &#39;affiliation&#39;: &#39;University of Massachusetts Boston&#39;}, {&#39;name&#39;: &#39;Hanspeter Pfister&#39;, &#39;email&#39;: &#39;pfister@seas.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard University&#39;}, {&#39;name&#39;: &#39;Jian Chen&#39;, &#39;email&#39;: &#39;chen.8028@osu.edu&#39;, &#39;affiliation&#39;: &#39;The Ohio State University&#39;}], &#39;abstract&#39;: &#39;We present a data-domain sampling regime for quantifying CNNs’ graphic perception behaviors. This regime lets us evaluate CNNs’ ratio estimation ability in bar charts from three perspectives: sensitivity to training-test distribution discrepancies, stability to limited samples, and relative expertise to human observers. After analyzing 16 million trials from 800 CNN models and 6,825 trials from 113 human participants, we arrived at a simple and actionable conclusion: CNNs can outperform humans and their biases simply depend on the training-test distance. We show evidence of this simple, elegant behavior of the machines when they interpret visualization images. osf.io/gfqc3 provides registration, the code for our sampling regime, and experimental results.&#39;, &#39;uid&#39;: &#39;72c11016-caee-44a4-82cf-1939cf5d21d3&#39;, &#39;keywords&#39;: [&#39;Quantification&#39;, &#39;convolutional neural network&#39;, &#39;sampling&#39;, &#39;graphical perception&#39;, &#39;evaluation&#39;], &#39;preprint_link&#39;: &#39;https://www.arxiv.org/abs/2507.03866&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;Our work features thoroughly documented source code and interactive Google Colab notebooks, enabling immediate, zero-setup replication of our analyses online.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/gfqc3/&#39;} <h3 class="session-list-title"><a href="paper_72c11016-caee-44a4-82cf-1939cf5d21d3.html"> A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Shuning Jiang, Wei-Lun Chao, Daniel Haehn, Hanspeter Pfister, Jian Chen </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Shuning Jiang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T09:30:00.000Z &ndash; 2025-11-05T09:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-05T08:30:00+00:00'
    endTime = '2025-11-05T09:45:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "0_94_0_95-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>