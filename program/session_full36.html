<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Volume"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Volume"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Volume</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Volume</li></ol></nav><h1 class="session-title">VIS Full Papers: Volume</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Stefan Gumhold </h3><h3 class="session-room mt-4"> Room: Hall M1 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-05T13:00:00+00:00 &ndash; 2025-11-05T14:15:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-05T13:00:00+00:00 &ndash; 2025-11-05T14:15:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full36.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945433034952759" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1455&#39;, &#39;session_id&#39;: &#39;full36&#39;, &#39;title&#39;: &#39;F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding&#39;, &#39;contributors&#39;: [&#39;Jianxin Sun&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Jianxin Sun&#39;, &#39;email&#39;: &#39;sunjianxin66@gmail.com&#39;, &#39;affiliation&#39;: &#39;University of Nebraska-Lincoln&#39;}, {&#39;name&#39;: &#39;David Lenz&#39;, &#39;email&#39;: &#39;dlenz@anl.gov&#39;, &#39;affiliation&#39;: &#39;Argonne National Laboratory&#39;}, {&#39;name&#39;: &#39;Hongfeng Yu&#39;, &#39;email&#39;: &#39;hfyu@unl.edu&#39;, &#39;affiliation&#39;: &#39;University of Nebraska-Lincoln&#39;}, {&#39;name&#39;: &#39;Tom Peterka&#39;, &#39;email&#39;: &#39;tpeterka@mcs.anl.gov&#39;, &#39;affiliation&#39;: &#39;Argonne National Laboratory&#39;}], &#39;abstract&#39;: &#39;Interactive time-varying volume visualization is challenging due to its complex spatiotemporal features and sheer size of the dataset. Recent works transform the original discrete time-varying volumetric data into continuous Implicit Neural Representations (INR) to address the issues of compression, rendering, and super-resolution in both spatial and temporal domains. However, training the INR takes a long time to converge, especially when handling large-scale time-varying volumetric datasets. In this work, we proposed F-Hash, a novel feature-based multi-resolution Tesseract encoding architecture to greatly enhance the convergence speed compared with existing input encoding methods for modeling time-varying volumetric data. The proposed design incorporates multi-level collision-free hash functions that map dynamic 4D multi-resolution embedding grids without bucket waste, achieving high encoding capacity with compact encoding parameters. Our encoding method is agnostic to time-varying feature detection methods, making it a unified encoding solution for feature tracking and evolution visualization. Experiments show the F-Hash achieves state-of-the-art convergence speed in training various time-varying volumetric datasets for diverse features. We also proposed an adaptive ray marching algorithm to optimize the sample streaming for faster rendering of the time-varying neural representation.&#39;, &#39;uid&#39;: &#39;72737da9-0932-41f6-99a2-803817cf8c15&#39;, &#39;keywords&#39;: [&#39;Time-varying volume&#39;, &#39;volume visualization&#39;, &#39;input encoding&#39;, &#39;deep learning&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.03836&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_72737da9-0932-41f6-99a2-803817cf8c15.html"> F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Jianxin Sun, David Lenz, Hongfeng Yu, Tom Peterka </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jianxin Sun </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:00:00.000Z &ndash; 2025-11-05T13:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1535&#39;, &#39;session_id&#39;: &#39;full36&#39;, &#39;title&#39;: &#39;VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians&#39;, &#39;contributors&#39;: [&#39;Siyuan Yao&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:12:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:12:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Siyuan Yao&#39;, &#39;email&#39;: &#39;syao2@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}, {&#39;name&#39;: &#39;Chaoli Wang&#39;, &#39;email&#39;: &#39;chaoli.wang@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}], &#39;abstract&#39;: &#39;Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I/O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization.&#39;, &#39;uid&#39;: &#39;3d97e8c0-6d8b-4fe5-aa06-3ddf284b1a45&#39;, &#39;keywords&#39;: [&#39;Volume visualization&#39;, &#39;novel view synthesis&#39;, &#39;scene segmentation&#39;, &#39;segment tracking&#39;, &#39;deformable Gaussian splatting&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.12667&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_3d97e8c0-6d8b-4fe5-aa06-3ddf284b1a45.html"> VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Siyuan Yao, Chaoli Wang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Siyuan Yao </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:12:00.000Z &ndash; 2025-11-05T13:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2025-02-0102&#39;, &#39;session_id&#39;: &#39;full36&#39;, &#39;title&#39;: &#39;DCINR: A Divide-and-Conquer Implicit Neural Representation for Compressing Time-Varying Volumetric Data in Hours&#39;, &#39;contributors&#39;: [&#39;Jun Han&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:36:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Jun Han&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Fan Yang&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;Implicit neural representation (INR) has been a powerful paradigm for effectively compressing time-varying volumetric data. However, the optimization process can span days or even weeks due to its reliance on coordinate-based inputs and outputs for modeling volumetric data. To address this issue, we introduce a divide-and-conquer INR (DCINR), significantly accelerating the compressing process of time-varying volumetric data in hours. Our approach starts by dividing the data set into a set of non-overlapping blocks. Then, we apply a block selection strategy to weed out redundant blocks to reduce the computation cost without sacrificing performance. In parallel, each selected block is modeled by a tiny INR, with the size of the INR being adapted to match the information richness in the block. The block size is determined by maximizing the average network capacity. After optimization, the optimized INRs are utilized to decompress the data set. By evaluating our approach across various time-varying volumetric data sets, DCINR surpasses learning-based and lossy compression approaches in compression ratio, visual fidelity, and various performance metrics. Additionally, this method operates within a comparable compression time to that of lossy compressors, achieves extreme compression ratios ranging from thousands to tens of thousands, and preserves features with high quality.&#39;, &#39;uid&#39;: &#39;d916cf6d-cda6-481e-b404-40f8fb053501&#39;, &#39;keywords&#39;: [&#39;Spatiotemporal phenomena&#39;, &#39;Training&#39;, &#39;Data models&#39;, &#39;Optimization&#39;, &#39;Data compression&#39;, &#39;Image coding&#39;, &#39;Data visualization&#39;, &#39;Partitioning algorithms&#39;, &#39;Graphics processing units&#39;, &#39;Entropy&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3564255&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_d916cf6d-cda6-481e-b404-40f8fb053501.html"> DCINR: A Divide-and-Conquer Implicit Neural Representation for Compressing Time-Varying Volumetric Data in Hours <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Jun Han, Fan Yang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jun Han </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:24:00.000Z &ndash; 2025-11-05T13:36:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG-2023-09-0524&#39;, &#39;session_id&#39;: &#39;full36&#39;, &#39;title&#39;: &#39;RmdnCache: Dual-Space Prefetching Neural Network for Large-Scale Volume Visualization&#39;, &#39;contributors&#39;: [&#39;Jianxin Sun&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:36:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:36:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Jianxin Sun&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Xinyan Xie&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Hongfeng Yu&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;Volume visualization plays a significant role in revealing important intrinsic patterns of 3D scientific datasets. However, these datasets are often large, making it challenging for interactive visualization systems to deliver a seamless user experience because of high input latency that arises from I/O bottlenecks and limited fast memory resources with high miss rates. To address this issue, we have proposed a deep learning-based prefetching method called RmdnCache, which optimizes the data flow across the memory hierarchy to reduce the input latency of large-scale volume visualization. Our approach accurately prefetches the content of the next view to fast memory using learning-based prediction while rendering the current view. The proposed deep learning architecture consists of two networks, RNN and MDN in respective spaces, which work together to predict both the location and likelihood distribution of the next view for defining an optimal prefetching range. Our method outperforms existing state-of-the-art prefetching algorithms in reducing overall input latency for visualizing real-world large-scale volumetric datasets.&#39;, &#39;uid&#39;: &#39;8a14c17e-0a05-4437-9f34-b5b570a00445&#39;, &#39;keywords&#39;: [&#39;Prefetching&#39;, &#39;Data visualization&#39;, &#39;Rendering (computer graphics)&#39;, &#39;Three-dimensional displays&#39;, &#39;Training&#39;, &#39;Data models&#39;, &#39;Deep learning&#39;], &#39;preprint_link&#39;: &#39;https://par.nsf.gov/servlets/purl/10539350&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3410091&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_8a14c17e-0a05-4437-9f34-b5b570a00445.html"> RmdnCache: Dual-Space Prefetching Neural Network for Large-Scale Volume Visualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Jianxin Sun, Xinyan Xie, Hongfeng Yu </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jianxin Sun </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:36:00.000Z &ndash; 2025-11-05T13:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-01-0045&#39;, &#39;session_id&#39;: &#39;full36&#39;, &#39;title&#39;: &#39;Two-Level Transfer Functions Using t-SNE for Data Segmentation in Direct Volume Rendering&#39;, &#39;contributors&#39;: [&#39;Sangbong Yoo&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T14:00:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Sangbong Yoo&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Seokyeon Kim&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yun Jang&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;The transfer function (TF) design is crucial for enhancing the visualization quality and understanding of volume data in volume rendering. Recent research has proposed various multidimensional TFs to utilize diverse attributes extracted from volume data for controlling individual voxel rendering. Although multidimensional TFs enhance the ability to segregate data, manipulating various attributes for the rendering is cumbersome. In contrast, low-dimensional TFs are more beneficial as they are easier to manage, but separating volume data during rendering is problematic. This paper proposes a novel approach, a two-level transfer function, for rendering volume data by reducing TF dimensions. The proposed technique involves extracting multidimensional TF attributes from volume data and applying t-Stochastic Neighbor Embedding (t-SNE) to the TF attributes for dimensionality reduction. The two-level transfer function combines the classical 2D TF and t-SNE TF in the conventional direct volume rendering pipeline. The proposed approach is evaluated by comparing segments in t-SNE TF and rendering images using various volume datasets. The results of this study demonstrate that the proposed approach can effectively allow us to manipulate multidimensional attributes easily while maintaining high visualization quality in volume rendering.&#39;, &#39;uid&#39;: &#39;51452519-94bc-439f-9dc5-3db3fac8080a&#39;, &#39;keywords&#39;: [&#39;Rendering (computer graphics)&#39;, &#39;Transfer functions&#39;, &#39;Histograms&#39;, &#39;Image color analysis&#39;, &#39;Self-organizing feature maps&#39;, &#39;Dimensionality reduction&#39;, &#39;Data visualization&#39;, &#39;Visualization&#39;, &#39;Image segmentation&#39;, &#39;Feature extraction&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3484471&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_51452519-94bc-439f-9dc5-3db3fac8080a.html"> Two-Level Transfer Functions Using t-SNE for Data Segmentation in Direct Volume Rendering <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Sangbong Yoo, Seokyeon Kim, Yun Jang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Sangbong Yoo </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:48:00.000Z &ndash; 2025-11-05T14:00:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG3427335&#39;, &#39;session_id&#39;: &#39;full36&#39;, &#39;title&#39;: &#39;Visualization of Large Non-Trivially Partitioned Unstructured Data with Native Distribution on High-Performance Computing Systems&#39;, &#39;contributors&#39;: [&#39;Alper Sahistan&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T14:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T14:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T14:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Alper Sahistan&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Serkan Demirci&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Ingo Wald&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Stefan Zellmann&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;João Barbosa&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Nate Morrical&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Uğur Güdükbay&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#34;Interactively visualizing large finite element simulation data on High-Performance Computing (HPC) systems poses several difficulties. Some of these relate to unstructured data, which, even on a single node, is much more expensive to render compared to structured volume data. Worse yet, in the data parallel rendering context, such data with highly non-convex spatial domain boundaries will cause rays along its silhouette to enter and leave a given rank&#39;s domains at different distances. This straddling, in turn, poses challenges for both ray marching, which usually assumes successive elements to share a face, and compositing, which usually assumes a single fragment per pixel per rank. We holistically address these issues using a combination of three inter-operating techniques: first, we use a highly optimized GPU ray marching technique that, given an entry point, can march a ray to its exit point with high-performance by exploiting an exclusive-or (XOR) based compaction scheme. Second, we use hardware-accelerated ray tracing to efficiently find the proper entry points for these marching operations. Third, we use a “deep” compositing scheme to properly handle cases where different ranks’ ray segments interleave in depth. We use GPU-to-GPU remote direct memory access (RDMA) to achieve interactive frame rates of 10–15 frames per second and higher for our motivating use case, the Fun3D NASA Mars Lander.&#34;, &#39;uid&#39;: &#39;c22daf54-0b62-4772-ac74-43e651c0184d&#39;, &#39;keywords&#39;: [&#39;Rendering (computer graphics)&#39;, &#39;Data visualization&#39;, &#39;Finite element analysis&#39;, &#39;Sorting&#39;, &#39;Graphics processing units&#39;, &#39;Distributed databases&#39;, &#39;Scalability&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3427335&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_c22daf54-0b62-4772-ac74-43e651c0184d.html"> Visualization of Large Non-Trivially Partitioned Unstructured Data with Native Distribution on High-Performance Computing Systems <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Alper Sahistan, Serkan Demirci, Ingo Wald, Stefan Zellmann, João Barbosa, Nate Morrical, Uğur Güdükbay </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Alper Sahistan </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T14:00:00.000Z &ndash; 2025-11-05T14:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-05T13:00:00+00:00'
    endTime = '2025-11-05T14:15:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "m1-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>