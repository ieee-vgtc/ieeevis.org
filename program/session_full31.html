<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: VA for AI"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: VA for AI"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: VA for AI</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">VA for AI</li></ol></nav><h1 class="session-title">VIS Full Papers: VA for AI</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Johannes Knittel </h3><h3 class="session-room mt-4"> Room: Hall E1 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-05T08:30:00+00:00 &ndash; 2025-11-05T09:45:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-05T08:30:00+00:00 &ndash; 2025-11-05T09:45:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full31.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945416660254782" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1254&#39;, &#39;session_id&#39;: &#39;full31&#39;, &#39;title&#39;: &#39;ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models&#39;, &#39;contributors&#39;: [&#39;Haoxuan Li&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T08:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T08:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T08:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Haoxuan Li&#39;, &#39;email&#39;: &#39;lihaoxuan@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;State Key Lab of CAD&amp;CG, Zhejiang University&#39;}, {&#39;name&#39;: &#39;Zhen Wen&#39;, &#39;email&#39;: &#39;wenzhen@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Zhejiang University&#39;}, {&#39;name&#39;: &#39;Qiqi Jiang&#39;, &#39;email&#39;: &#39;jiangqiqi348284@gmail.com&#39;, &#39;affiliation&#39;: &#39;State Key Lab of CAD&amp;CG, Zhejiang University&#39;}, {&#39;name&#39;: &#39;Chenxiao Li&#39;, &#39;email&#39;: &#39;3220101835@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Zhejiang University, Hangzhou, China&#39;}, {&#39;name&#39;: &#39;Yuwei Wu&#39;, &#39;email&#39;: &#39;wuyw0701@foxmail.com&#39;, &#39;affiliation&#39;: &#39;Zhejiang University&#39;}, {&#39;name&#39;: &#39;Yuchen Yang&#39;, &#39;email&#39;: &#39;yyc_yang@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Zhejiang University&#39;}, {&#39;name&#39;: &#39;Yiyao Wang&#39;, &#39;email&#39;: &#39;wangyiyao@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;State Key Lab of CAD&amp;CG, Zhejiang University&#39;}, {&#39;name&#39;: &#39;Xiuqi Huang&#39;, &#39;email&#39;: &#39;huangxiuqi@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;State Key Lab of CAD&amp;CG, Zhejiang University&#39;}, {&#39;name&#39;: &#39;Minfeng Zhu&#39;, &#39;email&#39;: &#39;minfeng_zhu@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Zhejiang University&#39;}, {&#39;name&#39;: &#39;Wei Chen&#39;, &#39;email&#39;: &#39;chenvis@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Zhejiang University&#39;}], &#39;abstract&#39;: &#39;Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification ⇒ Interpretation ⇒ Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.&#39;, &#39;uid&#39;: &#39;1f23efb5-deca-4391-a7c4-e455fce74cb9&#39;, &#39;keywords&#39;: [&#39;Large Language Models&#39;, &#39;Mechanistic Interpretability&#39;, &#39;Visual Analytics&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_1f23efb5-deca-4391-a7c4-e455fce74cb9.html"> ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Haoxuan Li, Zhen Wen, Qiqi Jiang, Chenxiao Li, Yuwei Wu, Yuchen Yang, Yiyao Wang, Xiuqi Huang, Minfeng Zhu, Wei Chen </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Haoxuan Li </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T08:30:00.000Z &ndash; 2025-11-05T08:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1774&#39;, &#39;session_id&#39;: &#39;full31&#39;, &#39;title&#39;: &#39;OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration&#39;, &#39;contributors&#39;: [&#39;Wei Xue&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T08:42:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T08:42:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T08:54:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Junwen Duan&#39;, &#39;email&#39;: &#39;jwduan@csu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Central South University&#39;}, {&#39;name&#39;: &#39;Wei Xue&#39;, &#39;email&#39;: &#39;234711076@csu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Central South University&#39;}, {&#39;name&#39;: &#39;Ziyao Kang&#39;, &#39;email&#39;: &#39;scv.kangziyao@gmail.com&#39;, &#39;affiliation&#39;: &#39;Central South University&#39;}, {&#39;name&#39;: &#39;Shixia Liu&#39;, &#39;email&#39;: &#39;shixia@tsinghua.edu.cn&#39;, &#39;affiliation&#39;: &#39;Tsinghua University&#39;}, {&#39;name&#39;: &#39;Jiazhi Xia&#39;, &#39;email&#39;: &#39;xiajiazhi@csu.edu.cn&#39;, &#39;affiliation&#39;: &#39;Central South University&#39;}], &#39;abstract&#39;: &#39;Open-world object detection (OWOD) extends traditional object detection to identifying both known and unknown object, necessitating continuous model adaptation as new annotations emerge. Current approaches face significant limitations: 1) data-hungry training due to reliance on a large number of crowdsourced annotations, 2) susceptibility to partial feature overfitting, and 3) limited flexibility due to required model architecture modifications. To tackle these issues, we present OW-CLIP, a visual analytics system that provides curated data and enables data-efficient OWOD model incremental training. OW-CLIP implements plug-and-play multimodal prompt tuning tailored for OWOD settings and introduces a novel Crop-Smoothing technique to mitigate partial feature overfitting. To meet the data requirements for the training methodology, we propose dual-modal data refinement methods that leverage large language models and cross-modal similarity for data generation and filtering. Simultaneously, we develope a visualization interface that enables users to explore and deliver high-quality annotations—including class-specific visual feature phrases and fine-grained differentiated images. Quantitative evaluation demonstrates that OW-CLIP achieves competitive performance at 89% of state-of-the-art performance while requiring only 3.8% self-generated data, while outperforming SOTA approach when trained with equivalent data volumes. A case study shows the effectiveness of the developed method and the improved annotation quality of our visualization system.&#39;, &#39;uid&#39;: &#39;a79b8861-5ff1-41ab-ab84-c564496de8de&#39;, &#39;keywords&#39;: [&#39;Open-world object detection&#39;, &#39;data-efficient supervision&#39;, &#39;large language model&#39;, &#39;human-AI collaboration&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.19870&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_a79b8861-5ff1-41ab-ab84-c564496de8de.html"> OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Junwen Duan, Wei Xue, Ziyao Kang, Shixia Liu, Jiazhi Xia </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Wei Xue </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T08:42:00.000Z &ndash; 2025-11-05T08:54:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-2057&#39;, &#39;session_id&#39;: &#39;full31&#39;, &#39;title&#39;: &#39;Visual Analytics Using Tensor Unified Linear Comparative Analysis&#39;, &#39;contributors&#39;: [&#39;Kazuki Miyake&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T08:54:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T08:54:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:06:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Naoki Okami&#39;, &#39;email&#39;: &#39;237x012x@stu.kobe-u.ac.jp&#39;, &#39;affiliation&#39;: &#39;Kobe University&#39;}, {&#39;name&#39;: &#39;Kazuki Miyake&#39;, &#39;email&#39;: &#39;kazuzaka53@gmail.com&#39;, &#39;affiliation&#39;: &#39;Kobe University&#39;}, {&#39;name&#39;: &#39;Naohisa Sakamoto&#39;, &#39;email&#39;: &#39;naohisa.sakamoto@people.kobe-u.ac.jp&#39;, &#39;affiliation&#39;: &#39;Kobe University&#39;}, {&#39;name&#39;: &#39;Jorji Nonaka&#39;, &#39;email&#39;: &#39;jorji@riken.jp&#39;, &#39;affiliation&#39;: &#39;RIKEN Center for Computational Science&#39;}, {&#39;name&#39;: &#39;Takanori Fujiwara&#39;, &#39;email&#39;: &#39;tfujiwara@ucdavis.edu&#39;, &#39;affiliation&#39;: &#39;Linköping University&#39;}], &#39;abstract&#39;: &#39;Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors’ essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA’s functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.&#39;, &#39;uid&#39;: &#39;b5fec96f-039e-4af2-93d6-898b9a2caece&#39;, &#39;keywords&#39;: [&#39;Tensor decomposition&#39;, &#39;tensor analysis&#39;, &#39;contrastive learning&#39;, &#39;dimensionality reduction&#39;, &#39;interpretability&#39;, &#39;supercomputer&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.19988&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;We provide an open-source library and an open-source visual interface, with easy installation and thoroughly documented source code (following the Python standard). In addition, to help replicate our results, we provide datasets and source code used for evaluations.&#39;, &#39;open_access_supplemental_link&#39;: &#39;https://github.com/vizlab-kobe/tulca&#39;} <h3 class="session-list-title"><a href="paper_b5fec96f-039e-4af2-93d6-898b9a2caece.html"> Visual Analytics Using Tensor Unified Linear Comparative Analysis <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Naoki Okami, Kazuki Miyake, Naohisa Sakamoto, Jorji Nonaka, Takanori Fujiwara </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Kazuki Miyake </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T08:54:00.000Z &ndash; 2025-11-05T09:06:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-02-0136&#39;, &#39;session_id&#39;: &#39;full31&#39;, &#39;title&#39;: &#39;A Simulation-based Approach for Quantifying the Impact of Interactive Label Correction for Machine Learning&#39;, &#39;contributors&#39;: [&#39;Ross Maciejewski&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T09:06:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T09:06:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:18:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Yixuan Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Jieqiong Zhao&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Jiayi Hong&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Ronald G. Askin&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Ross Maciejewski&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;Recent years have witnessed growing interest in understanding the sensitivity of machine learning to training data characteristics. While researchers have claimed the benefits of activities such as a human-in-the-loop approach of interactive label correction for improving model performance, there have been limited studies to quantitatively probe the relationship between the cost of label correction and the associated benefit in model performance. We employ a simulation-based approach to explore the efficacy of label correction under diverse task conditions, namely different datasets, noise properties, and machine learning algorithms. We measure the impact of label correction on model performance under the best-case scenario assumption: perfect correction (perfect human and visual systems), serving as an upper-bound estimation of the benefits derived from visual interactive label correction. The simulation results reveal a trade-off between the label correction effort expended and model performance improvement. Notably, task conditions play a crucial role in shaping the trade-off. Based on the simulation results, we develop a set of recommendations to help practitioners determine conditions under which interactive label correction is an effective mechanism for improving model performance.&#39;, &#39;uid&#39;: &#39;efeed64e-a269-420a-a742-b9816bd710fd&#39;, &#39;keywords&#39;: [&#39;Costs&#39;, &#39;Labeling&#39;, &#39;Machine learning&#39;, &#39;Data models&#39;, &#39;Noise&#39;, &#39;Analytical models&#39;, &#39;Training&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2024.3468352&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;noteworthy re-analyses or replications, simulation methods.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_efeed64e-a269-420a-a742-b9816bd710fd.html"> A Simulation-based Approach for Quantifying the Impact of Interactive Label Correction for Machine Learning <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Yixuan Wang, Jieqiong Zhao, Jiayi Hong, Ronald G. Askin, Ross Maciejewski </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Ross Maciejewski </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T09:06:00.000Z &ndash; 2025-11-05T09:18:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2023.3339585&#39;, &#39;session_id&#39;: &#39;full31&#39;, &#39;title&#39;: &#39;An In-Situ Visual Analytics Framework for Deep Neural Networks&#39;, &#39;contributors&#39;: [&#39;Guan Li&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T09:18:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T09:18:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:30:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Guan Li&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Junpeng Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Yang Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Guihua Shan&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Ying Zhao&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;The past decade has witnessed the superior power of deep neural networks (DNNs) in applications across various domains. However, training a high-quality DNN remains a non-trivial task due to its massive number of parameters. Visualization has shown great potential in addressing this situation, as evidenced by numerous recent visualization works that aid in DNN training and interpretation. These works commonly employ a strategy of logging training-related data and conducting post-hoc analysis. Based on the results of offline analysis, the model can be further trained or fine-tuned. This strategy, however, does not cope with the increasing complexity of DNNs, because (1) the time-series data collected over the training are usually too large to be stored entirely; (2) the huge I/O overhead significantly impacts the training efficiency; (3) post-hoc analysis does not allow rapid human-interventions (e.g., stop training with improper hyper-parameter settings to save computational resources). To address these challenges, we propose an in-situ visualization and analysis framework for the training of DNNs. Specifically, we employ feature extraction algorithms to reduce the size of training-related data in-situ and use the reduced data for real-time visual analytics. The states of model training are disclosed to model designers in real-time, enabling human interventions on demand to steer the training. Through concrete case studies, we demonstrate how our in-situ framework helps deep learning experts optimize DNNs and improve their analysis efficiency.&#39;, &#39;uid&#39;: &#39;eabf678a-dc8f-4794-9da1-3c30f6b26896&#39;, &#39;keywords&#39;: [&#39;Data visualization&#39;, &#39;Training&#39;, &#39;Data models&#39;, &#39;Analytical models&#39;, &#39;Feature extraction&#39;, &#39;Artificial neural networks&#39;, &#39;Computational modeling&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2023.3339585&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: &#39;Our work introduces a novel in-situ visualization framework that performs real-time feature extraction and visual analytics during DNN training.&#39;, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_eabf678a-dc8f-4794-9da1-3c30f6b26896.html"> An In-Situ Visual Analytics Framework for Deep Neural Networks <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Guan Li, Junpeng Wang, Yang Wang, Guihua Shan, Ying Zhao </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Guan Li </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T09:18:00.000Z &ndash; 2025-11-05T09:30:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-TVCG2024-07-0626&#39;, &#39;session_id&#39;: &#39;full31&#39;, &#39;title&#39;: &#39;VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels&#39;, &#39;contributors&#39;: [&#39;Jorge Ono&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T09:30:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T09:30:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T09:42:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Xiwei Xuan&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Xiaoqi Wang&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Wenbin He&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Jorge Piazentin Ono&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Liang Gou&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Kwan-Liu Ma&#39;, &#39;email&#39;: None}, {&#39;name&#39;: &#39;Liu Ren&#39;, &#39;email&#39;: None}], &#39;abstract&#39;: &#39;The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA’s effectiveness from both quantitative and qualitative perspectives.&#39;, &#39;uid&#39;: &#39;594e8ec2-1e1d-4c9a-8893-5134132f5232&#39;, &#39;keywords&#39;: [&#39;Data integrity&#39;, &#39;Data models&#39;, &#39;Frequency modulation&#39;, &#39;Pipelines&#39;, &#39;Visual analytics&#39;, &#39;Measurement&#39;, &#39;Computational modeling&#39;, &#39;Labeling&#39;, &#39;Analytical models&#39;, &#39;Human in the loop&#39;], &#39;preprint_link&#39;: &#39;&#39;, &#39;has_pdf&#39;: False, &#39;paper_award&#39;: None, &#39;doi&#39;: &#39;10.1109/TVCG.2025.3535896&#39;, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_594e8ec2-1e1d-4c9a-8893-5134132f5232.html"> VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Xiwei Xuan, Xiaoqi Wang, Wenbin He, Jorge Piazentin Ono, Liang Gou, Kwan-Liu Ma, Liu Ren </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Jorge Ono </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T09:30:00.000Z &ndash; 2025-11-05T09:42:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-05T08:30:00+00:00'
    endTime = '2025-11-05T09:45:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "e1-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>