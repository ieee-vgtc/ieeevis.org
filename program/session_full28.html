<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: The VIS in GenAI"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: The VIS in GenAI"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: The VIS in GenAI</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">The VIS in GenAI</li></ol></nav><h1 class="session-title">VIS Full Papers: The VIS in GenAI</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Narges Mahyar </h3><h3 class="session-room mt-4"> Room: Hall E1 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-05T13:00:00+00:00 &ndash; 2025-11-05T14:15:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-05T13:00:00+00:00 &ndash; 2025-11-05T14:15:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full28.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5 discord-public-link hide-auth-controls"><div class="col-md-8"><p><span class="fas mx-1">&#xf086;</span><a href="https://discord.com/channels/1422883912660549696/1430945416660254782" target="_blank">Discord link</a></p></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1207&#39;, &#39;session_id&#39;: &#39;full28&#39;, &#39;title&#39;: &#39;Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles&#39;, &#39;contributors&#39;: [&#39;Shaolun RUAN&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Shaolun Ruan&#39;, &#39;email&#39;: &#39;haywardryan@foxmail.com&#39;, &#39;affiliation&#39;: &#39;Singapore Management University&#39;}, {&#39;name&#39;: &#39;Rui Sheng&#39;, &#39;email&#39;: &#39;rshengac@connect.ust.hk&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Xiaolin Wen&#39;, &#39;email&#39;: &#39;xiaolin004@e.ntu.edu.sg&#39;, &#39;affiliation&#39;: &#39;Nanyang Technological University&#39;}, {&#39;name&#39;: &#39;Jiachen Wang&#39;, &#39;email&#39;: &#39;wangjiachen@zju.edu.cn&#39;, &#39;affiliation&#39;: &#39;Zhejiang University&#39;}, {&#39;name&#39;: &#39;Tianyi Zhang&#39;, &#39;email&#39;: &#39;tianyizhang.2023@phdcs.smu.edu.sg&#39;, &#39;affiliation&#39;: &#39;Singapore Management University&#39;}, {&#39;name&#39;: &#39;Yong WANG&#39;, &#39;email&#39;: &#39;yong-wang@ntu.edu.sg&#39;, &#39;affiliation&#39;: &#39;Nanyang Technological University&#39;}, {&#39;name&#39;: &#39;Tim Dwyer&#39;, &#39;email&#39;: &#39;tgdwyer@gmail.com&#39;, &#39;affiliation&#39;: &#39;Monash University&#39;}, {&#39;name&#39;: &#39;Jiannan Li&#39;, &#39;email&#39;: &#39;jiannanli@smu.edu.sg&#39;, &#39;affiliation&#39;: &#39;Singapore Management University&#39;}], &#39;abstract&#39;: &#39;Design studies aim to develop visualization solutions for real-world problems across various application domains. Recently, the emergence of large language models (LLMs) has introduced new opportunities to enhance the design study process, providing capabilities such as creative problem-solving, data handling, and insightful analysis. However, despite their growing popularity, there remains a lack of systematic understanding of how LLMs can effectively assist researchers in visualization-specific design studies. In this paper, we conducted a multi-stage qualitative study to fill this gap, which involved 30 design study researchers from diverse backgrounds and expertise levels. Through in-depth interviews and carefully-designed questionnaires, we investigated strategies for utilizing LLMs, the challenges encountered, and the practices used to overcome them. We further compiled the roles that LLMs can play across different stages of the design study process. Our findings highlight practical implications to inform visualization practitioners, and also provide a framework for leveraging LLMs to facilitate the design study process in visualization research.&#39;, &#39;uid&#39;: &#39;29656e00-00d7-4080-b589-2d7c838c72be&#39;, &#39;keywords&#39;: [&#39;Design Study&#39;, &#39;Large Language Models (LLMs)&#39;, &#39;Qualitative Study&#39;, &#39;Visualization&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.10024&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_29656e00-00d7-4080-b589-2d7c838c72be.html"> Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Shaolun Ruan, Rui Sheng, Xiaolin Wen, Jiachen Wang, Tianyi Zhang, Yong WANG, Tim Dwyer, Jiannan Li </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Shaolun RUAN </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:00:00.000Z &ndash; 2025-11-05T13:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1399&#39;, &#39;session_id&#39;: &#39;full28&#39;, &#39;title&#39;: &#39;HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery&#39;, &#39;contributors&#39;: [&#39;Haoran Jiang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:12:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:12:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:24:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Shaohan Shi&#39;, &#39;email&#39;: &#39;shishh2023@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Haoran Jiang&#39;, &#39;email&#39;: &#39;jianghr2023@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Yunjie Yao&#39;, &#39;email&#39;: &#39;yaoyj2024@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}, {&#39;name&#39;: &#39;Chang Jiang&#39;, &#39;email&#39;: &#39;cjiang_fdu@yeah.net&#39;, &#39;affiliation&#39;: &#39;Shanghai Clinical Research and Trial Center&#39;}, {&#39;name&#39;: &#39;Quan Li&#39;, &#39;email&#39;: &#39;liquan@shanghaitech.edu.cn&#39;, &#39;affiliation&#39;: &#39;ShanghaiTech University&#39;}], &#39;abstract&#39;: &#39;Modern scientific discovery encounters significant challenges in integrating the rapidly expanding and heterogeneous body of knowledge required for driving breakthroughs in biomedicine and drug development. While traditional hypothesis-driven research has proven effective, it is constrained by human cognitive limitations, the complexity of biological systems, and the high costs associated with trial-and-error experimentation. Deep learning models, particularly graph neural networks (GNNs), have accelerated scientific progress. However, the sheer volume of predictions they generate makes manual selection for experimental validation impractical. Attempts to leverage large language models (LLMs) for filtering predictions and generating novel hypotheses have been impeded by issues such as hallucinations and the lack of structured knowledge grounding, which undermine their reliability. To address these challenges, we propose HypoChainer, a collaborative visualization framework that integrates human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance scientific discovery and validation visually. HypoChainer operates through three key stages: (1) Exploration and Contextualization: Domain experts employ retrieval-augmented LLMs (RAGs) and dimensionality reduction techniques to extract insights and research entry points from vast GNN predictions, supplemented by interactive explanations for in-depth understanding; (2) Hypothesis Chain Formation: Experts iteratively explore the relationships between KG information relevant to the predictions and semantically linked nodes consistent with the hypothesis, gaining knowledge and insights while refining the hypothesis through suggestions from LLMs and KGs; and (3) Validation Prioritization: Predictions are filtered and prioritized based on the refined hypothesis chains and KG-supported evidence, identifying high-priority candidates for experimental validation. Weak points in the hypothesis chain are further optimized through visual analytics of the retrieval results. We evaluated the effectiveness of HypoChainer in hypothesis construction and scientific discovery through case studies in two distinct domains and expert interviews.&#39;, &#39;uid&#39;: &#39;c06df49a-9aa2-40f9-94b9-23789ec137b8&#39;, &#39;keywords&#39;: [&#39;Large Language Model&#39;, &#39;Visual Analytics&#39;, &#39;Iterative Human-AI Collaboration&#39;, &#39;Knowledge Graph&#39;, &#39;Hypothesis Construction&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.17209&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_c06df49a-9aa2-40f9-94b9-23789ec137b8.html"> HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Shaohan Shi, Haoran Jiang, Yunjie Yao, Chang Jiang, Quan Li </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Haoran Jiang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:12:00.000Z &ndash; 2025-11-05T13:24:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1507&#39;, &#39;session_id&#39;: &#39;full28&#39;, &#39;title&#39;: &#39;Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances&#39;, &#39;contributors&#39;: [&#39;Chase Stokes&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:24:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:24:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:36:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Chase Stokes&#39;, &#39;email&#39;: &#39;chase_stokes@berkeley.edu&#39;, &#39;affiliation&#39;: &#39;University of California Berkeley&#39;}, {&#39;name&#39;: &#39;Kylie Lin&#39;, &#39;email&#39;: &#39;klin368@gatech.edu&#39;, &#39;affiliation&#39;: &#39;Georgia Institute of Technology&#39;}, {&#39;name&#39;: &#39;Cindy Xiong Bearfield&#39;, &#39;email&#39;: &#39;cxiong@gatech.edu&#39;, &#39;affiliation&#39;: &#39;Georgia Tech&#39;}], &#39;abstract&#39;: &#39;A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.&#39;, &#39;uid&#39;: &#39;4cf2c4a7-98c7-4d49-a5c2-410ecde134cb&#39;, &#39;keywords&#39;: [&#39;Information visualizations&#39;, &#39;affordances&#39;, &#39;methodology&#39;, &#39;conclusions&#39;, &#39;large-language models.&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.17024&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_4cf2c4a7-98c7-4d49-a5c2-410ecde134cb.html"> Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Chase Stokes, Kylie Lin, Cindy Xiong Bearfield </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Chase Stokes </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:24:00.000Z &ndash; 2025-11-05T13:36:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1548&#39;, &#39;session_id&#39;: &#39;full28&#39;, &#39;title&#39;: &#39;&#34;It looks sexy but it\&#39;s wrong.\&#39;\&#39; Tensions in creativity and accuracy using genAI for biomedical visualization&#39;, &#39;contributors&#39;: [&#39;Roxanne Ziman&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:36:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:36:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T13:48:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Roxanne Ziman&#39;, &#39;email&#39;: &#39;roxanne.ziman@uib.no&#39;, &#39;affiliation&#39;: &#39;University of Bergen&#39;}, {&#39;name&#39;: &#39;Shehryar Saharan&#39;, &#39;email&#39;: &#39;s.saharan@utoronto.ca&#39;, &#39;affiliation&#39;: &#39;University of Toronto&#39;}, {&#39;name&#39;: &#39;Gaël McGill&#39;, &#39;email&#39;: &#39;mcgill@digizyme.com&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}, {&#39;name&#39;: &#39;Laura Garrison&#39;, &#39;email&#39;: &#39;laura.garrison@uib.no&#39;, &#39;affiliation&#39;: &#39;University of Bergen&#39;}], &#39;abstract&#39;: &#39;We contribute an in-depth analysis of the workflows and tensions arising from generative AI (genAI) use in biomedical visualization (BioMedVis). Although genAI affords facile production of aesthetic visuals for biological and medical content, the architecture of these tools fundamentally limits the accuracy and trustworthiness of the depicted information, from imaginary (or fanciful) molecules to alien anatomy. Through 17 interviews with a diverse group of practitioners and researchers, we qualitatively analyze the concerns and values driving genAI (dis)use for the visual representation of spatially oriented biomedical data. We find that BioMedVis experts, both in roles as developers and designers, use genAI tools at different stages of their daily workflows and hold attitudes ranging from enthusiastic adopters to skeptical avoiders of genAI. In contrasting the current use and perspectives on genAI observed in our study with predictions towards genAI in the visualization pipeline from prior work, we refocus the discussion of genAI’s effects on projects in visualization in the here and now with its respective opportunities and pitfalls for future visualization research. At a time when public trust in science is in jeopardy, we are reminded to first do no harm, not just in biomedical visualization but in science communication more broadly. Our observations reaffirm the necessity of human intervention for empathetic design and assessment of accurate scientific visuals. Supplemental study materials are available at https://osf.io/genaixbiomedvis/.&#39;, &#39;uid&#39;: &#39;619269e9-f6be-4882-aec8-0ffe4cee1a96&#39;, &#39;keywords&#39;: [&#39;biomedical visualization&#39;, &#39;science communication&#39;, &#39;generative AI&#39;, &#39;human-AI collaboration&#39;, &#39;creativity&#39;, &#39;qualitative methods&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.14494&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://osf.io/mbw86/?view_only=e087ab5b90a6474abec7bfc42cd2b105&#39;} <h3 class="session-list-title"><a href="paper_619269e9-f6be-4882-aec8-0ffe4cee1a96.html"> &#34;It looks sexy but it&#39;s wrong.&#39;&#39; Tensions in creativity and accuracy using genAI for biomedical visualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Roxanne Ziman, Shehryar Saharan, Gaël McGill, Laura Garrison </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Roxanne Ziman </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:36:00.000Z &ndash; 2025-11-05T13:48:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1770&#39;, &#39;session_id&#39;: &#39;full28&#39;, &#39;title&#39;: &#39;Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation&#39;, &#39;contributors&#39;: [&#39;Nan Xiang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T13:48:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T13:48:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T14:00:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Nan Xiang&#39;, &#39;email&#39;: &#39;51275902056@stu.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}, {&#39;name&#39;: &#39;Tianyi Liang&#39;, &#39;email&#39;: &#39;51215901019@stu.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}, {&#39;name&#39;: &#39;Haiwen Huang&#39;, &#39;email&#39;: &#39;hwhuang@stu.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}, {&#39;name&#39;: &#39;Shiqi Jiang&#39;, &#39;email&#39;: &#39;52265901032@stu.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}, {&#39;name&#39;: &#39;Hao Huang&#39;, &#39;email&#39;: &#39;2690210766@qq.com&#39;, &#39;affiliation&#39;: &#39;School of Computer Science and Technology&#39;}, {&#39;name&#39;: &#39;Yifei Huang&#39;, &#39;email&#39;: &#39;yifeihuang17@gmail.com&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}, {&#39;name&#39;: &#39;Liangyu Chen&#39;, &#39;email&#39;: &#39;lychen@sei.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}, {&#39;name&#39;: &#39;Changbo Wang&#39;, &#39;email&#39;: &#39;cbwang@cs.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;School of Computer Science and Technology&#39;}, {&#39;name&#39;: &#39;Chenhui Li&#39;, &#39;email&#39;: &#39;chli@cs.ecnu.edu.cn&#39;, &#39;affiliation&#39;: &#39;East China Normal University&#39;}], &#39;abstract&#39;: &#39;Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and a user study demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.&#39;, &#39;uid&#39;: &#39;84ef8d69-bdba-43e8-a078-ae651a3de164&#39;, &#39;keywords&#39;: [&#39;Prompt engineering&#39;, &#39;text-to-3D generation&#39;, &#39;shape exploration&#39;, &#39;visualization design&#39;, &#39;visual perception&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2508.00428&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_84ef8d69-bdba-43e8-a078-ae651a3de164.html"> Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Nan Xiang, Tianyi Liang, Haiwen Huang, Shiqi Jiang, Hao Huang, Yifei Huang, Liangyu Chen, Changbo Wang, Chenhui Li </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Nan Xiang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T13:48:00.000Z &ndash; 2025-11-05T14:00:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1905&#39;, &#39;session_id&#39;: &#39;full28&#39;, &#39;title&#39;: &#39;Can LLMs Bridge Domain and Visualization? A Case Study on High-Dimension Data Visualization in Single-Cell Transcriptomics&#39;, &#39;contributors&#39;: [&#39;Qianwen Wang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Premise&#39;, &#39;time_stamp&#39;: &#39;2025-11-05T14:00:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-05T14:00:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-05T14:12:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Qianwen Wang&#39;, &#39;email&#39;: &#39;qianwen@umn.edu&#39;, &#39;affiliation&#39;: &#39;University of Minnesota&#39;}, {&#39;name&#39;: &#39;Xinyi Liu&#39;, &#39;email&#39;: &#39;xinyi.liu@utexas.edu&#39;, &#39;affiliation&#39;: &#39;Northeastern University&#39;}, {&#39;name&#39;: &#39;Nils Gehlenborg&#39;, &#39;email&#39;: &#39;nils@hms.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}], &#39;abstract&#39;: &#39;While many visualizations are build for domain users (e.g., biologists, machine learning developers), understanding how visualizations are used in the domain has long been a challenging task. Previous research has relied on either interviewing a limited number of domain users or reviewing relevant application papers in the visualization community, neither of which provides comprehensive insight into visualizations in the wild of a specific domain. This paper aims to fill this gap by examining the potential of using Large Language Models (LLM) to analyze visualization usage in domain literature. We use high-dimension (HD) data visualization in sing-cell transcriptomics as a test case, analyzing 1,203 papers that describe 2,056 HD visualizations with highly specialized domain terminologies (e.g., biomarkers, cell lineage). To facilitate this analysis, we introduce a multi-step, human-in-the-loop LLM workflow. Instead of relying solely on LLMs for end-to-end analysis, our workflow enhances analytical quality through 1) integrating image processing and traditional NLP methods to prepare well-structured inputs for three targeted LLM subtasks (i.e., translating domain terminology, summarizing analysis tasks, and performing categorization), and 2) establishing checkpoints for human involvement and validation throughout the process.\nThe analysis results was validated with expert interviews and a test set, revealing three often overlooked aspects in HD visualization: trajectories in HD spaces, inter-cluster relationships, and dimension clustering.\nThis research provides a stepping stone for future studies seeking to use LLMs to bridge the gap between visualization design and domain-specific usage.&#39;, &#39;uid&#39;: &#39;32bd145e-06bf-411a-af2e-109192108a7e&#39;, &#39;keywords&#39;: [&#39;High dimensional visualization; LLM-supported literature review; Visualization in the wild&#39;], &#39;preprint_link&#39;: &#39;https://osf.io/preprints/osf/qtsak_v2?view_only=&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: &#39;https://hdvis.github.io&#39;} <h3 class="session-list-title"><a href="paper_32bd145e-06bf-411a-af2e-109192108a7e.html"> Can LLMs Bridge Domain and Visualization? A Case Study on High-Dimension Data Visualization in Single-Cell Transcriptomics <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Qianwen Wang, Xinyi Liu, Nils Gehlenborg </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Qianwen Wang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-05T14:00:00.000Z &ndash; 2025-11-05T14:12:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-05T13:00:00+00:00'
    endTime = '2025-11-05T14:15:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "e1-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>