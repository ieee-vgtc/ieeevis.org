<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><script src="https://cdn.auth0.com/js/auth0-spa-js/2.1/auth0-spa-js.production.js"></script><script>
              const auth0_domain = "ieeevis.us.auth0.com";
              const auth0_client_id = "oF5BXUklWOjSjUeg5Tzai2DysHITXYhT";
            </script><script src="/static/2025/js/modules/auth0protect.js"></script><script src="https://cdn.jsdelivr.net/npm/d3@6/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script><script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/static/2025/js/libs_ext/typeahead.bundle.js"></script><script src="/static/2025/js/data/persistor.js"></script><script src="/static/2025/js/data/api.js"></script><link rel="shortcut icon" href="/static/2025/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous"><link href="/static/2025/css/Zilla.css" rel="stylesheet"><link href="/static/2025/css/Fira.css" rel="stylesheet"><link rel="stylesheet" href="/static/2025/css/main.css"><link rel="stylesheet" href="/static/2025/css/fa_solid.css"><link rel="stylesheet" href="/static/2025/css/lazy_load.css"><link rel="stylesheet" href="/static/2025/css/typeahead.css"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ieeevis"><meta name="twitter:title" content="IEEE VIS 2025 - Session: VIS Full Papers: Virtual Session: Volumes & 3D"><meta name="twitter:description" content="See the session and its presentations inside."><meta name="twitter:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="image" property="og:image" content="https://ieeevis.b-cdn.net/vis_2021/vis_preview.png"><meta name="description" property="og:description" content="See the session and its presentations inside."><meta name="title" property="og:title" content="Virtual IEEE VIS 2025 - Session: VIS Full Papers: Virtual Session: Volumes & 3D"><meta property="og:type" content="website"><title>IEEE VIS 2025 Content: VIS Full Papers: Virtual Session: Volumes &amp; 3D</title></head><body data-bs-spy="scroll" data-bs-target="#nav-scrollspy" style="display: none;"><div class="container mb-5"><div class="tabs"></div><div class="content"><div class="row mt-3"><div class="col-md-8"><nav class="nav-breadcrumb mb-3" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item" aria-current="page"><a href="events.html">All events</a></li><li class="breadcrumb-item"><a href="event_v-full.html">VIS Full Papers</a></li><li class="breadcrumb-item active text-truncate" aria-current="page">Virtual Session: Volumes &amp; 3D</li></ol></nav><h1 class="session-title">VIS Full Papers: Virtual Session: Volumes &amp; 3D</h1><h3 class="session-url"><a href="https://ieeevis.org/year/2025/program/event_v-full.html" target="_blank"><span class="fas mr-1">&#xf57c;</span> https://ieeevis.org/year/2025/program/event_v-full.html</a></h3><h3 class="session-chair"><span class="fas mr-1">&#xf007;</span> Session chair: Stefan Bruckner </h3><h3 class="session-room mt-4"> Room: Room 0.94 + 0.95 </h3><h3 class="session-date"><span class="fas mr-1">&#xf017;</span><span class="format-date-span-full">2025-11-06T10:15:00+00:00 &ndash; 2025-11-06T11:30:00+00:00</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span><br><span style="margin-left: 2rem; font-size: 1rem;" class="timezone tztooltip"><span class="relative-time">2025-11-06T10:15:00+00:00 &ndash; 2025-11-06T11:30:00+00:00</span><span class="current-time tztooltiptext"></span></span></h3></div><div class="col-md-1"></div><div class="col-md-3 session-links"><h5 class="session-info my-4"><a href="#list"><span class="fas mr-2">&#xf358;</span> Jump to event listing </a></h5><h5 class="session-info my-4"><a href="https://data.tech.ieeevis.org/storage/v1/object/public/ics/vis2025/full32.ics"><span class="fas mr-2">&#xf073;</span> Add to Calendar</a></h5></div></div><div class="row my-5"><div class="col-md-8"><p>Recordings will be made available after the session.</p></div></div><hr><div class="row my-4"><div class="col-md-8"><h2 class="room_session_descriptor d-block py-2"><a name="list">Presentations in this session:</a></h2></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1080&#39;, &#39;session_id&#39;: &#39;full32&#39;, &#39;title&#39;: &#39;Design Space and Declarative Grammar for 3D Genomic Data Visualization&#39;, &#39;contributors&#39;: [&#39;David Kouřil&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:15:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:15:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T10:27:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;David Kouřil&#39;, &#39;email&#39;: &#39;david_kouril@hms.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}, {&#39;name&#39;: &#39;Trevor Manz&#39;, &#39;email&#39;: &#39;trevor_manz@g.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}, {&#39;name&#39;: &#34;Sehi L&#39;Yi&#34;, &#39;email&#39;: &#39;sehi_lyi@hms.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}, {&#39;name&#39;: &#39;Nils Gehlenborg&#39;, &#39;email&#39;: &#39;nils@hms.harvard.edu&#39;, &#39;affiliation&#39;: &#39;Harvard Medical School&#39;}], &#39;abstract&#39;: &#39;Various computational approaches predict chromatin structure, yielding concrete models that position genomic loci in physical space and help reveal genome organization and function. While prior visualization research has explored data and task abstractions for genomics, the design space for depicting these three-dimensional (3D) genome models---and associated genome-mapped data---remains unclear. In this paper, we investigate the visualization of genomic data with a spatial component. First, we systematically survey how 3D genome models are used and depicted in computational biology. We analyze over 300 papers with figures that visualize 3D genomic data and categorize the methods for visual representation. From this survey, we derive a design space for visualizing 3D genome data, identifying common patterns and key properties such as representation, visual channels, and composition. We position these findings within an existing genomics visualization taxonomy, refining and extending existing classifications. Second, we augment Gosling, a declarative visualization grammar for genomics, to support 3D genomic data. Our integration enables expressive authoring of visualizations that connect traditional genome-mapped information with 3D genome models, emphasizing their spatial characteristics. To demonstrate its utility, we employ our extended grammar to recreate interactive examples, showcasing its ability to represent complex visual designs. Comprehensive examples and an interactive editor are available at https://3d.gosling-lang.org.&#39;, &#39;uid&#39;: &#39;1af413bb-b7e1-4e7a-bbba-ec896ec760e7&#39;, &#39;keywords&#39;: [&#39;genomic data&#39;, &#39;3D visualization&#39;, &#39;declarative grammars&#39;], &#39;preprint_link&#39;: &#39;https://doi.org/10.31219/osf.io/dtr6u_v1&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_1af413bb-b7e1-4e7a-bbba-ec896ec760e7.html"> Design Space and Declarative Grammar for 3D Genomic Data Visualization <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: David Kouřil, Trevor Manz, Sehi L&#39;Yi, Nils Gehlenborg </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> David Kouřil </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:15:00.000Z &ndash; 2025-11-06T10:27:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1100&#39;, &#39;session_id&#39;: &#39;full32&#39;, &#39;title&#39;: &#39;GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting&#39;, &#39;contributors&#39;: [&#39;David Bauer&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:27:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:27:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T10:39:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;David Bauer&#39;, &#39;email&#39;: &#39;david.bauer009@gmail.com&#39;, &#39;affiliation&#39;: &#39;UC Davis&#39;}, {&#39;name&#39;: &#39;Qi Wu&#39;, &#39;email&#39;: &#39;qiwu@nvidia.com&#39;, &#39;affiliation&#39;: &#39;NVIDIA&#39;}, {&#39;name&#39;: &#39;Hamid Gadirov&#39;, &#39;email&#39;: &#39;h.gadirov@rug.nl&#39;, &#39;affiliation&#39;: &#39;University of Groningen&#39;}, {&#39;name&#39;: &#39;Kwan-Liu Ma&#39;, &#39;email&#39;: &#39;ma@cs.ucdavis.edu&#39;, &#39;affiliation&#39;: &#39;University of California at Davis&#39;}], &#39;abstract&#39;: &#39;Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.&#39;, &#39;uid&#39;: &#39;99d47ba0-22e8-4d68-9484-346e6fc722b8&#39;, &#39;keywords&#39;: [&#39;Radiance caching&#39;, &#39;path tracing&#39;, &#39;volume rendering&#39;, &#39;gaussian splatting&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.19718&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_99d47ba0-22e8-4d68-9484-346e6fc722b8.html"> GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: David Bauer, Qi Wu, Hamid Gadirov, Kwan-Liu Ma </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> David Bauer </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:27:00.000Z &ndash; 2025-11-06T10:39:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1161&#39;, &#39;session_id&#39;: &#39;full32&#39;, &#39;title&#39;: &#39;VolMoVis: Real-Time Volume Generation and Motion Visualization with Dynamic Tomographic Reconstruction&#39;, &#39;contributors&#39;: [&#39;Gaofeng Deng&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:39:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:39:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T10:51:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Gaofeng Deng&#39;, &#39;email&#39;: &#39;dgaofeng@cs.stonybrook.edu&#39;, &#39;affiliation&#39;: &#39;Stony Brook University&#39;}, {&#39;name&#39;: &#39;Arie Kaufman&#39;, &#39;email&#39;: &#39;ari@cs.stonybrook.edu&#39;, &#39;affiliation&#39;: &#39;Stony Brook University&#39;}], &#39;abstract&#39;: &#39;We present VolMoVis, a method for dynamic tomographic reconstruction that supports real-time volume generation and volumetric motion visualization from 2D projections. Visualizing the motion of 3D anatomical structures, such as organs and tumors, is critical for computer-aided interventions. However, conventional 4D volumetric reconstruction methods typically produce a limited set of volumes at discrete phases, suffering from low temporal resolution. Moreover, it often requires extensive segmentation of 3D structures or regions for visualizing volumetric data, making it challenging to segment and visualize dynamic volumes in real-time. To address these challenges, VolMoVis framework employs a continuous implicit neural representation that decomposes the dynamic volumetric data into a static reference volume and a continuous deformation field. This decomposition, along with an efficient deformation network, enables our framework to achieve real-time volume generation and volumetric visualization of continuous anatomical motions. We evaluate VolMoVis on both 4D digital phantoms and real patient datasets, demonstrating its effectiveness for accurate anatomical reconstruction and motion tracking. Furthermore, we highlight its capabilities in real-time simultaneous volume generation and tumor segmentation for visualizing dynamic volumes and 4D tumor tracking, showcasing its potential in image-guided radiation therapy.&#39;, &#39;uid&#39;: &#39;f84ebbf0-98bb-4a65-9640-3161c7449f60&#39;, &#39;keywords&#39;: [&#39;Neural Representations&#39;, &#39;4D Reconstruction&#39;, &#39;Dynamic Volume Visualization&#39;, &#39;Real-Time Generation&#39;, &#39;Volume Rendering&#39;], &#39;preprint_link&#39;: None, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_f84ebbf0-98bb-4a65-9640-3161c7449f60.html"> VolMoVis: Real-Time Volume Generation and Motion Visualization with Dynamic Tomographic Reconstruction <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Gaofeng Deng, Arie Kaufman </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Gaofeng Deng </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:39:00.000Z &ndash; 2025-11-06T10:51:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1206&#39;, &#39;session_id&#39;: &#39;full32&#39;, &#39;title&#39;: &#39;AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation&#39;, &#39;contributors&#39;: [&#39;Delin An&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T10:51:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T10:51:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T11:03:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Delin An&#39;, &#39;email&#39;: &#39;dan3@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}, {&#39;name&#39;: &#39;Pan Du&#39;, &#39;email&#39;: &#39;pdu@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}, {&#39;name&#39;: &#39;Jian-Xun Wang&#39;, &#39;email&#39;: &#39;jw2837@cornell.edu&#39;, &#39;affiliation&#39;: &#39;Cornell University&#39;}, {&#39;name&#39;: &#39;Chaoli Wang&#39;, &#39;email&#39;: &#39;chaoli.wang@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}], &#39;abstract&#39;: &#39;Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.&#39;, &#39;uid&#39;: &#39;8330bc4e-c9eb-41e0-8fa0-8043f0518241&#39;, &#39;keywords&#39;: [&#39;Conditional diffusion model&#39;, &#39;volume-guided surface generation&#39;, &#39;multi-branch vessel modeling&#39;], &#39;preprint_link&#39;: &#39;http://arxiv.org/abs/2507.13404&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_8330bc4e-c9eb-41e0-8fa0-8043f0518241.html"> AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Delin An, Pan Du, Jian-Xun Wang, Chaoli Wang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Delin An </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T10:51:00.000Z &ndash; 2025-11-06T11:03:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row mb-20"><div class="col-md-8 session-listing"> {&#39;slot_id&#39;: &#39;v-full-1274&#39;, &#39;session_id&#39;: &#39;full32&#39;, &#39;title&#39;: &#39;TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting&#39;, &#39;contributors&#39;: [&#39;Kaiyuan Tang&#39;], &#39;paper_type&#39;: &#39;Full&#39;, &#39;presentation_mode&#39;: &#39;Virtual&#39;, &#39;time_stamp&#39;: &#39;2025-11-06T11:03:00.000Z&#39;, &#39;time_start&#39;: &#39;2025-11-06T11:03:00.000Z&#39;, &#39;time_end&#39;: &#39;2025-11-06T11:15:00.000Z&#39;, &#39;authors&#39;: [{&#39;name&#39;: &#39;Kaiyuan Tang&#39;, &#39;email&#39;: &#39;ktang2@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}, {&#39;name&#39;: &#39;Kuangshi Ai&#39;, &#39;email&#39;: &#39;kai@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}, {&#39;name&#39;: &#39;Jun Han&#39;, &#39;email&#39;: &#39;junhanvis@outlook.com&#39;, &#39;affiliation&#39;: &#39;The Hong Kong University of Science and Technology&#39;}, {&#39;name&#39;: &#39;Chaoli Wang&#39;, &#39;email&#39;: &#39;chaoli.wang@nd.edu&#39;, &#39;affiliation&#39;: &#39;University of Notre Dame&#39;}], &#39;abstract&#39;: &#39;Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.&#39;, &#39;uid&#39;: &#39;77c7ae29-07ac-4a7b-90d1-5ca6794c6632&#39;, &#39;keywords&#39;: [&#39;Novel view synthesis&#39;, &#39;style transfer&#39;, &#39;textured Gaussian splatting&#39;, &#39;vision-language model&#39;, &#39;volume visualization&#39;], &#39;preprint_link&#39;: &#39;https://arxiv.org/abs/2507.13586&#39;, &#39;has_pdf&#39;: True, &#39;paper_award&#39;: None, &#39;doi&#39;: None, &#39;fno&#39;: None, &#39;open_access_supplemental_question&#39;: None, &#39;open_access_supplemental_link&#39;: None} <h3 class="session-list-title"><a href="paper_77c7ae29-07ac-4a7b-90d1-5ca6794c6632.html"> TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting <span class="fas mr-1">&#xf0c1;</span></a></h3><h5 class="session-list-presenter"> Authors: Kaiyuan Tang, Kuangshi Ai, Jun Han, Chaoli Wang </h5><h4 class="session-list-presenter mt-3"><span class="fas mr-1">&#xf21d;</span> Kaiyuan Tang </h4><h5 class="session-list-time"><span class="fas mr-1">&#xf017;</span><span class="format-date-span">2025-11-06T11:03:00.000Z &ndash; 2025-11-06T11:15:00.000Z</span><span alt="Change timezone on schedule page" class="timezone tztooltip"><strong>GMT<span class="selectedTimezone">-0600</span></strong><span class="tztooltiptext">Change your timezone on the schedule page</span></span></h5></div></div><div class="row my-5"><div class="col-md-8"><p>You may want to also jump to the parent event to see related presentations: <a href="event_v-full.html">VIS Full Papers</a></p></div></div><script src="/static/2025/js/views/timezone.js"></script></div></div><script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });

        const current_tz = getTimezone();
        $("#tzCurrent").html(moment().tz(current_tz).format("Z"));

        function getTimezone() {
          const urlTz = window.getUrlParameter && getUrlParameter('tz');
          if (urlTz) return urlTz;

          const storageTz = window.localStorage.getItem("tz")
          if (storageTz) return storageTz;

          return moment.tz.guess();
        }

        // find all parseable dates and localize them
        function formatDate(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY"))
        }

        function formatDateTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz)
          console.log("current_tz is ", current_tz, " element.text() is ", element.text(), " and atime is ", atime)
          element.html(atime.format("dddd, MMMM Do, YYYY @ HH:mm"))
        }

        function formatTimeSpan(element, includeDate) {
          const current_tz = getTimezone();
          console.log("current_tz is ", current_tz)
          // return '';
          // let parts = element.text().split(/[(\s-\s)|]/);
          let parts = element.text().split(" – ");
          let start = parts[0] && parts[0].trim();
          let end = parts[1] && parts[1].trim();

          let starttime = moment.utc(start).clone().tz(current_tz)
          let endtime = moment.utc(end).clone().tz(current_tz)

          //if(starttime.diff(endtime, "days") <= 0) // Making difference between the "D" numbers because the diff function
          // seems like not considering the timezone
          if (starttime.format("D") == endtime.format("D")) {
            element.html(starttime.format(
              "dddd, MMM Do, YYYY @ HH:mm") + " &ndash; " + endtime.format(
              "HH:mm"));
          } else {
            element.html(starttime.format(
              "dddd, MMM Do @ HH:mm") + " &ndash; " + endtime.format(
              "dddd, MMM Do @ HH:mm"))
          }
        }

        function formatTime(element) {
          const current_tz = getTimezone();
          let atime = moment.utc(element.text()).clone().tz(current_tz);
          element.html(atime.format("HH:mm"));
        }

        $(".format-just-date").each((_i, element) => {
          formatDate($(element));
        });

        $(".format-date").each((_i, element) => {
          formatDateTime($(element));
        });

        $(".format-date-span").each((_i, element) => {
          formatTimeSpan($(element));
        });

        $(".format-date-span-short").each((_i, element) => {
          formatTimeSpan($(element), false);
        });

        $(".format-date-span-full").each((_i, element) => {
          formatTimeSpan($(element), true);
        });

        $(".format-time").each((_i, element) => {
          formatTime($(element));
        });

        function gtag() {
          dataLayer.push(arguments);
        }

        
        
        
      });
    </script><script>

$(document).ready(function() {
    setInterval(updateSessionStatus, 30 * 1000);
    updateSessionStatus(true)
})

function updateSessionStatus(onFirstLoad) {
    // We need to write the session timings into the page to calculate current/next session.
    // See blueprint_2025.py
    startTime = '2025-11-06T10:15:00+00:00'
    endTime = '2025-11-06T11:30:00+00:00'

    // We take a 8 minute grace period to the start of a session
    const sessionStart = moment(startTime).tz(current_timezone).subtract(8, 'minutes');
    const sessionEnd = moment(endTime).tz(current_timezone);

    const sessionIsCurrent = moment().tz(current_timezone).isBetween(sessionStart, sessionEnd);
    const sessionIsFuture = moment().tz(current_timezone).isBefore(sessionStart);
    const sessionIsPast = moment().tz(current_timezone).isAfter(sessionStart);

    const classesToHide = ['.future_session_alert', '.current_session_alert', '.past_session_alert', '.room_session_stream']
    classesToHide.forEach((c) => { $(c).hide();});
    if (sessionIsCurrent) {
        $('.current_session_alert').show();
    } else if (sessionIsFuture) {
        $('.future_session_alert').show();
    } else {
        // Session is in the future
        $('#zoom_alert').hide();
        $('.past_session_alert').show();
        $('.room_session_stream').show();
    }

    // Update youtube link for older sessions
    // EDIT - We don't need this when the conference isn't live
    if(sessionIsPast){
        const roomidjson = "0_94_0_95-"+moment(startTime).format('ddd').toLowerCase()+".json"
        // console.log("this session was in:" , roomidjson)

        return fetch("https://virtual-data.ieeevis.org/"+roomidjson, {cache: "no-store"})
        .then(response => response.json())
        .then(data => {
            //console.log(data)
            // console.log('youtube_url: ' + data.youtube_url)
            //$("ytold").attr("href",data.youtube_url)

            // clear link element to avoid multiple appends
            document.getElementById("ytold").innerHTML = '';

            var a = document.createElement('a'); 
            var link = document.createTextNode(data.youtube_url);  
            a.appendChild(link); 
            a.title = "YouTube Link"; 
            a.href = data.youtube_url; 
            
            document.getElementById("ytold").appendChild(a);
        })
        .catch(error => {
            console.log("err: ", error);
        })
    }
    

}

</script></body></html>